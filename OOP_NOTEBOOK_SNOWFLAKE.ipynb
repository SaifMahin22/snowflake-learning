{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t*** WORKING WITH - AHP-QA DATA ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the mode manually\n",
    "# Options:\n",
    "#   \"AHP-QA\"        -> Standard QA environment (non-anonymized)\n",
    "#   \"AHP-PROD\"      -> Standard Production environment (non-anonymized)\n",
    "#   \"AHP-ANON-QA\"   -> QA environment with anonymized data\n",
    "#   \"AHP-ANON-PROD\" -> Production environment with anonymized data\n",
    "\n",
    "DATA_MODE = \"AHP-QA\"\n",
    "print(f'\\n\\t*** WORKING WITH - {DATA_MODE} DATA ***\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Section 1: All Imports | Load Environment & Initialize DB Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Generic Python Libraries\n",
    "# ===================================\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import pickle\n",
    "import calendar\n",
    "import re\n",
    "from datetime import datetime\n",
    "from textwrap import indent\n",
    "from typing import cast, Dict, List, Any, Optional\n",
    "\n",
    "# ===================================\n",
    "# Data Science & Display\n",
    "# ===================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# ===================================\n",
    "# ML & Similarity\n",
    "# ===================================\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ===================================\n",
    "# Network / API\n",
    "# ===================================\n",
    "import requests\n",
    "\n",
    "# ===================================\n",
    "# Langchain & Pydantic\n",
    "# ===================================\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage\n",
    "from langchain_core.prompts import (\n",
    "    HumanMessagePromptTemplate, PromptTemplate, SystemMessagePromptTemplate,\n",
    "    ChatPromptTemplate, MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ===================================\n",
    "# PostgreSQL & .env\n",
    "# ===================================\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "### ✅ Section 2: Load Environment & Initialize DB Pool\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI Key for Langchain\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize DB connection pool\n",
    "def initialize_connection_pool(DB_HOST, DB_NAME, DB_PORT, DB_USER, DB_PASSWORD, minconn=1, maxconn=20):\n",
    "    dsn = f\"dbname='{DB_NAME}' user='{DB_USER}' password='{DB_PASSWORD}' host='{DB_HOST}' port='{DB_PORT}'\"\n",
    "    return psycopg2.pool.SimpleConnectionPool(minconn, maxconn, dsn=dsn)\n",
    "\n",
    "# DB Credentials\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "\n",
    "connection_pool = initialize_connection_pool(DB_HOST, DB_NAME, DB_PORT, DB_USER, DB_PASSWORD)\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "# AWS\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "region_name = os.getenv(\"AWS_REGION\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Section 2: Config Setup (QA/PROD/ANON) + Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Previous Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set defaults\n",
    "# ROBOT_TABLE = None\n",
    "# FINANCE_TABLE = None\n",
    "\n",
    "# # Table Descriptions\n",
    "# ROBOT_DESC = \"Robot Assisted Procedure Analysis\"\n",
    "# FINANCE_DESC = \"Overall Financial Data\"\n",
    "# P_EVENT_DESC = \"Procedure/Case Level Utilization Data\"\n",
    "# PRODUCT_LEVEL_DESC = \"Product Level Utilization Data\"\n",
    "\n",
    "# # DATA_MODE-specific logic\n",
    "# if DATA_MODE == \"QA\":\n",
    "#     collection_name = 'knowledge_base_ahp_qa'\n",
    "#     knowledge_base_file_name = 'ahp_qa_llm_knowledge_base.xlsx'\n",
    "#     QUESTION_BANK_FILE_NAME = 'Question_Bank_For_BEE_AHP.xlsx'\n",
    "\n",
    "#     TABLE_INFO = {\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_p_event_with_outliers_v2_chatbot_qa\": P_EVENT_DESC,\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_with_outliers_v2_chatbot_qa\": PRODUCT_LEVEL_DESC,\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_p_event_w_outliers_rbt_chatbot_qa\": ROBOT_DESC\n",
    "#     }\n",
    "\n",
    "#     s3_directory = os.getenv(\"S3_DIRECTORY_QA\")\n",
    "#     endpoint_for_redirect_link = os.getenv(\"API_ENDPOINT_FOR_DOWNLOAD_LINK_QA\")\n",
    "\n",
    "# elif DATA_MODE == \"PROD\":\n",
    "#     collection_name = 'knowledge_base_ahp_prod'\n",
    "#     knowledge_base_file_name = 'ahp_prod_llm_knowledge_base.xlsx'\n",
    "#     QUESTION_BANK_FILE_NAME = 'Question_Bank_For_BEE_AHP.xlsx'\n",
    "\n",
    "#     TABLE_INFO = {\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_p_event_with_outliers_v2_chatbot_prod\": P_EVENT_DESC,\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_with_outliers_v2_chatbot_prod\": PRODUCT_LEVEL_DESC,\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_p_event_w_outliers_rbt_chatbot_prod\": ROBOT_DESC\n",
    "#     }\n",
    "\n",
    "#     s3_directory = os.getenv(\"S3_DIRECTORY_PROD\")\n",
    "#     endpoint_for_redirect_link = os.getenv(\"API_ENDPOINT_FOR_DOWNLOAD_LINK_PROD\")\n",
    "\n",
    "# elif DATA_MODE == \"ANON\":\n",
    "#     collection_name = 'knowledge_base_anonymized'\n",
    "#     knowledge_base_file_name = 'anonymized_llm_knowledge_base.xlsx'\n",
    "#     QUESTION_BANK_FILE_NAME = 'Question_Bank_For_BEE.xlsx'\n",
    "\n",
    "#     TABLE_INFO = {\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_p_event_w_outliers_av2_chatbot_preview\": P_EVENT_DESC,\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_with_outliers_v2_av2_chatbot_preview\": PRODUCT_LEVEL_DESC,\n",
    "#         \"production_load.l_mt_opportunity_case_procedure_p_event_w_outliers_av2_rbt_chatbot_preview\": ROBOT_DESC\n",
    "#     }\n",
    "\n",
    "#     s3_directory = os.getenv(\"S3_DIRECTORY_QA\")\n",
    "#     endpoint_for_redirect_link = os.getenv(\"API_ENDPOINT_FOR_DOWNLOAD_LINK_ANON_QA\")\n",
    "#     # endpoint_for_redirect_link = os.getenv(\"API_ENDPOINT_FOR_DOWNLOAD_LINK_ANON_PROD\")\n",
    "\n",
    "# else:\n",
    "#     raise ValueError(f\"Invalid DATA_MODE: {DATA_MODE}\")\n",
    "\n",
    "# # Assign individual tables by description\n",
    "# P_EVENT_TABLE = next((k for k, v in TABLE_INFO.items() if v == P_EVENT_DESC), None)\n",
    "# PRODUCT_LEVEL_TABLE = next((k for k, v in TABLE_INFO.items() if v == PRODUCT_LEVEL_DESC), None)\n",
    "# FINANCE_TABLE = next((k for k, v in TABLE_INFO.items() if v == FINANCE_DESC), None)\n",
    "# ROBOT_TABLE = next((k for k, v in TABLE_INFO.items() if v == ROBOT_DESC), None)\n",
    "\n",
    "# # Directory setup\n",
    "# knowledge_base_directory = './Data_For_LLM'\n",
    "# persist_directory = f'{knowledge_base_directory}/{collection_name}'\n",
    "\n",
    "# # File paths\n",
    "# question_file_name = \"prompt_for_complex_queries.xlsx\"\n",
    "# question_df = pd.read_excel(f'{knowledge_base_directory}/{question_file_name}').fillna('')\n",
    "# knowledge_base_path = os.path.join(knowledge_base_directory, knowledge_base_file_name)\n",
    "# question_bank_file_path = os.path.join(knowledge_base_directory, QUESTION_BANK_FILE_NAME)\n",
    "\n",
    "\n",
    "# # Debug Print\n",
    "# print(\"-----------------------------------------------------------------\")\n",
    "# print(f\"DATA_MODE: {DATA_MODE}\")\n",
    "# print(f\"P_EVENT_TABLE: {P_EVENT_TABLE}\")\n",
    "# print(f\"PRODUCT_LEVEL_TABLE: {PRODUCT_LEVEL_TABLE}\")\n",
    "# if ROBOT_TABLE:\n",
    "#     print(f\"ROBOT_TABLE: {ROBOT_TABLE}\")\n",
    "# if FINANCE_TABLE:\n",
    "#     print(f\"FINANCE_TABLE: {FINANCE_TABLE}\")\n",
    "# print(f\"Data Dictionary: {knowledge_base_file_name}\")\n",
    "# print(f\"Collection Name: {collection_name}\")\n",
    "# print(\"-----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "DATA_MODE: AHP-QA\n",
      "P_EVENT_TABLE: production_load.l_mt_cqo_p_event_with_outliers_chatbot_qa\n",
      "PRODUCT_LEVEL_TABLE: production_load.l_mt_cqo_product_with_outliers_chatbot_qa\n",
      "ROBOT_TABLE: production_load.l_mt_cqo_p_event_with_outliers_rbt_chatbot_qa\n",
      "Data Dictionary: ahp_qa_llm_knowledge_base.xlsx\n",
      "Collection Name: knowledge_base_ahp_qa\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ===================\n",
    "# Set defaults\n",
    "# ===================\n",
    "ROBOT_TABLE = None\n",
    "FINANCE_TABLE = None\n",
    "\n",
    "# Table Descriptions\n",
    "ROBOT_DESC = \"Robot Assisted Procedure Analysis\"\n",
    "P_EVENT_DESC = \"Procedure/Case Level Utilization Data\"\n",
    "PRODUCT_LEVEL_DESC = \"Product Level Utilization Data\"\n",
    "\n",
    "# ===================\n",
    "# DATA_MODE-specific logic\n",
    "# ===================\n",
    "if DATA_MODE == \"AHP-QA\":\n",
    "    collection_name = 'knowledge_base_ahp_qa'\n",
    "    knowledge_base_file_name = 'ahp_qa_llm_knowledge_base.xlsx'\n",
    "    QUESTION_BANK_FILE_NAME = 'Question_Bank_For_BEE_AHP.xlsx'\n",
    "\n",
    "    TABLE_INFO = {\n",
    "        \"cqo_chatbot_case_level\": P_EVENT_DESC,\n",
    "        \"cqo_chatbot_product_level\": PRODUCT_LEVEL_DESC,\n",
    "        \"cqo_chatbot_robotics\": ROBOT_DESC\n",
    "    }\n",
    "\n",
    "    s3_directory = os.getenv(\"S3_DIRECTORY_QA\")\n",
    "    endpoint_for_redirect_link = os.getenv(\"API_ENDPOINT_FOR_DOWNLOAD_LINK_QA\")\n",
    "\n",
    "elif DATA_MODE == \"AHP-PROD\":\n",
    "    collection_name = 'knowledge_base_ahp_prod'\n",
    "    knowledge_base_file_name = 'ahp_prod_llm_knowledge_base.xlsx'\n",
    "    QUESTION_BANK_FILE_NAME = 'Question_Bank_For_BEE_AHP.xlsx'\n",
    "\n",
    "    TABLE_INFO = {\n",
    "        \"cqo_chatbot_case_level\": P_EVENT_DESC,\n",
    "        \"cqo_chatbot_product_level\": PRODUCT_LEVEL_DESC,\n",
    "        \"cqo_chatbot_robotics\": ROBOT_DESC\n",
    "    }\n",
    "\n",
    "    s3_directory = os.getenv(\"S3_DIRECTORY_PROD\")\n",
    "    endpoint_for_redirect_link = os.getenv(\"API_ENDPOINT_FOR_DOWNLOAD_LINK_PROD\")\n",
    "\n",
    "elif DATA_MODE == \"AHP-ANON-QA\":\n",
    "    collection_name = 'knowledge_base_ahp_anonymized_qa'\n",
    "    knowledge_base_file_name = 'ahp_qa_anonymized_llm_knowledge_base.xlsx'\n",
    "    QUESTION_BANK_FILE_NAME = 'Question_Bank_For_BEE.xlsx'\n",
    "\n",
    "    TABLE_INFO = {\n",
    "        \"cqo_chatbot_case_level\": P_EVENT_DESC,\n",
    "        \"cqo_chatbot_product_level\": PRODUCT_LEVEL_DESC,\n",
    "        \"cqo_chatbot_robotics\": ROBOT_DESC\n",
    "    }\n",
    "\n",
    "    s3_directory = os.getenv(\"S3_DIRECTORY_QA\")\n",
    "    endpoint_for_redirect_link = os.getenv(\"API_ENDPOINT_FOR_DOWNLOAD_LINK_ANON_QA\")\n",
    "\n",
    "elif DATA_MODE == \"AHP-ANON-PROD\":\n",
    "    collection_name = 'knowledge_base_ahp_anonymized_prod'\n",
    "    knowledge_base_file_name = 'ahp_prod_anonymized_llm_knowledge_base.xlsx'\n",
    "    QUESTION_BANK_FILE_NAME = 'Question_Bank_For_BEE.xlsx'\n",
    "\n",
    "    TABLE_INFO = {\n",
    "        \"cqo_chatbot_case_level\": P_EVENT_DESC,\n",
    "        \"cqo_chatbot_product_level\": PRODUCT_LEVEL_DESC,\n",
    "        \"cqo_chatbot_robotics\": ROBOT_DESC\n",
    "    }\n",
    "\n",
    "    s3_directory = os.getenv(\"S3_DIRECTORY_PROD\")\n",
    "    endpoint_for_redirect_link = os.getenv(\"API_ENDPOINT_FOR_DOWNLOAD_LINK_ANON_PROD\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Invalid DATA_MODE: {DATA_MODE}\")\n",
    "\n",
    "# ===================\n",
    "# Assign individual tables by description\n",
    "# ===================\n",
    "P_EVENT_TABLE = next((k for k, v in TABLE_INFO.items() if v == P_EVENT_DESC), None)\n",
    "PRODUCT_LEVEL_TABLE = next((k for k, v in TABLE_INFO.items() if v == PRODUCT_LEVEL_DESC), None)\n",
    "ROBOT_TABLE = next((k for k, v in TABLE_INFO.items() if v == ROBOT_DESC), None)\n",
    "\n",
    "# ===================\n",
    "# Directory setup\n",
    "# ===================\n",
    "knowledge_base_directory = './Data_For_LLM'\n",
    "persist_directory = f'{knowledge_base_directory}/{collection_name}'\n",
    "\n",
    "# File paths\n",
    "question_file_name = \"prompt_for_complex_queries.xlsx\"\n",
    "question_df = pd.read_excel(f'{knowledge_base_directory}/{question_file_name}').fillna('')\n",
    "knowledge_base_path = os.path.join(knowledge_base_directory, knowledge_base_file_name)\n",
    "question_bank_file_path = os.path.join(knowledge_base_directory, QUESTION_BANK_FILE_NAME)\n",
    "\n",
    "# ===================\n",
    "# Debug Print\n",
    "# ===================\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(f\"DATA_MODE: {DATA_MODE}\")\n",
    "print(f\"P_EVENT_TABLE: {P_EVENT_TABLE}\")\n",
    "print(f\"PRODUCT_LEVEL_TABLE: {PRODUCT_LEVEL_TABLE}\")\n",
    "if ROBOT_TABLE:\n",
    "    print(f\"ROBOT_TABLE: {ROBOT_TABLE}\")\n",
    "if FINANCE_TABLE:\n",
    "    print(f\"FINANCE_TABLE: {FINANCE_TABLE}\")\n",
    "print(f\"Data Dictionary: {knowledge_base_file_name}\")\n",
    "print(f\"Collection Name: {collection_name}\")\n",
    "print(\"-----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Function (Not Important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import textwrap\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "try:\n",
    "    from pygments import highlight\n",
    "    from pygments.lexers import SqlLexer, JsonLexer, PythonLexer\n",
    "    from pygments.formatters import TerminalFormatter\n",
    "    HAS_PYGMENTS = True\n",
    "except ImportError:\n",
    "    HAS_PYGMENTS = False\n",
    "\n",
    "def pretty_display(var: Any, title=\"Output\", width=100):\n",
    "    \"\"\"\n",
    "    Beautified pretty print function for any variable.\n",
    "    \"\"\"\n",
    "    _print_header(title, width)\n",
    "    print()  # Line break before content\n",
    "\n",
    "    # Handle string-based types\n",
    "    if isinstance(var, str):\n",
    "        _print_highlighted_string(var)\n",
    "\n",
    "    # Handle dict or list\n",
    "    elif isinstance(var, (dict, list)):\n",
    "        pretty_json = json.dumps(var, indent=4, ensure_ascii=False)\n",
    "        _print_highlighted_string(pretty_json, lexer=JsonLexer)\n",
    "\n",
    "    # Other objects\n",
    "    else:\n",
    "        pprint.pprint(var, width=width)\n",
    "\n",
    "    print(f\"\\n{'╰' + '─' * (width - 2) + '╯'}\\n\")\n",
    "\n",
    "def _print_header(title, width):\n",
    "    \"\"\"\n",
    "    Prints a stylized header box for better visual separation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'╭' + '─' * (width - 2) + '╮'}\")\n",
    "    centered_title = f\" {title} \".center(width - 2, '─')\n",
    "    print(f\"│{centered_title}│\")\n",
    "    print(f\"{'├' + '─' * (width - 2) + '┤'}\")\n",
    "\n",
    "def _print_highlighted_string(text: str, lexer=None):\n",
    "    \"\"\"\n",
    "    Highlights and prints a string using Pygments, if available.\n",
    "    \"\"\"\n",
    "    if HAS_PYGMENTS:\n",
    "        if not lexer:\n",
    "            lexer = SqlLexer if \"SELECT\" in text.upper() else PythonLexer\n",
    "        print(highlight(text.strip(), lexer(), TerminalFormatter()))\n",
    "    else:\n",
    "        print(textwrap.dedent(text).strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Utils | Get Redirect Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import base64\n",
    "from urllib.parse import urlparse\n",
    "import posixpath  # Add this at the top\n",
    "import requests\n",
    "# Add these imports (some may already exist in your file)\n",
    "import os\n",
    "import fnmatch\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "import posixpath\n",
    "import hashlib\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class S3Utils:\n",
    "    def __init__(self, aws_access_key_id: str, aws_secret_access_key: str, endpoint_for_redirect_link: str, s3_directory: str, region_name: str = 'us-east-2'):\n",
    "        self.session = boto3.Session(\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "            region_name=region_name\n",
    "        )\n",
    "        self.s3 = self.session.client('s3')\n",
    "        self.endpoint_for_redirect_link = endpoint_for_redirect_link\n",
    "        self.s3_directory = s3_directory\n",
    "\n",
    "    def _parse_s3_uri(self, s3_uri: str):\n",
    "        if not s3_uri.startswith(\"s3://\"):\n",
    "            raise ValueError(\"Invalid S3 URI\")\n",
    "        parsed = urlparse(s3_uri)\n",
    "        bucket = parsed.netloc\n",
    "        prefix = parsed.path.lstrip(\"/\")\n",
    "        return bucket, prefix\n",
    "\n",
    "    def list_objects(self, bucket=None, prefix=None, s3_uri=None):\n",
    "        if s3_uri:\n",
    "            bucket, prefix = self._parse_s3_uri(s3_uri)\n",
    "        elif not (bucket and prefix):\n",
    "            raise ValueError(\"Provide either s3_uri or bucket and prefix\")\n",
    "\n",
    "        paginator = self.s3.get_paginator('list_objects_v2')\n",
    "        full_s3_uris = []\n",
    "\n",
    "        normalized_prefix = prefix.rstrip(\"/\")\n",
    "\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            contents = page.get(\"Contents\", [])\n",
    "            for obj in contents:\n",
    "                key = obj[\"Key\"].rstrip(\"/\")\n",
    "                if key == normalized_prefix:\n",
    "                    continue  # Skip folder object matching the prefix itself\n",
    "                full_s3_uris.append(f\"s3://{bucket}/{obj['Key']}\")\n",
    "\n",
    "        print(f\"Found {len(full_s3_uris)} valid objects in {bucket}/{prefix}\")\n",
    "        return full_s3_uris\n",
    "\n",
    "    def delete_file(self, s3_uri: str):\n",
    "        \"\"\"\n",
    "        Deletes a specific file from S3. Validates existence and ensures it's not a directory.\n",
    "\n",
    "        Args:\n",
    "            s3_uri (str): Full S3 URI of the file to delete.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the URI is a directory or the object doesn't exist.\n",
    "        \"\"\"\n",
    "        if not s3_uri.startswith(\"s3://\"):\n",
    "            raise ValueError(\"s3_uri must start with 's3://'\")\n",
    "\n",
    "        bucket, key = self._parse_s3_uri(s3_uri)\n",
    "\n",
    "        # Validate it's a file (not a folder key or just a prefix)\n",
    "        if key.endswith(\"/\") or key.rstrip(\"/\") == key.split(\"/\")[-1]:\n",
    "            raise ValueError(\"Only files can be deleted. Directories are not allowed.\")\n",
    "\n",
    "        # Check existence first\n",
    "        try:\n",
    "            self.s3.head_object(Bucket=bucket, Key=key)\n",
    "        except self.s3.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                raise FileNotFoundError(f\"File not found: {s3_uri}\")\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # If exists, delete it\n",
    "        self.s3.delete_object(Bucket=bucket, Key=key)\n",
    "        print(f\"Deleted file from {s3_uri}\")\n",
    "\n",
    "    def upload_file(self, df, file_format='csv'):\n",
    "        if file_format not in ['csv']:\n",
    "            raise ValueError(\"Supported formats: csv\")\n",
    "        \n",
    "        s3_uri = self.s3_directory\n",
    "        ## Naming of the CSV file\n",
    "        name_suffix = str(uuid.uuid4()).split('-')[0]\n",
    "        uploaded_file_name = f'atb_{name_suffix}.csv'\n",
    "        s3_uri_ = f'{s3_uri}/{uploaded_file_name}'\n",
    "\n",
    "        bucket, key = self._parse_s3_uri(s3_uri_)\n",
    "        # print(f'Key: {key}')\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "\n",
    "        if file_format == 'csv':\n",
    "            df.to_csv(buffer, index=False, encoding='utf-8-sig')\n",
    "            content_type = 'text/csv'\n",
    "\n",
    "        buffer.seek(0)\n",
    "        self.s3.upload_fileobj(buffer, Bucket=bucket, Key=key,\n",
    "                               ExtraArgs={'ContentType': content_type})\n",
    "\n",
    "        # print(f\"Uploaded file to s3://{bucket}/{key}\")\n",
    "        return s3_uri_\n",
    " \n",
    "    def generate_presigned_url(self, s3_uri: str, expiration: int = 600) -> str:\n",
    "        \"\"\"\n",
    "        Generates a pre-signed URL for downloading a file from S3.\n",
    "\n",
    "        Args:\n",
    "            s3_uri (str): Full S3 URI (must include filename).\n",
    "            expiration (int): Link expiration time in seconds (default: 600 = 10 minutes).\n",
    "\n",
    "        Returns:\n",
    "            str: Pre-signed HTTPS URL for download.\n",
    "        \"\"\"\n",
    "        bucket, key = self._parse_s3_uri(s3_uri)\n",
    "        filename = key.split(\"/\")[-1]  # Extract just the filename from key\n",
    "\n",
    "        print(f'bucket: {bucket}')\n",
    "        print(f'key: {key}')\n",
    "\n",
    "        try:\n",
    "            url = self.s3.generate_presigned_url(\n",
    "                'get_object',\n",
    "                Params={\n",
    "                    'Bucket': bucket,\n",
    "                    'Key': key,\n",
    "                    'ResponseContentDisposition': f'attachment; filename=\"{filename}\"'\n",
    "                },\n",
    "                ExpiresIn=expiration,\n",
    "            )\n",
    "            return url\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate presigned URL: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_redirect_link_from_df(self, df):\n",
    "        s3_uri_of_the_file = None\n",
    "        download_link = None\n",
    "        redirect_link = None\n",
    "\n",
    "        try:\n",
    "            # Step 1: Upload the DataFrame to S3\n",
    "            try:\n",
    "                s3_uri_of_the_file = self.upload_file(df.copy())\n",
    "            except Exception as upload_err:\n",
    "                print(f\"[Upload Error] Failed to upload file to S3: {str(upload_err)[:300]}\")\n",
    "                return s3_uri_of_the_file, download_link, redirect_link\n",
    "\n",
    "            # Step 2: Generate presigned URL\n",
    "            try:\n",
    "                download_link = self.generate_presigned_url(s3_uri_of_the_file)\n",
    "            except Exception as url_err:\n",
    "                print(f\"[Presigned URL Error] Failed to generate presigned URL: {str(url_err)[:300]}\")\n",
    "                return s3_uri_of_the_file, download_link, redirect_link\n",
    "\n",
    "            # Step 3: Request a redirect link from external API\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.endpoint_for_redirect_link,\n",
    "                    json={\"presigned_url\": download_link},\n",
    "                    timeout=20\n",
    "                )\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    redirect_link = data.get(\"download_url\")\n",
    "                    if not redirect_link:\n",
    "                        print(\"[Redirect Link Error] Response JSON did not contain 'download_url'\")\n",
    "                else:\n",
    "                    print(f\"[Redirect Link Error] Non-200 response: {response.status_code} - {response.text[:300]}\")\n",
    "\n",
    "            except Exception as api_err:\n",
    "                print(f\"[API Request Error] Failed to get redirect link: {str(api_err)[:300]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[General Error] Unexpected failure: {str(e)[:500]}\")\n",
    "\n",
    "        return s3_uri_of_the_file, download_link, redirect_link\n",
    "\n",
    "\n",
    "    # Add this method inside the S3Utils class\n",
    "    def upload_directory(\n",
    "        self,\n",
    "        src_dir: str,\n",
    "        dest_s3_uri: str,\n",
    "        *,\n",
    "        exclude_exts: tuple[str, ...] = (\".pkl\",),\n",
    "        exclude_dirs: tuple[str, ...] = (\"__pycache__\", \".venv\", \".git\", \".mypy_cache\", \".pytest_cache\"),\n",
    "        exclude_files: tuple[str, ...] = (\".DS_Store\",),\n",
    "        exclude_globs: tuple[str, ...] = (\"*.pyc\", \"*.pyo\", \".coverage*\", \"*.egg-info*\", \"*.ipynb_checkpoints*\", \"*.log\"),\n",
    "        dry_run: bool = False,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Upload an entire local directory tree to an S3 prefix, preserving structure.\n",
    "        Excludes common cache/virtualenv/artifact files and directories.\n",
    "\n",
    "        Args:\n",
    "            src_dir: Local directory to upload.\n",
    "            dest_s3_uri: Destination S3 URI prefix (e.g., 's3://my-bucket/path/to/base/').\n",
    "            exclude_exts: File extensions to exclude (exact suffix match, case-insensitive).\n",
    "            exclude_dirs: Directory names to skip (match on any path part).\n",
    "            exclude_files: Specific file basenames to skip.\n",
    "            exclude_globs: Glob patterns (fnmatch) applied to basenames to skip.\n",
    "            dry_run: If True, don't upload—just return what would be uploaded/skipped.\n",
    "\n",
    "        Returns:\n",
    "            dict with:\n",
    "                - \"uploaded\": list of s3://bucket/key uploaded (or would upload if dry_run=True)\n",
    "                - \"skipped\": list of local file paths skipped\n",
    "                - \"errors\":  list of (local_path, error_message)\n",
    "        \"\"\"\n",
    "        src_path = Path(src_dir).expanduser().resolve()\n",
    "        if not src_path.exists() or not src_path.is_dir():\n",
    "            raise ValueError(f\"Source directory not found or not a directory: {src_dir}\")\n",
    "\n",
    "        # Parse destination S3 URI\n",
    "        bucket, prefix = self._parse_s3_uri(dest_s3_uri)\n",
    "        # Ensure prefix ends with a single '/'\n",
    "        if prefix and not prefix.endswith(\"/\"):\n",
    "            prefix = prefix + \"/\"\n",
    "\n",
    "        uploaded: list[str] = []\n",
    "        skipped: list[str] = []\n",
    "        errors: list[tuple[str, str]] = []\n",
    "\n",
    "        # Normalize exclusion sets for case-insensitive checks where appropriate\n",
    "        exclude_exts_lower = tuple(e.lower() for e in exclude_exts)\n",
    "\n",
    "        for root, dirs, files in os.walk(src_path):\n",
    "            # Prune excluded directories in-place so os.walk doesn't descend into them\n",
    "            pruned_dirs = []\n",
    "            for d in list(dirs):\n",
    "                if d in exclude_dirs:\n",
    "                    pruned_dirs.append(d)\n",
    "            for d in pruned_dirs:\n",
    "                dirs.remove(d)\n",
    "\n",
    "            for fname in files:\n",
    "                local_path = Path(root) / fname\n",
    "                rel_path = local_path.relative_to(src_path)\n",
    "                rel_posix = rel_path.as_posix()\n",
    "\n",
    "                # Exclusion checks\n",
    "                name_lower = fname.lower()\n",
    "                suffix_lower = local_path.suffix.lower()\n",
    "\n",
    "                if fname in exclude_files:\n",
    "                    skipped.append(str(local_path))\n",
    "                    continue\n",
    "\n",
    "                if suffix_lower in exclude_exts_lower:\n",
    "                    skipped.append(str(local_path))\n",
    "                    continue\n",
    "\n",
    "                # Match globs against basename and posix rel path (to allow patterns like '*/.ipynb_checkpoints/*')\n",
    "                if any(fnmatch.fnmatch(fname, pattern) or fnmatch.fnmatch(rel_posix, pattern) for pattern in exclude_globs):\n",
    "                    skipped.append(str(local_path))\n",
    "                    continue\n",
    "\n",
    "                # Also skip if any parent part matches excluded directory names (defense-in-depth)\n",
    "                if any(part in exclude_dirs for part in rel_path.parts):\n",
    "                    skipped.append(str(local_path))\n",
    "                    continue\n",
    "\n",
    "                # Build S3 key using POSIX joining to ensure '/'\n",
    "                key = posixpath.join(prefix, rel_posix) if prefix else rel_posix\n",
    "\n",
    "                # Guess content-type\n",
    "                ctype, _ = mimetypes.guess_type(fname)\n",
    "                extra_args = {\"ContentType\": ctype} if ctype else None\n",
    "\n",
    "                s3_uri_out = f\"s3://{bucket}/{key}\"\n",
    "\n",
    "                try:\n",
    "                    if not dry_run:\n",
    "                        if extra_args:\n",
    "                            self.s3.upload_file(str(local_path), Bucket=bucket, Key=key, ExtraArgs=extra_args)\n",
    "                        else:\n",
    "                            self.s3.upload_file(str(local_path), Bucket=bucket, Key=key)\n",
    "                    uploaded.append(s3_uri_out)\n",
    "                except Exception as e:\n",
    "                    errors.append((str(local_path), str(e)))\n",
    "\n",
    "        return {\"uploaded\": uploaded, \"skipped\": skipped, \"errors\": errors}\n",
    "\n",
    "    # ==================================================== # \n",
    "    ## Download Feature ## \n",
    "    # ==================================================== # \n",
    "\n",
    "    # Add this method inside the S3Utils class\n",
    "    def download_uris_to_tree(\n",
    "        self,\n",
    "        file_uris: list[str],\n",
    "        dest_dir: str = \".\",\n",
    "        anchor: str = \"atb_all_modules_import/\",\n",
    "        skip_suffixes: tuple[str, ...] = (),\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Download a list of S3 URIs into the local filesystem, preserving the folder\n",
    "        structure *after* the given `anchor` segment. For example, for:\n",
    "            s3://atb-chatbot/atb_all_modules_import/modules/chart_modules/bar_chart_generator.py\n",
    "        the part after `anchor` is:\n",
    "            modules/chart_modules/bar_chart_generator.py\n",
    "        which will be recreated under `dest_dir`.\n",
    "\n",
    "        Args:\n",
    "            file_uris: List of S3 URIs to download.\n",
    "            dest_dir: Local base directory to create (defaults to current directory).\n",
    "            anchor: Path segment in S3 key after which the structure is preserved.\n",
    "            skip_suffixes: File suffixes to skip (e.g., (\".DS_Store\",)).\n",
    "\n",
    "        Returns:\n",
    "            dict with:\n",
    "                - \"downloaded\": list of local file paths successfully downloaded.\n",
    "                - \"errors\": list of (s3_uri, error_message) for failures.\n",
    "        \"\"\"\n",
    "        dest_root = Path(dest_dir).expanduser().resolve()\n",
    "        dest_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        downloaded: list[str] = []\n",
    "        errors: list[tuple[str, str]] = []\n",
    "\n",
    "        for s3_uri in file_uris:\n",
    "            try:\n",
    "                bucket, key = self._parse_s3_uri(s3_uri)\n",
    "\n",
    "                # Skip folder-like keys\n",
    "                if key.endswith(\"/\"):\n",
    "                    continue\n",
    "\n",
    "                # Optionally skip unwanted files by suffix\n",
    "                if skip_suffixes and any(key.endswith(sfx) for sfx in skip_suffixes):\n",
    "                    continue\n",
    "\n",
    "                # Determine the relative key after the anchor\n",
    "                anchor_idx = key.find(anchor)\n",
    "                if anchor_idx != -1:\n",
    "                    rel_key = key[anchor_idx + len(anchor):]\n",
    "                else:\n",
    "                    # If anchor not found, fallback to just the filename\n",
    "                    rel_key = key.split(\"/\")[-1]\n",
    "\n",
    "                # Normalize to a local path\n",
    "                local_rel_path = Path(*rel_key.split(\"/\"))\n",
    "                local_path = dest_root / local_rel_path\n",
    "\n",
    "                # Ensure parent directories exist\n",
    "                local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Download\n",
    "                self.s3.download_file(Bucket=bucket, Key=key, Filename=str(local_path))\n",
    "\n",
    "                downloaded.append(str(local_path))\n",
    "            except Exception as e:\n",
    "                errors.append((s3_uri, str(e)))\n",
    "\n",
    "        return {\"downloaded\": downloaded, \"errors\": errors}\n",
    "\n",
    "\n",
    "## Initialize\n",
    "s3_utils = S3Utils(aws_access_key_id=aws_access_key_id, aws_secret_access_key = aws_secret_access_key, region_name=region_name, endpoint_for_redirect_link=endpoint_for_redirect_link, s3_directory=s3_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## List All the Files\n",
    "# # file_list = s3_utils.list_objects(s3_uri=\"s3://document-management-new/dev-test/atb_csv\")\n",
    "# file_list = s3_utils.list_objects(s3_uri=\"s3://atb-chatbot/ahp/qa/atb_csv\")\n",
    "\n",
    "# file_list\n",
    "# s3_utils.generate_presigned_url('s3://atb-chatbot/ahp/qa/atb_csv/atb_196655e3.csv')\n",
    "# s3_utils.generate_presigned_url('s3://atb-chatbot/ahp/qa/atb_csv/atb_010d427c.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: 49\n",
      "Skipped: 5\n"
     ]
    }
   ],
   "source": [
    "# --------\n",
    "# Example usage:\n",
    "# --------\n",
    "\n",
    "result = s3_utils.upload_directory(\n",
    "    src_dir=\"../BEE_API/sc-bee-app-ahp\",                          # local directory to upload\n",
    "    dest_s3_uri=\"s3://atb-chatbot/atb_all_modules_import/ahp/qa\",  # destination S3 prefix\n",
    "    exclude_exts=(\".pkl\", \".db\", \".sqlite\", \".sqlite3\"),\n",
    "    exclude_dirs=(\"__pycache__\", \"venv\", \"bee-venv\", \".git\", \".mypy_cache\", \".pytest_cache\"),\n",
    "    exclude_files=(\".DS_Store\",),\n",
    "    exclude_globs=(\"*.pyc\", \"*.pyo\", \"*.log\", \"*.ipynb_checkpoints*\", \"*.egg-info*\", \"*/node_modules/*\"),\n",
    "    dry_run=False,                                    # set True to preview\n",
    ")\n",
    "\n",
    "print(\"Uploaded:\", len(result[\"uploaded\"]))\n",
    "print(\"Skipped:\", len(result[\"skipped\"]))\n",
    "\n",
    "if result[\"errors\"]:\n",
    "    print(\"Errors:\", result[\"errors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_to_list = s3_utils.list_objects(s3_uri=\"s3://atb-chatbot/atb_all_modules_import/ahp/qa/OOP_CODING\")\n",
    "\n",
    "# # Example usage:\n",
    "# results = s3_utils.download_uris_to_tree(\n",
    "#     file_uris=files_to_list,\n",
    "#     dest_dir=\"./temp_use_case_atb_modules\",                           # optional; default \".\"\n",
    "#     anchor=\"OOP_CODING/\",                   # keep structure AFTER this segment\n",
    "#     skip_suffixes=(\".DS_Store\",),                       # optional skipping\n",
    "# )\n",
    "# print(\"Downloaded:\", len(results[\"downloaded\"]))\n",
    "# if results[\"errors\"]:\n",
    "#     print(\"Errors:\", results[\"errors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Knowledge Base - Data Dictionary | DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBaseReader:\n",
    "    def __init__(self, knowledge_base_directory: str, knowledge_base_file_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the KnowledgeBaseReader class with the file directory and file name for the knowledge base.\n",
    "\n",
    "        Args:\n",
    "            knowledge_base_directory (str): The directory where the knowledge base file is located.\n",
    "            knowledge_base_file_name (str): The file name of the knowledge base (Excel file).\n",
    "        \"\"\"\n",
    "        self.knowledge_base_directory = knowledge_base_directory\n",
    "        self.knowledge_base_file_name = knowledge_base_file_name\n",
    "\n",
    "    def get_cleaned_columns_info(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads the knowledge base from the Excel file, cleans it, and processes the column information,\n",
    "        and returns the final DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed DataFrame with the column information.\n",
    "        \"\"\"\n",
    "        # Load the Excel file\n",
    "        file_path = f'{self.knowledge_base_directory}/{self.knowledge_base_file_name}'\n",
    "        data_dictionary_dataframe = pd.read_excel(file_path).fillna('')\n",
    "\n",
    "        # Clean column names by stripping whitespace\n",
    "        data_dictionary_dataframe.columns = data_dictionary_dataframe.columns.str.strip()\n",
    "\n",
    "        # Remove specific columns\n",
    "        columns_to_delete = ['Data Set Type', 'Definition']\n",
    "        data_dictionary_dataframe.drop(\n",
    "            columns=[col for col in columns_to_delete if col in data_dictionary_dataframe.columns],\n",
    "            axis=1,\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        # Clean the column values by stripping whitespace\n",
    "        for col in data_dictionary_dataframe.columns:\n",
    "            data_dictionary_dataframe[col] = data_dictionary_dataframe[col].str.strip()\n",
    "\n",
    "        # Process the 'Column Name' and 'Synonyms' fields\n",
    "        index_to_work_with = data_dictionary_dataframe.index\n",
    "        for idx in index_to_work_with:\n",
    "            column_name = data_dictionary_dataframe.at[idx, 'Column Name']\n",
    "\n",
    "            # Clean and format the 'Column Name'\n",
    "            processed_column_name = column_name.replace('emr_', '').replace('fin_', '')\n",
    "            processed_column_name = (' ').join(processed_column_name.split('_')).title()\n",
    "\n",
    "            # Skip 'Asa Rating'\n",
    "            if processed_column_name == 'Asa Rating':\n",
    "                continue\n",
    "\n",
    "            # Update the 'Synonyms' field\n",
    "            if processed_column_name not in data_dictionary_dataframe.at[idx, 'Synonyms']:\n",
    "                updated_synonyms = processed_column_name + ', ' + data_dictionary_dataframe.at[idx, 'Synonyms']\n",
    "                data_dictionary_dataframe.at[idx, 'Synonyms'] = updated_synonyms.strip(', ').strip()\n",
    "\n",
    "        return data_dictionary_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmbeddingManager -> Data Dictionary\n",
    "\n",
    "- Knowledge Base Chroma | Spine Tables | Synonyms -> table_module.py \n",
    "- llm_knowledge_base_clinical_fin.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, embedding_function, persist_directory, collection_name):\n",
    "        \"\"\"\n",
    "        Initializes the EmbeddingManager with the necessary parameters.\n",
    "\n",
    "        Args:\n",
    "            embedding_function: An instance of an embedding function or model (e.g., OpenAIEmbeddings).\n",
    "            persist_directory (str): Directory path where embeddings are saved.\n",
    "            collection_name (str): Name of the collection to manage.\n",
    "        \"\"\"\n",
    "        self.embedding_function = embedding_function\n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    def is_collection_available(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the collection is available in the persist directory by looking for a Pickle (.pkl) file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the collection exists, False otherwise.\n",
    "        \"\"\"\n",
    "        collection_path = os.path.join(self.persist_directory, f'{self.collection_name}.pkl')\n",
    "        return os.path.exists(collection_path)\n",
    "\n",
    "    def save_to_persist_directory(self, data_dictionary_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Save column information embeddings to the specified persist directory using Pickle.\n",
    "\n",
    "        Args:\n",
    "            data_dictionary_dataframe (pd.DataFrame): DataFrame containing column details with\n",
    "                'Column Name', 'Synonyms', and 'Brief Details' columns.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the text and embeddings that were saved.\n",
    "        \"\"\"\n",
    "        # Ensure the persist directory exists\n",
    "        if not os.path.exists(self.persist_directory):\n",
    "            os.makedirs(self.persist_directory)\n",
    "\n",
    "        # String to embed\n",
    "        em_docs = (\n",
    "            data_dictionary_dataframe['Column Name'] + ' | ' +\n",
    "            data_dictionary_dataframe['Synonyms'] + ' | ' +\n",
    "            data_dictionary_dataframe['Brief Details']\n",
    "        ).tolist()\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_function.embed_documents(em_docs)\n",
    "\n",
    "        # Create DataFrame with text and embeddings\n",
    "        df_embeddings = pd.DataFrame({\n",
    "            'text': em_docs,\n",
    "            'embeddings': embeddings\n",
    "        })\n",
    "\n",
    "        # Save the DataFrame to a pickle file\n",
    "        file_path = os.path.join(self.persist_directory, f'{self.collection_name}.pkl')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(df_embeddings, f)\n",
    "\n",
    "        print(f'Saved to: {file_path}')\n",
    "\n",
    "        return df_embeddings\n",
    "\n",
    "    def load_and_update_embeddings(self, data_dictionary_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load an existing Pickle collection, identify new or changed documents, remove old ones,\n",
    "        and update the collection with new or changed documents.\n",
    "\n",
    "        Args:\n",
    "            data_dictionary_dataframe (pd.DataFrame): DataFrame containing column details with\n",
    "                'Column Name', 'Synonyms', and 'Brief Details' columns.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Updated DataFrame containing the text and embeddings.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.persist_directory, f'{self.collection_name}.pkl')\n",
    "\n",
    "        # Load existing embeddings\n",
    "        if os.path.exists(file_path):\n",
    "            all_docs = pd.read_pickle(file_path)\n",
    "            existing_documents = all_docs['text'].tolist()\n",
    "            updated_df = all_docs.copy()\n",
    "        else:\n",
    "            all_docs = pd.DataFrame(columns=['text', 'embeddings'])\n",
    "            existing_documents = []\n",
    "            updated_df = all_docs.copy()\n",
    "\n",
    "        # Prepare new documents list\n",
    "        new_documents = (\n",
    "            data_dictionary_dataframe['Column Name'] + ' | ' +\n",
    "            data_dictionary_dataframe['Synonyms'] + ' | ' +\n",
    "            data_dictionary_dataframe['Brief Details']\n",
    "        ).tolist()\n",
    "\n",
    "        # Find new or changed documents\n",
    "        new_or_changed_documents = [doc for doc in new_documents if doc not in existing_documents]\n",
    "\n",
    "        # Find documents to remove\n",
    "        documents_to_remove = [doc for doc in existing_documents if doc not in new_documents]\n",
    "\n",
    "        # Remove old documents\n",
    "        if documents_to_remove:\n",
    "            updated_df = updated_df[~updated_df['text'].isin(documents_to_remove)].reset_index(drop=True)\n",
    "            print(f'\\n\\t# Documents Removed:')\n",
    "            for idx, doc in enumerate(documents_to_remove):\n",
    "                print(f'\\t{idx+1}: {doc}')\n",
    "\n",
    "        # Update only if there are new or changed documents\n",
    "        if new_or_changed_documents:\n",
    "            # Add new or changed documents\n",
    "            new_embeddings = self.embedding_function.embed_documents(new_or_changed_documents)\n",
    "            new_df = pd.DataFrame({\n",
    "                'text': new_or_changed_documents,\n",
    "                'embeddings': new_embeddings\n",
    "            })\n",
    "            final_df = pd.concat([updated_df, new_df], ignore_index=True)\n",
    "\n",
    "            # Save updated embeddings to Pickle\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(final_df, f)\n",
    "\n",
    "            print(f'\\n\\t# Documents Added:')\n",
    "            for idx, doc in enumerate(new_or_changed_documents):\n",
    "                print(f'\\t{idx+1}: {doc}')\n",
    "\n",
    "            print(f'\\nKnowledge Base Updated and saved to: {file_path}')\n",
    "            return final_df\n",
    "        else:\n",
    "            print('No changes detected. No updates were made inside the Knowledge Base.')\n",
    "            return updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBInfoManager\n",
    "\n",
    "- table_info_from_db\n",
    "- column_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBInfoManager:\n",
    "    def __init__(self, connection_pool):\n",
    "        \"\"\"\n",
    "        Initializes the DBInfoManager class with a connection pool and prepares the table information DataFrame\n",
    "        and column dictionary.\n",
    "\n",
    "        Args:\n",
    "            connection_pool: Connection pool object to manage database connections.\n",
    "        \"\"\"\n",
    "        self.connection_pool = connection_pool\n",
    "        self.table_info_from_db = None\n",
    "        self.column_dictionary = None\n",
    "\n",
    "    def _get_column_names(self, table_name: str):\n",
    "        \"\"\"\n",
    "        Fetches column names for a given table from the database using the connection pool.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): The fully qualified table name (e.g., 'schema.table').\n",
    "\n",
    "        Returns:\n",
    "            list: A list of column names.\n",
    "        \"\"\"\n",
    "        schema_name = table_name.split('.')[0]\n",
    "        table_name = table_name.split('.')[-1]\n",
    "\n",
    "        # SQL query to get column names\n",
    "        query = f'''\n",
    "        SELECT column_name\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = '{schema_name}' AND table_name = '{table_name}'\n",
    "        ORDER BY ordinal_position;\n",
    "        '''\n",
    "\n",
    "        conn = None\n",
    "        column_list = []\n",
    "\n",
    "        try:\n",
    "            conn = self.connection_pool.getconn()\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(query)\n",
    "                col_result = cursor.fetchall()\n",
    "\n",
    "            # Extract column names from the result\n",
    "            column_list = [row[0] for row in col_result]\n",
    "\n",
    "        except Exception as error:\n",
    "            print(f\"Error fetching column names for {table_name}: {error}\")\n",
    "\n",
    "        finally:\n",
    "            if conn:\n",
    "                self.connection_pool.putconn(conn)\n",
    "\n",
    "        return column_list\n",
    "\n",
    "    def _initialize_table_data(self, table_info):\n",
    "        \"\"\"\n",
    "        Initializes table information and column dictionary by fetching the column names for each table.\n",
    "        \"\"\"\n",
    "\n",
    "        # Fetch column names for each table\n",
    "        table_columns = {}\n",
    "        for table, description in table_info.items():\n",
    "            table_columns[table] = self._get_column_names(table)\n",
    "\n",
    "        # Remove redundant columns (optional step)\n",
    "        delete_columns = ['emr_patient_class']\n",
    "        for col in delete_columns:\n",
    "            for table in table_columns:\n",
    "                if col in table_columns[table]:\n",
    "                    table_columns[table].remove(col)\n",
    "\n",
    "        # Create the table_info_from_db DataFrame\n",
    "        data = []\n",
    "        for table, description in table_info.items():\n",
    "            related_info = f\"Table {table} contains {description.lower()}.\"\n",
    "            data.append([description, table, table_columns[table], related_info])\n",
    "\n",
    "        self.table_info_from_db = pd.DataFrame(data, columns=[\"table_for\", \"table_name\", \"columns\", \"table_related_info\"])\n",
    "\n",
    "        # Create the column dictionary\n",
    "        self.column_dictionary = {row['table_name']: row['columns'] for _, row in self.table_info_from_db.iterrows()}\n",
    "\n",
    "        return self.table_info_from_db, self.column_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QueryEvaluationEngine -> Comparative Question or Not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Pydantic Schema for Output\n",
    "# -----------------------------\n",
    "class AnalyticalQueryIntent(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured output indicating the type(s) of analytical intent present in the query.\n",
    "    More fields (like utilization, price parity, etc.) can be added over time.\n",
    "    \"\"\"\n",
    "    is_matrix_comparison: int = Field(..., description=\"1 if the query compares two or more matrices, otherwise 0\")\n",
    "\n",
    "# -----------------------------\n",
    "# Query Evaluation Engine Class\n",
    "# -----------------------------\n",
    "\n",
    "class QueryEvaluationEngine:\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    The QueryEvaluationEngine checks what kind of analytical question the user is asking.\n",
    "\n",
    "    Right now, it can tell if the question is about comparing two or more matrices.\n",
    "\n",
    "    In the future, it can be expanded to detect other types of questions too,\n",
    "    like utilization, price comparisons, or trends.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, model: str = \"gpt-4.1-mini\", temperature: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initializes the engine with the specified model and temperature.\n",
    "\n",
    "        Args:\n",
    "            model (str): OpenAI model name. Default is 'gpt-4.1.mini'.\n",
    "            temperature (float): LLM temperature for deterministic output. Default is 0.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "        self.prompt_template = self._build_prompt_template()\n",
    "        self.chain = self._build_chain()\n",
    "\n",
    "    def _build_prompt_template(self) -> PromptTemplate:\n",
    "        \"\"\"\n",
    "        Builds the prompt template used to evaluate whether a query involves matrix comparison.\n",
    "\n",
    "        Returns:\n",
    "            PromptTemplate: A fully formatted prompt with detailed instructions and examples.\n",
    "        \"\"\"\n",
    "        return PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"\"\"MATRIX COMPARISON CLASSIFIER - DECISION TASK\n",
    "\n",
    "Goal: Classify whether the user's question involves comparing between 2-3 explicitly named entities.\n",
    "\n",
    "SCHEMA:\n",
    "--------\n",
    "- is_matrix_comparison (integer)\n",
    "  - 1 → The query explicitly compares 2-3 specifically named entities (markets, facilities, etc.)\n",
    "  - 0 → The query does not involve comparison between specific named entities.\n",
    "\n",
    "RULES FOR CLASSIFICATION:\n",
    "--------------------------\n",
    "Return '1' if:\n",
    "- The question explicitly compares 2-3 individually named entities of the same type:\n",
    "  * Named markets (e.g., \"LORAIN and YOUNGSTOWN\", \"RICHMOND vs DALLAS\")\n",
    "  * Named facilities (e.g., \"Facility A compared to Facility B\")\n",
    "  * Named physicians (e.g., \"Dr. Smith compared to Dr. Jones\")\n",
    "  * Named matrices (e.g., \"Matrix A vs Matrix B\")\n",
    "  * Named regions (e.g., \"East region vs West region\")\n",
    "\n",
    "IMPORTANT: Focus on what's being compared, not what's being analyzed:\n",
    "- If comparing specific named markets/facilities/entities → Return 1\n",
    "- Even if analyzing \"top N\" items or multiple procedures WITHIN those markets → Still Return 1\n",
    "\n",
    "Return '0' if:\n",
    "- The question uses \"across\" terminology without naming specific entities to compare\n",
    "- The question is about general patterns without naming specific entities to compare\n",
    "- The question only mentions comparison over time periods\n",
    "- It asks for trends or variations within a single matrix/entity\n",
    "- It uses comparison language but doesn't specify which entities to compare\n",
    "- Uses terms like \"between markets\" or \"across facilities\" without naming specific entities\n",
    "\n",
    "EXAMPLES:\n",
    "----------\n",
    "✅ \"Compare the cost matrices between Facility A and Facility B\" → 1\n",
    "✅ \"What's the difference between performance in RICHMOND and DALLAS?\" → 1\n",
    "✅ \"How do metrics vary between Dr. Smith, Dr. Jones and Dr. Williams?\" → 1\n",
    "✅ \"How does avg CPC vary across top 20 procedures for @market LORAIN and @market YOUNGSTOWN?\" → 1\n",
    "✅ \"Compare the top 10 primary procedures by avg cpc for @market LORAIN and @market YOUNGSTOWN\" → 1\n",
    "\n",
    "❌ \"Compare the Total Encounters, Total Acquisition Cost across Markets\" → 0\n",
    "❌ \"Compare the Outcome Scores across facilities for @Market RICHMOND\" → 0\n",
    "❌ \"What is the Supply Cost to Actual Patient Revenue ratio across facilities?\" → 0\n",
    "❌ \"Compare the top 10 physicians based on performance\" → 0\n",
    "❌ \"Compare facilities based on cost per case\" → 0\n",
    "❌ \"Compare the Total Charge across facilities\" → 0\n",
    "❌ \"Show me how the values changed from January to December\" → 0\n",
    "\n",
    "NOTE:\n",
    "- The key requirement is comparison between specific named entities (like LORAIN and YOUNGSTOWN).\n",
    "- It doesn't matter if we're analyzing \"top N\" procedures or multiple metrics within those entities.\n",
    "- Terms like \"across markets\" without naming specific markets should return 0.\n",
    "- Return only valid JSON with 'is_matrix_comparison' field.\n",
    "\n",
    "INPUT:\n",
    "{text}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    def _build_chain(self):\n",
    "        \"\"\"\n",
    "        Creates the structured output chain combining the prompt with the LLM.\n",
    "\n",
    "        Returns:\n",
    "            Runnable chain for evaluating user input.\n",
    "        \"\"\"\n",
    "        structured_llm = self.llm.with_structured_output(AnalyticalQueryIntent)\n",
    "        return self.prompt_template | structured_llm\n",
    "\n",
    "    def classify(self, user_query: str) -> AnalyticalQueryIntent:\n",
    "        \"\"\"\n",
    "        Evaluates the user query to determine if it involves a matrix comparison.\n",
    "\n",
    "        Args:\n",
    "            user_query (str): The input query from the user.\n",
    "\n",
    "        Returns:\n",
    "            MatrixQueryDecision: A structured result with is_matrix_comparison = 0 or 1.\n",
    "        \"\"\"\n",
    "        return self.chain.invoke({\"text\": user_query})\n",
    "\n",
    "    def get_sql_template_if_applicable(self, user_query: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Returns a fully generic SQL template for matrix comparison if the intent matches.\n",
    "\n",
    "        Args:\n",
    "            user_query (str): Input question.\n",
    "\n",
    "        Returns:\n",
    "            str | None: SQL guidance and template if matrix comparison is detected.\n",
    "        \"\"\"\n",
    "        result = self.classify(user_query)\n",
    "\n",
    "        if result.is_matrix_comparison == 1:\n",
    "            return \"\"\"⸻\n",
    "\n",
    "🧩 Fully Generic SQL Template for Comparison Queries\n",
    "\n",
    "Question Sample: \n",
    "\\t1.\\tExplicit comparison using “compare”\n",
    "Compare the top N entities based on a specific metric across multiple groups within a selected category and time period.\n",
    "\\t2.\\tImplicit comparison using “how does X vary”\n",
    "How does the average value of a metric vary across top entities between two selected groups for a given timeframe and category?\n",
    "\\t3.\\tMulti-metric comparison\n",
    "How do multiple metrics (e.g., cost, encounter count, and score) vary across top-ranked entities within two or more comparison groups?\n",
    "\\t4.\\tGroup-based performance comparison\n",
    "Show the differences in performance metrics across departments or facilities for a specific category over a defined period.\n",
    "\\t5.\\tEntity-to-entity variation\n",
    "How do key metrics differ between selected individuals or groups (e.g., physicians, vendors, facilities) for top items or procedures in a specific domain?\n",
    "\n",
    "SQL Query Template:\n",
    "⸻⸻⸻⸻\n",
    "\n",
    "WITH base_filtered AS (\n",
    "    SELECT \n",
    "        -- required entity columns (e.g., procedure, physician, item),\n",
    "        -- grouping columns (e.g., market, facility),\n",
    "        -- unique identifier for event-level data,\n",
    "        -- relevant metric column (e.g., cost, score, revenue)\n",
    "    FROM <your_source_table>\n",
    "    WHERE \n",
    "        -- apply filters based on question (e.g., year, category, market, service line, etc.)\n",
    "        -- exclude irrelevant or unknown values if needed\n",
    "),\n",
    "top_entities AS (\n",
    "    SELECT \n",
    "        <entity_column>,\n",
    "        -- calculate metric (e.g., AVG, SUM, COUNT) as required by the question\n",
    "    FROM base_filtered\n",
    "    GROUP BY <entity_column>\n",
    "    ORDER BY <calculated_metric> DESC\n",
    "    LIMIT <N>\n",
    "),\n",
    "comparison_result AS (\n",
    "    SELECT \n",
    "        <entity_column>,\n",
    "        <comparison_group_column>,  -- e.g., market, facility, physician\n",
    "        -- calculate metric again for each comparison group\n",
    "    FROM base_filtered bf\n",
    "    JOIN top_entities te \n",
    "      ON bf.<entity_column> = te.<entity_column>\n",
    "    GROUP BY <entity_column>, <comparison_group_column>\n",
    ")\n",
    "SELECT * \n",
    "FROM comparison_result\n",
    "ORDER BY <comparison_group_column>, <calculated_metric> DESC;\n",
    "\n",
    "⸻⸻⸻⸻⸻⸻\n",
    "\n",
    "🧭 How to Use It (LLM Guidance)\n",
    "\n",
    "Placeholder\\tWhat to Replace It With\n",
    "<your_source_table>\\tThe source dataset or view\n",
    "<entity_column>\\tWhat you’re comparing (e.g., procedure, physician, product, etc.)\n",
    "<comparison_group_column>\\tThe groups you’re comparing across (e.g., market, region, etc.)\n",
    "<calculated_metric>\\tMetric to compare (e.g., average cost, count, score, etc.)\n",
    "<N>\\tNumber of top entities to include in the comparison\n",
    "⸻⸻⸻⸻⸻⸻\n",
    "\"\"\"\n",
    "        return None\n",
    "    \n",
    "query_evaluation_engine = QueryEvaluationEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rough Run - Query Evaluation Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"compare the top 10 primary procedures by average cpc for Market Lorain and Market Youngstown for Service line WOMEN'S HEALTH for 2024\"\n",
    "# query = \"Whats my name?\"\n",
    "# query = \"Compare the spend variations over time.\"\n",
    "# query = \"Compare the  Total Encounters, Total Acquisition Cost, and Outcome Score across Facilities for @Market RICHMOND.\"\n",
    "# query = \"How does avg CPC, and total encounter vary across top 20 procedures for @market LORAIN and @market @YOUNGSTOWN for @service line @WOMEN'S HEALTH?\"\n",
    "# query = \"Compare the top 10 physicians based on the number of encounters and total acquisition cost.\"\n",
    "# query = 'Whats the total encounter count between Physician A and PHysician B?'\n",
    "\n",
    "\n",
    "# # ## Optional\n",
    "# classify_result = query_evaluation_engine.classify(query).is_matrix_comparison\n",
    "# print(f'Query: {query}\\nClassified as =>> {classify_result}')  # ➝ 1\n",
    "\n",
    "# # ## Main Code\n",
    "# comparison_sql_template_prompt = query_evaluation_engine.get_sql_template_if_applicable(query)\n",
    "# print(f'\\nPrompt To Be Used: \\n{comparison_sql_template_prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_questions = pd.read_excel(f'Data_For_LLM/Question_Bank_For_BEE_AHP.xlsx')\n",
    "# all_questions = check_questions['Questions'].tolist()\n",
    "\n",
    "# records = []\n",
    "# for q in range(len(all_questions)):\n",
    "#     question = all_questions[q]\n",
    "#     print(f'\\nQuestion: {question}')\n",
    "#     classify_result = query_evaluation_engine.classify(question).is_matrix_comparison\n",
    "#     print(f'Classified as =>> {classify_result}')  # ➝ 1\n",
    "#     print('-----------------------------------')\n",
    "\n",
    "#     records.append([classify_result, question])\n",
    "\n",
    "# temp_df = pd.DataFrame(records, columns=['is_matrix_comparison', 'Question'])\n",
    "# # temp_df.to_excel('Matrix_Comparison_Report.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmbeddingTaskHelper \n",
    "\n",
    "- generating_df_from_embeddings\n",
    "- helper_function_for_table_selection\n",
    "- find_appropriate_row_from_complex_query_sheet\n",
    "- get_column_info_for_sql_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTaskHelper:\n",
    "    def __init__(self, embedding_function, df_embeddings, column_dictionary, data_dictionary_dataframe, table_info_from_db, question_df):\n",
    "        \"\"\"\n",
    "        Initializes the EmbeddingHandling class with the necessary embedding function.\n",
    "\n",
    "        Args:\n",
    "            embedding_function: The embedding function or model used to generate embeddings.\n",
    "        \"\"\"\n",
    "        self.embedding_function = embedding_function\n",
    "        self.df_embeddings = df_embeddings\n",
    "        self.column_dictionary = column_dictionary\n",
    "        self.data_dictionary_dataframe = data_dictionary_dataframe\n",
    "        self.table_info_from_db = table_info_from_db\n",
    "        self.question_df = question_df\n",
    "\n",
    "        self.query_evaluation_engine = QueryEvaluationEngine()\n",
    "\n",
    "    def generating_df_from_embeddings(\n",
    "        self,\n",
    "        user_prompt: str,\n",
    "        df_embeddings: pd.DataFrame,\n",
    "        list_of_columns: list,\n",
    "        data_dictionary_dataframe: pd.DataFrame,\n",
    "        n_top: int = 8\n",
    "    ) -> tuple:\n",
    "        \"\"\"\n",
    "        Generates a DataFrame from embeddings by finding the best matching columns based on the user's query.\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The query from the user.\n",
    "            df_embeddings (pd.DataFrame): DataFrame containing stored embeddings and text.\n",
    "            list_of_columns (list): List of columns to filter.\n",
    "            data_dictionary_dataframe (pd.DataFrame): DataFrame containing column information for tables.\n",
    "            n_top (int, optional): The number of top matches to return. Default is 8.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the filtered DataFrame, list of best match columns, and the original DataFrame with similarity scores.\n",
    "                - filtered_df (pd.DataFrame): DataFrame with filtered and merged results.\n",
    "                - best_match_columns (list): List of best matching column names.\n",
    "                - open_df (pd.DataFrame): Original DataFrame with similarity scores.\n",
    "        \"\"\"\n",
    "        # Generate embedding for the user prompt\n",
    "        user_embedding = self.embedding_function.embed_query(user_prompt)\n",
    "        \n",
    "        # Calculate cosine similarity between user embedding and stored embeddings\n",
    "        stored_embeddings = np.vstack(df_embeddings['embeddings'].to_numpy())\n",
    "        similarity_scores = cosine_similarity([user_embedding], stored_embeddings)[0]\n",
    "        \n",
    "        # Prepare the DataFrame for results\n",
    "        df_embeddings = df_embeddings.copy()  # To avoid modifying the original DataFrame\n",
    "        df_embeddings['similarity_score'] = similarity_scores\n",
    "        df_embeddings['col_name'] = df_embeddings['text'].apply(lambda x: x.split(' | ')[0])\n",
    "        df_embeddings['synonyms'] = df_embeddings['text'].apply(lambda x: x.split(' | ')[1])\n",
    "\n",
    "        # Filter based on columns and similarity score\n",
    "        open_df = df_embeddings[df_embeddings['col_name'].isin(list_of_columns)]\n",
    "        open_df = open_df[open_df['similarity_score'] > 0.40]\n",
    "        open_df = open_df.reset_index(drop=True)\n",
    "        best_match_columns = []\n",
    "\n",
    "        # Reward matching words in synonyms\n",
    "        if not open_df.empty:\n",
    "            user_prompt_cleaned = re.sub(r'[^a-zA-Z\\s]', '', user_prompt).lower()\n",
    "            user_prompt_cleaned = ' '.join(user_prompt_cleaned.split())\n",
    "            \n",
    "            for index, row in open_df.iterrows():\n",
    "                vocab_list = [vocab.strip().lower() for vocab in row['synonyms'].split(',')]\n",
    "                \n",
    "                for vocab in vocab_list:\n",
    "                    if vocab in user_prompt_cleaned:\n",
    "                        open_df.at[index, 'similarity_score'] += 0.15\n",
    "                        break\n",
    "            \n",
    "            open_df = open_df.sort_values('similarity_score', ascending=False).head(n_top)\n",
    "            best_match_columns = open_df['col_name'].tolist()\n",
    "\n",
    "        # Merge with the original table for additional info\n",
    "        filtered_df = pd.merge(\n",
    "            open_df,\n",
    "            data_dictionary_dataframe,\n",
    "            left_on='col_name',\n",
    "            right_on='Column Name',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Combine columns while avoiding duplicates\n",
    "        final_columns = list(open_df.columns) + [\n",
    "            col for col in data_dictionary_dataframe.columns\n",
    "            if col not in ['Column Name', 'Synonyms']\n",
    "        ]\n",
    "        filtered_df = filtered_df[final_columns]\n",
    "\n",
    "        return filtered_df, best_match_columns, open_df\n",
    "    \n",
    "    def helper_function_for_table_selection(self, filtered_df):\n",
    "        \"\"\"\n",
    "        Generates a knowledge base string from the provided DataFrame for use in SQL query generation.\n",
    "\n",
    "        Args:\n",
    "            filtered_df (DataFrame): The DataFrame containing the required columns ('col_name', 'synonyms', 'Possible Values', 'Special Instruction').\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted string representing the knowledge base, including column names, synonyms, possible values, and special instructions.\n",
    "        \"\"\"\n",
    "\n",
    "        # Assuming filtered_df is already defined and contains the required columns\n",
    "        knowledge_base = []\n",
    "        for index, row in filtered_df[['col_name', 'synonyms', 'Possible Values', 'Special Insturction']].iterrows():\n",
    "            # Prepare the information in the desired format\n",
    "            info = f\"\\t\\t{index+1}. Column Name: {row['col_name']}\\n\"\n",
    "            # info += f\"\\t\\t- Synonyms of {row['col_name']}: {row['synonyms'].split(', ')[:4]}\\n\"\n",
    "            info += f\"\\t\\t- Synonyms of {row['col_name']}: {row['synonyms'].split(', ')}\\n\"\n",
    "\n",
    "        \n",
    "            # Append the information to the knowledge_base\n",
    "            knowledge_base.append(info)\n",
    "            if len(knowledge_base) == 6:\n",
    "                break\n",
    "\n",
    "        knowledge_base = ('\\n').join(knowledge_base)\n",
    "        return knowledge_base\n",
    "\n",
    "    def knowledge_base_function_for_sql_generation(self, filtered_df):\n",
    "        \"\"\"\n",
    "        Generates a knowledge base string from the provided DataFrame for use in SQL query generation.\n",
    "\n",
    "        Args:\n",
    "            filtered_df (DataFrame): The DataFrame containing the required columns ('col_name', 'synonyms', 'Possible Values', 'Special Instruction').\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted string representing the knowledge base, including column names, synonyms, possible values, and special instructions.\n",
    "        \"\"\"\n",
    "\n",
    "        # Assuming filtered_df is already defined and contains the required columns\n",
    "        knowledge_base = []\n",
    "        for index, row in filtered_df[['col_name', 'synonyms', 'Possible Values', 'Special Insturction']].iterrows():\n",
    "            # Prepare the information in the desired format\n",
    "            info = f\"\\t{index+1}. Column Name: {row['col_name']}\\n\"\n",
    "            info += f\"\\t\\t- Synonyms: {row['synonyms'].split(', ')}\\n\"\n",
    "\n",
    "            possible_values = row.get('Possible Values', '')\n",
    "            if possible_values:\n",
    "                possible_values = possible_values.replace(\"\\n\", \"\\n\\t\\t\\t\")\n",
    "                info += f\"\"\"\\t\\t- Possible Values:\\n\\t\\t\\t{possible_values}\\n\"\"\"\n",
    "\n",
    "            special_instruction = row.get('Special Insturction', '')\n",
    "            if special_instruction:\n",
    "                special_instruction = special_instruction.replace(\"\\n\", \"\\n\\t\\t\\t\")\n",
    "                info += f\"\"\"\\t\\t- Note:\\n\\t\\t\\t{special_instruction}\\n\"\"\"\n",
    "\n",
    "            # Append the information to the knowledge_base\n",
    "            knowledge_base.append(info)\n",
    "\n",
    "        # Now knowledge_base contains all the rows in the desired format\n",
    "        # This can be fed into a model as a knowledge base\n",
    "        knowledge_base = ('\\n\\n').join(knowledge_base)\n",
    "\n",
    "\n",
    "        knowledge_base = f\"\\n\\t- Most Probable Columns, its synonyms and related information are given below in descending order:\\n\\t============== Context for Probable Columns ================\\n{knowledge_base}\\n\\t=========================\\n\\tWhile generating the sql query, give great importance to the above context.\"\n",
    "        return knowledge_base\n",
    "    \n",
    "    def clean_prompt(self, user_prompt):\n",
    "        \"\"\"\n",
    "        Cleans the user prompt by:\n",
    "        - Removing special characters (except alphanumeric, spaces, and basic punctuation).\n",
    "        - Normalizing whitespace.\n",
    "        - Converting to lowercase.\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The user prompt to clean.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned user prompt.\n",
    "        \"\"\"\n",
    "        # Replace newline and tab characters with a space, remove unwanted characters, and normalize spaces\n",
    "        clean_user_prompt = re.sub(\n",
    "            r'\\s+', ' ',\n",
    "            re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', user_prompt.replace('\\n', ' ').replace('\\t', ' '))\n",
    "        ).strip().lower()\n",
    "        return clean_user_prompt\n",
    "\n",
    "    def check_prompt_conditions_for_system_benchmark(self, user_prompt):\n",
    "        \"\"\"\n",
    "        Check if the word \"System Benchmark\" is present and \"System Benchmark CPC\" (or its elaborated version) is absent in the user prompt.\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The user prompt to check.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the conditions are met, False otherwise.\n",
    "        \"\"\"\n",
    "        # Clean the user prompt\n",
    "        clean_user_prompt = self.clean_prompt(user_prompt)\n",
    "\n",
    "        # Check for \"system benchmark\"\n",
    "        has_system_benchmark = \"system benchmark\" in clean_user_prompt\n",
    "\n",
    "        # Check for \"system benchmark cpc\" or its elaborated version\n",
    "        has_system_benchmark_cpc = any(\n",
    "            phrase in clean_user_prompt\n",
    "            for phrase in [\n",
    "                \"system benchmark cpc\",\n",
    "                \"system benchmark cost per case\"\n",
    "            ]\n",
    "        )\n",
    "        # Return True if \"system benchmark\" is present and \"system benchmark cpc\" is absent\n",
    "        return has_system_benchmark and not has_system_benchmark_cpc\n",
    "\n",
    "    def find_appropriate_row_from_complex_query_sheet(self, df, prompt):\n",
    "        # Clean the user prompt\n",
    "        # prompt = re.sub(r'[^a-zA-Z0-9\\s]', '', prompt).lower()\n",
    "        # prompt = re.sub(r'\\s+', ' ', re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', prompt.replace('\\n', ' ').replace('\\t', ' '))).strip().lower()\n",
    "\n",
    "        prompt = self.clean_prompt(prompt)\n",
    "        for index, row in df.iterrows():\n",
    "            synonyms = [syn.strip().lower() for syn in row['synonyms'].split(',')]\n",
    "            for syn in synonyms:\n",
    "                if syn in prompt:\n",
    "                    \n",
    "                    # If the word \"System Benchmark\" exists and \n",
    "                    # \"System Benchmark CPC\" or its elaborated form is not found, the function returns True.\n",
    "                    if self.check_prompt_conditions_for_system_benchmark(prompt):\n",
    "                        continue  \n",
    "                    return row['prompt']\n",
    "        \n",
    "        return None\n",
    "\n",
    "    ## Updated Prompt ## With Potentital Savings Calculation\n",
    "    def get_robot_analysis_prompt(self, table_to_use):\n",
    "        \"\"\"\n",
    "        Returns the mandatory robotic vs. non-robotic analysis SQL template\n",
    "        WITHOUT any time-frame/date filter.\n",
    "        Now includes potential savings calculation for relevant questions.\n",
    "        \"\"\"\n",
    "        robot_prompt = ''\n",
    "        try:\n",
    "            if table_to_use == self.table_info_from_db[self.table_info_from_db['table_for'] == 'Robot Assisted Procedure Analysis']['table_name'].values[0]:\n",
    "                robot_prompt = '''# MANDATORY TEMPLATE: ROBOTIC PROCEDURE ANALYSIS\n",
    "\n",
    "    ALWAYS USE THIS EXACT TEMPLATE FOR ANY ROBOTIC VS. NON-ROBOTIC PROCEDURE ANALYSIS\n",
    "\n",
    "    ## REQUIRED FILTERS\n",
    "    The following filters MUST be applied to ANY analysis involving robotic procedures:\n",
    "\n",
    "    1. PATIENT ELIGIBILITY:\n",
    "    - Include ONLY patients with ASA scores 1-3\n",
    "    - SQL: WHERE emr_asa_rating NOT IN ('4 - CONSTANT THREAT TO LIFE', '5 - MORIBUND PATIENT', '6 - BRAIN DEAD PATIENT')\n",
    "\n",
    "    2. ROBOTICS DATA QUALITY:\n",
    "    - Exclude cases with unknown robotics status \n",
    "    - SQL: AND emr_robotics_y_n <> 'NOT AVAILABLE'\n",
    "\n",
    "    3. PATIENT TYPE STANDARDIZATION:\n",
    "    - Combine Emergency Cases with Outpatient\n",
    "    - SQL: CASE WHEN emr_patient_type_bucket = 'EMERGENCY' THEN 'OUTPATIENT' ELSE emr_patient_type_bucket END AS patient_type\n",
    "\n",
    "    4. PROCEDURE ELIGIBILITY:\n",
    "    - Include ONLY procedures with BOTH robotic AND non-robotic cases\n",
    "    - Each procedure MUST HAVE at least one robotic case AND at least one non-robotic case\n",
    "\n",
    "    5. FACILITY ELIGIBILITY\n",
    "    - Include only facilities that have at least one robotic case\n",
    "    - Facilities with zero robotic cases are completely excluded.\n",
    "\n",
    "    ## MANDATORY SQL IMPLEMENTATION\n",
    "    WITH\n",
    "    -- Step 0: Facilities that performed at least one robotic case\n",
    "    robotics_facility_list AS (\n",
    "        SELECT DISTINCT\n",
    "            emr_facility_name\n",
    "        FROM\n",
    "            procedures_table           -- ← replace with your table name\n",
    "        WHERE\n",
    "            emr_robotics_y_n = 'Y'\n",
    "            AND emr_asa_rating NOT IN (\n",
    "                '4 - CONSTANT THREAT TO LIFE',\n",
    "                '5 - MORIBUND PATIENT',\n",
    "                '6 - BRAIN DEAD PATIENT'\n",
    "            )\n",
    "    ),\n",
    "    -- Step 1: Procedures that have BOTH robotic and non-robotic cases\n",
    "    eligible_procedures AS (\n",
    "        SELECT\n",
    "            emr_primary_procedure\n",
    "        FROM\n",
    "            procedures_table\n",
    "        WHERE\n",
    "            emr_asa_rating NOT IN (\n",
    "                '4 - CONSTANT THREAT TO LIFE',\n",
    "                '5 - MORIBUND PATIENT',\n",
    "                '6 - BRAIN DEAD PATIENT'\n",
    "            )\n",
    "            AND emr_robotics_y_n <> 'NOT AVAILABLE'\n",
    "        GROUP BY\n",
    "            emr_primary_procedure\n",
    "        HAVING\n",
    "            SUM(CASE WHEN emr_robotics_y_n = 'Y' THEN 1 ELSE 0 END) > 0\n",
    "            AND SUM(CASE WHEN emr_robotics_y_n = 'N' THEN 1 ELSE 0 END) > 0\n",
    "    ),\n",
    "    -- Step 2: Main filtered dataset\n",
    "    filtered_robotic_data AS (\n",
    "        SELECT\n",
    "            p.*,\n",
    "            CASE\n",
    "                WHEN p.emr_patient_type_bucket = 'EMERGENCY' THEN 'OUTPATIENT'\n",
    "                ELSE p.emr_patient_type_bucket\n",
    "            END AS patient_type\n",
    "        FROM\n",
    "            procedures_table p\n",
    "            JOIN eligible_procedures ep\n",
    "                ON p.emr_primary_procedure = ep.emr_primary_procedure\n",
    "            JOIN robotics_facility_list rfl\n",
    "                ON p.emr_facility_name = rfl.emr_facility_name\n",
    "        WHERE\n",
    "            p.emr_asa_rating NOT IN (\n",
    "                '4 - CONSTANT THREAT TO LIFE',\n",
    "                '5 - MORIBUND PATIENT',\n",
    "                '6 - BRAIN DEAD PATIENT'\n",
    "            )\n",
    "            AND p.emr_robotics_y_n <> 'NOT AVAILABLE'\n",
    "    )\n",
    "    -- Step 3: Final query\n",
    "    SELECT\n",
    "        frd.*,\n",
    "        /* POTENTIAL SAVINGS CALCULATION */\n",
    "        CASE\n",
    "            WHEN AVG(CASE WHEN frd.emr_robotics_y_n = 'N' THEN frd.emr_total_acquisition_cost END) \n",
    "                > AVG(CASE WHEN frd.emr_robotics_y_n = 'Y' THEN frd.emr_total_acquisition_cost END)\n",
    "            THEN ROUND(\n",
    "                (\n",
    "                    AVG(CASE WHEN frd.emr_robotics_y_n = 'N' THEN frd.emr_total_acquisition_cost END) \n",
    "                    - AVG(CASE WHEN frd.emr_robotics_y_n = 'Y' THEN frd.emr_total_acquisition_cost END)\n",
    "                ) * COUNT(DISTINCT CASE WHEN frd.emr_robotics_y_n = 'N' THEN frd.emr_p_event END)\n",
    "            ,1)\n",
    "        END AS potential_savings\n",
    "    FROM\n",
    "        filtered_robotic_data frd;\n",
    "\n",
    "    ## CRITICAL WARNING\n",
    "    DO NOT MODIFY THESE CRITERIA UNDER ANY CIRCUMSTANCES. These filters are REQUIRED for all robotic procedure analyses to ensure data consistency and valid comparisons.\n",
    "\n",
    "    ## IMPLEMENTATION NOTES\n",
    "    1. Replace \"procedures_table\" with the actual table name in your database\n",
    "    2. You may add additional columns or filters as needed for specific analyses\n",
    "    3. You must NEVER remove or modify the base filters defined above\n",
    "    4. All metrics (LOS, cost, complications, potential savings, etc.) must be calculated using this filtered dataset\n",
    "    5. Return only the SQL query, and ensure it ends with a semicolon (‘;’).\n",
    "    6. ** average_cost_per_case = SUM(emr_total_acquisition_cost) / COUNT(DISTINCT emr_p_event) ** \n",
    "    '''\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return robot_prompt\n",
    "\n",
    "    def get_column_info_for_sql_generation(self, user_prompt_to_use, selected_table_text):\n",
    "\n",
    "        ## Selected Table and Column Information to Use\n",
    "        try:\n",
    "            table_to_use = self.table_info_from_db[self.table_info_from_db['table_name'].str.contains(selected_table_text)]['table_name'].values.tolist()[0]\n",
    "        except:\n",
    "            print(f'\\nFailed to select a table. \\nSelected Table Text Used: {selected_table_text}\\n')\n",
    "            return None, None, None, None, None, None, None\n",
    "        \n",
    "        filtered_df, best_match_columns, embedded_df = self.generating_df_from_embeddings(user_prompt_to_use, self.df_embeddings, self.column_dictionary[table_to_use], self.data_dictionary_dataframe)\n",
    "\n",
    "        column_info_from_knowledge_base = ''\n",
    "        prompt_to_use_for_complex_question = ''\n",
    "        \n",
    "        ## Final Column Information that will be used to generate sql query\n",
    "        table_columns = self.table_info_from_db[self.table_info_from_db['table_name'] == table_to_use]['columns'].tolist()[0]\n",
    "\n",
    "        ## We are adding the Synonyms as well | Using this file: LLM_data_definition_v2/LLM_data_definition_v2.xlsx\n",
    "        if best_match_columns:\n",
    "            column_info_from_knowledge_base = self.knowledge_base_function_for_sql_generation(filtered_df)\n",
    "\n",
    "            ## Get Prompt for Robotic Analysis\n",
    "            robot_analysis_prompt = self.get_robot_analysis_prompt(table_to_use)\n",
    "\n",
    "            if robot_analysis_prompt !='': \n",
    "                prompt_to_use_for_complex_question = robot_analysis_prompt\n",
    "    \n",
    "            else: ## If No Prompt found for Robotic Analysis, then \n",
    "                prompt_to_use_for_complex_question = self.find_appropriate_row_from_complex_query_sheet(self.question_df, user_prompt_to_use)\n",
    "\n",
    "                # 🧩 Attempt to retrieve a comparison-specific SQL template prompt based on the user query.\n",
    "                # This is useful when the query implies or explicitly requests a comparison between two or more matrics (e.g., \"compare X and Y\").\n",
    "                sql_comparison_prompt = self.query_evaluation_engine.get_sql_template_if_applicable(user_prompt_to_use)\n",
    "                # 🧠 If a comparison prompt is applicable and there's already a base prompt, append the comparison logic to it.\n",
    "                if sql_comparison_prompt:\n",
    "                    print(\"\\n\" + \"=\"*60)\n",
    "                    print(\"⚖️  COMPARISON MODE ACTIVATED: Question requires comparing two matrices.\")\n",
    "                    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "                    if prompt_to_use_for_complex_question:\n",
    "                        # Add the comparison prompt to the existing prompt, preserving both layers of logic.\n",
    "                        prompt_to_use_for_complex_question = sql_comparison_prompt + \"\\n\" + prompt_to_use_for_complex_question\n",
    "                    else:\n",
    "                        # If no base prompt exists yet, use the comparison prompt as the starting point.\n",
    "                        prompt_to_use_for_complex_question = sql_comparison_prompt\n",
    "\n",
    "\n",
    "            if prompt_to_use_for_complex_question: # If Complex Question - Benchmark CPC, Utilization Percentage found | Or RAS Analysis Prompt Available\n",
    "                columns_info_to_generate_sql = str(table_columns) + column_info_from_knowledge_base + prompt_to_use_for_complex_question\n",
    "            else:\n",
    "                columns_info_to_generate_sql = str(table_columns) + column_info_from_knowledge_base\n",
    "\n",
    "        return table_to_use, filtered_df, best_match_columns, table_columns, column_info_from_knowledge_base, prompt_to_use_for_complex_question, columns_info_to_generate_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <mark >====== Preliminary Readings ====== </mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Function Running -> \"load_and_update_embeddings\".\n",
      "\n",
      "\n",
      "\t# Documents Removed:\n",
      "\t1: emr_month_yr | Month Yr, Discharge Date, Release Date, Hospital Discharge Date, Patient Discharge Date, Exit Date, Discharge Month, Discharge Year, Month, Year | Month and Year when a patient is officially discharged from the hospital or healthcare facility.\n",
      "\n",
      "\t# Documents Added:\n",
      "\t1: emr_month_yr | Month Yr | Month and Year when a patient is officially discharged from the hospital or healthcare facility.\n",
      "\n",
      "Knowledge Base Updated and saved to: ./Data_For_LLM/knowledge_base_ahp_qa\\knowledge_base_ahp_qa.pkl\n"
     ]
    }
   ],
   "source": [
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "####              DB Connection Establishment          |        Redshift Table and Column Information\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "\n",
    "information_from_db = DBInfoManager(connection_pool)\n",
    "table_info_from_db, column_dictionary = information_from_db._initialize_table_data(TABLE_INFO)\n",
    "\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "####                           Reading Data Dictionary | Reading Data Dictionary\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "\n",
    "# Initialize the KnowledgeBaseReader class\n",
    "knowledge_base_reader = KnowledgeBaseReader(\n",
    "    knowledge_base_directory=knowledge_base_directory,\n",
    "    knowledge_base_file_name=knowledge_base_file_name\n",
    ")\n",
    "\n",
    "# Get the cleaned columns information DataFrame\n",
    "data_dictionary_dataframe = knowledge_base_reader.get_cleaned_columns_info()\n",
    "\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "####              Embeddings | Embed the Knowledge Base or Update The knowledge base | df_embeddings\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "\n",
    "\n",
    "# Create an instance of EmbeddingManager\n",
    "embedding_manager = EmbeddingManager(\n",
    "    embedding_function=embedding_function,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "if embedding_manager.is_collection_available():\n",
    "    print(f'Embedding Function Running -> \"load_and_update_embeddings\".\\n')\n",
    "    df_embeddings = embedding_manager.load_and_update_embeddings(data_dictionary_dataframe)\n",
    "else:\n",
    "    print(f'Embedding and Saving Function -> \"save_to_persist_directory\". \\nFolder Name: {collection_name}\\npersist_directory: {persist_directory}\\n')\n",
    "    df_embeddings = embedding_manager.save_to_persist_directory(data_dictionary_dataframe)\n",
    "\n",
    "\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "####    Handling Complex Questions | Retrieving Column Information for SQL Generation\n",
    "####    This section retrieves relevant column information and metadata based on the user's prompt and the selected table.\n",
    "####    The data is filtered, matched, and prepared to generate an appropriate SQL query for complex user questions.\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "\n",
    "embedding_task_helper = EmbeddingTaskHelper(embedding_function, df_embeddings, column_dictionary, data_dictionary_dataframe, table_info_from_db, question_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableSelectionChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableSelectionChain:\n",
    "    def __init__(self, df_embeddings, embedding_handler_function, table_info_from_db, column_dictionary, data_dictionary_dataframe):\n",
    "        \"\"\"\n",
    "        Initializes the TableSelectionChain class with the necessary data.\n",
    "\n",
    "        Args:\n",
    "            embedding_handler (EmbeddingHandling): An instance of the EmbeddingHandling class.\n",
    "            table_info_from_db (pd.DataFrame): DataFrame containing information about the tables.\n",
    "            column_dictionary (dict): Dictionary mapping table names to their relevant columns.\n",
    "            data_dictionary_dataframe (pd.DataFrame): DataFrame containing column information for tables.\n",
    "        \"\"\"\n",
    "        self.df_embeddings = df_embeddings\n",
    "        self.embedding_handler_function = embedding_handler_function\n",
    "        self.table_info_from_db = table_info_from_db.copy()  # To avoid modifying the original DataFrame\n",
    "        self.column_dictionary = column_dictionary\n",
    "        self.data_dictionary_dataframe = data_dictionary_dataframe\n",
    "\n",
    "    def refined_langchain_table_selection_prompt(self, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Constructs a system message prompt for selecting the most appropriate table based on the user's query.\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The query from the user.\n",
    "\n",
    "        Returns:\n",
    "            str: The constructed prompt.\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        filtered_df, best_match_columns, embedded_df = [None, None, None]  # Placeholder\n",
    "\n",
    "        # Convert 'columns' from string representation of list to actual list\n",
    "        self.table_info_from_db['columns'] = self.table_info_from_db['columns'].apply(\n",
    "            lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \") if isinstance(x, str) else x\n",
    "        )\n",
    "\n",
    "        # Construct the table descriptions\n",
    "        tables_description_list = []\n",
    "        for _, row in self.table_info_from_db.iterrows():\n",
    "            table_name = row['table_name']\n",
    "            if table_name in self.column_dictionary:\n",
    "                filter_by_columns = self.column_dictionary[table_name]\n",
    "\n",
    "                # Use EmbeddingHandling to generate DataFrame from embeddings\n",
    "                filtered_df, best_match_columns, embedded_df = self.embedding_handler_function.generating_df_from_embeddings(\n",
    "                    user_prompt=user_prompt,\n",
    "                    df_embeddings=self.df_embeddings,\n",
    "                    list_of_columns=filter_by_columns,\n",
    "                    data_dictionary_dataframe=self.data_dictionary_dataframe\n",
    "                )\n",
    "\n",
    "                # Generate helper information for table selection (assuming the method exists elsewhere)\n",
    "                column_values_for_table_selection = self.embedding_handler_function.helper_function_for_table_selection(filtered_df)\n",
    "                specific_table_info = f\"\\t\\tTABLE_NAME: '{table_name}'\\n\"\n",
    "\n",
    "                if column_values_for_table_selection:\n",
    "                    table_description = (\n",
    "                        f\"\\n\\t- Information on {row['table_for']} can be found inside the table '{table_name}' \"\n",
    "                        f\"and most relevant columns to user's query are given below:\\n{specific_table_info}\\n\"\n",
    "                        f\"{column_values_for_table_selection}\"\n",
    "                    )\n",
    "                else:\n",
    "                    table_description = (\n",
    "                        f\"\\n\\t- Information on {row['table_for']} can be found inside the table '{table_name}' \"\n",
    "                        f\"with columns: {', '.join(row['columns'])}\"\n",
    "                    )\n",
    "            else:\n",
    "                table_description = (\n",
    "                    f\"\\n\\t- Information on {row['table_for']} can be found inside the table '{table_name}' \"\n",
    "                    f\"with columns: {', '.join(row['columns'])}\"\n",
    "                )\n",
    "            tables_description_list.append(table_description)\n",
    "\n",
    "        tables_description = \"\\n\".join(tables_description_list)\n",
    "\n",
    "        table_names = self.table_info_from_db['table_name'].tolist()\n",
    "        priority_list = ''\n",
    "        for i, table in enumerate(table_names):\n",
    "            priority_list += f'\\n\\t{i+1}: {table}'\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = (\n",
    "            \"You are an expert in identifying the most relevant table for a given question. Based on the provided table information, \"\n",
    "            \"your task is to choose the most appropriate table name that can answer the user's question:\\n\"\n",
    "            \"\\n## *Table Information*:\\n--------------------------------------------\\n\"\n",
    "            f\"{tables_description}\\n---------------------------------------------\\n\"\n",
    "            f\"\\n## Prioritize this table order for questions unrelated to cost, spend: {priority_list}\"\n",
    "            f\"\\n\\n-------------\\n** Special Instruction to follow **\\n-------------\\n\"\n",
    "            f\"\\n\\t- *For any question related to ras, robot-assisted procedures, non-robotic procedures, or robot usage comparisons, always use the table: {table_names[2]}\"\n",
    "            f\"\\n\\t- *For Questions related to UNSPSC, Contract Category, Manufacturer, Supplier, Manufacturer Part Number - Must select this table: {table_names[1]}\"\n",
    "            f\"\\n\\t- *For Questions related to 'Benchmark' (calculated for UNSPSC, Contract Category, Manufacturer, Supplier, Manufacturer Part Number) - Select this table: {table_names[1]}\"\n",
    "            f\"\\n\\t- *For Questions related to 'Benchmark' (calculated for other than UNSPSC, Contract Category, Manufacturer, Supplier, Manufacturer Part Number), 'Benchmark CPC', 'Percentile' - Select this table: {table_names[0]}\"\n",
    "            f\"\\n\\t- *For Questions related to 'Savings & Opportunities' (calculated for UNSPSC, Contract Category, Manufacturer, Supplier, Manufacturer Part Number) - Select this table: {table_names[1]}\"\n",
    "            f\"\\n\\t- *For Questions related to 'Savings & Opportunities'  (calculated for other than UNSPSC, Contract Category, Manufacturer, Supplier, Manufacturer Part Number)- Must Select this table: {table_names[0]}\"\n",
    "            f\"\\n\\t- *For Questions related to 'Price Parity' - Must Select this table: {table_names[1]}\"\n",
    "            \"\\n\\n## Return only the Full Table Name.\\n## Response Template:\\nTABLE_NAME\\n\"\n",
    "        )\n",
    "\n",
    "        return prompt.strip()\n",
    "\n",
    "    def generate_table_selection_chain(self, user_prompt: str):\n",
    "        \"\"\"\n",
    "        Creates a language model chain to select the most appropriate table based on the user's query.\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The query from the user.\n",
    "\n",
    "        Returns:\n",
    "            LLMChain: A chain object for selecting the appropriate table.\n",
    "        \"\"\"\n",
    "        # Initialize the language model\n",
    "        # llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        # llm = ChatOpenAI(model='gpt-4.1-mini', temperature=0)\n",
    "        # llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0)\n",
    "        llm = ChatOpenAI(model='gpt-5-mini', temperature=1)\n",
    "\n",
    "\n",
    "\n",
    "        # Generate the system message for table selection\n",
    "        system_message = self.refined_langchain_table_selection_prompt(user_prompt)\n",
    "\n",
    "        # Create prompt templates\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "            \"Provide the full table name that is most suitable to answer the human question: {human_prompt}\"\n",
    "        )\n",
    "\n",
    "        # Combine prompts into a chat prompt template\n",
    "        chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            system_message_prompt,\n",
    "            human_message_prompt\n",
    "        ])\n",
    "\n",
    "        # Create a basic single chain\n",
    "        table_selection_chain = chat_prompt_template | llm | StrOutputParser()\n",
    "\n",
    "        return table_selection_chain\n",
    "    \n",
    "    def get_table_name(self, user_prompt):\n",
    "        table_selection_chain = self.generate_table_selection_chain(user_prompt)\n",
    "        selected_table_text = table_selection_chain.invoke({'human_prompt': user_prompt}).replace('```','').strip()\n",
    "\n",
    "        return selected_table_text\n",
    "\n",
    "table_selection_object = TableSelectionChain(df_embeddings, embedding_task_helper, table_info_from_db, column_dictionary, data_dictionary_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################### ONLY FOR TESTING #################\n",
    "\n",
    "# # test_user_prompt = 'Whats the total procedure count in ras'\n",
    "# # test_user_prompt = \"whats the total number of Robotic cases in 2024, include only ASA ratings 1, 2 and 3\"\n",
    "# # test_user_prompt = 'Compare facilities based on cost per case for @primary procedure 02L73DK-OCCLUSION OF LAA WITH INTRALUM DEV, PERC APPROACH.'\n",
    "\n",
    "# print(f'user_prompt: {test_user_prompt}\\n')\n",
    "# # print(table_selection_object.refined_langchain_table_selection_prompt(test_user_prompt))\n",
    "# print('\\n----------------------------------------\\n')\n",
    "# print(table_selection_object.get_table_name(test_user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for run in range(5):\n",
    "#     print(table_selection_object.get_table_name(user_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemoryChainManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NOT IN USE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "#     \"\"\"In-memory implementation of chat message history.\"\"\"\n",
    "\n",
    "#     messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "#     def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "#         \"\"\"Add a list of messages to the store, maintaining a maximum of 12 messages.\"\"\"\n",
    "#         if len(self.messages) >= 12:\n",
    "#             self.messages = self.messages[2:]\n",
    "#         self.messages.extend(messages)\n",
    "\n",
    "#     def clear(self) -> None:\n",
    "#         \"\"\"Clear all messages from the history.\"\"\"\n",
    "#         self.messages = []\n",
    "\n",
    "#     def remove_last_two_if_human_ai(self) -> None:\n",
    "#         \"\"\"Remove the last two messages if they are a HumanMessage followed by an AIMessage.\"\"\"\n",
    "#         if len(self.messages) >= 2:\n",
    "#             if isinstance(self.messages[-2], HumanMessage) and isinstance(self.messages[-1], AIMessage):\n",
    "#                 self.messages = self.messages[:-2]\n",
    "\n",
    "# class MemoryChainManager:\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Initializes the MemoryChainManager with a store for session histories.\n",
    "#         \"\"\"\n",
    "#         self.store: Dict[str, InMemoryHistory] = {}\n",
    "\n",
    "#     def get_by_session_id(self, session_id: str) -> InMemoryHistory:\n",
    "#         \"\"\"\n",
    "#         Retrieves the message history for a given session ID, creating a new one if it doesn't exist.\n",
    "\n",
    "#         Args:\n",
    "#             session_id (str): The session ID of the user.\n",
    "\n",
    "#         Returns:\n",
    "#             InMemoryHistory: The message history associated with the session ID.\n",
    "#         \"\"\"\n",
    "#         if session_id not in self.store:\n",
    "#             self.store[session_id] = InMemoryHistory()\n",
    "#             print(f\"\\n===========\\n**New User Found: {session_id}.\\n\\nAvailable User Ids in the Store: {(' | ').join(list(self.store.keys()))}\\n===========\\n\")\n",
    "#         return self.store[session_id]\n",
    "\n",
    "#     def remove_user_by_session_id(self, session_id: str) -> dict:\n",
    "#         \"\"\"\n",
    "#         Removes a user and their message history from the store.\n",
    "\n",
    "#         Args:\n",
    "#             session_id (str): The session ID of the user to remove.\n",
    "\n",
    "#         Returns:\n",
    "#             dict: A dictionary containing the status and message of the operation.\n",
    "#         \"\"\"\n",
    "#         result = {}\n",
    "#         if session_id in self.store:\n",
    "#             del self.store[session_id]\n",
    "#             remaining_users = ', '.join(self.store.keys())\n",
    "#             result['status'] = 'success'\n",
    "#             result['message'] = f'User Removed: \"{session_id}\".'\n",
    "#             result['remaining_users_count'] = len(self.store)\n",
    "#             result['remaining_users'] = remaining_users\n",
    "#         else:\n",
    "#             remaining_users = ', '.join(self.store.keys())\n",
    "#             result['status'] = 'error'\n",
    "#             result['message'] = f'User Not Found: \"{session_id}\".'\n",
    "#             result['remaining_users_count'] = len(self.store)\n",
    "#             result['remaining_users'] = remaining_users\n",
    "#         return result\n",
    "\n",
    "#     def refinement_chain(self, user_prompt: str, user_id: str) -> str:\n",
    "#         \"\"\"\n",
    "#         Refines the user's prompt using memory to make it clear and ready for SQL generation.\n",
    "\n",
    "#         Args:\n",
    "#             user_prompt (str): The user's original query.\n",
    "#             user_id (str): The session ID of the user.\n",
    "\n",
    "#         Returns:\n",
    "#             str: The refined query.\n",
    "#         \"\"\"\n",
    "#         llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "\n",
    "#         today_date = datetime.now()\n",
    "#         formatted_date = today_date.strftime('%Y-%m-%d')\n",
    "\n",
    "#         refinement_system_message = f'''\n",
    "# You are a query refinement assistant. Your task is to refine user queries using relevant memory, ensuring they are clear, concise, and ready for SQL generation.\n",
    "\n",
    "# ### Instructions:\n",
    "# 1. Include only essential details from memory (e.g., {{time_period}}, {{entity}}, {{metric}}) to make the query precise and fully formed for SQL generation.\n",
    "# 2. Ensure that both the relevant parameter (e.g., {{metric}}) and the time period (e.g., {{time_period}}) are included in the refined query if available in memory.\n",
    "# 3. **If the query contains an ambiguous time reference (e.g., \"this month\", \"this year\") and no specific time period is available in memory, replace the ambiguous reference with today's date.** Use the following format:\n",
    "#     - Today's date is: {formatted_date}. It is the {today_date.day} day of {today_date.strftime('%B')} in the year {today_date.year}.\n",
    "# 4. **If no refinement is needed, return the query as is.**\n",
    "# 5. Provide only the refined query, without any explanation or description.\n",
    "# 6. If the user's query refers to information not covered in the memory, refine the query to clearly state the original intent without relying on memory details.\n",
    "# 7. **Output only the refined query**. Do not include any answers, explanations, or other text.\n",
    "\n",
    "# ### Output Format:\n",
    "# Only the refined query, formatted as:\n",
    "# {{refined_query}}\n",
    "# '''\n",
    "\n",
    "#         # Create the prompt template\n",
    "#         refinement_prompt = ChatPromptTemplate.from_messages([\n",
    "#             MessagesPlaceholder(variable_name=\"history\"),\n",
    "#             (\"system\", refinement_system_message),\n",
    "#             (\"human\", \"{user_prompt}\")\n",
    "#         ])\n",
    "\n",
    "#         # Create the chain with history management\n",
    "#         refinement_chain = refinement_prompt | llm\n",
    "#         refinement_chain_with_history = RunnableWithMessageHistory(\n",
    "#             refinement_chain,\n",
    "#             self.get_by_session_id,\n",
    "#             input_messages_key=\"user_prompt\",\n",
    "#             history_messages_key=\"history\"\n",
    "#         )\n",
    "\n",
    "#         # Invoke the chain\n",
    "#         refinement_chain_result = refinement_chain_with_history.invoke(\n",
    "#             {'user_prompt': user_prompt},\n",
    "#             config={\"configurable\": {\"session_id\": user_id}}\n",
    "#         ).content\n",
    "\n",
    "#         # Remove the last two messages if necessary\n",
    "#         self.store[user_id].remove_last_two_if_human_ai()\n",
    "\n",
    "#         return refinement_chain_result\n",
    "\n",
    "#     def decision_chain(self, user_prompt: str, user_id: str) -> str:\n",
    "#         \"\"\"\n",
    "#         Determines whether the user's query can be answered from memory.\n",
    "\n",
    "#         Args:\n",
    "#             user_prompt (str): The user's query.\n",
    "#             user_id (str): The session ID of the user.\n",
    "\n",
    "#         Returns:\n",
    "#             str: The answer from memory or '0' if not available.\n",
    "#         \"\"\"\n",
    "#         # llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "#         llm = ChatOpenAI(model='gpt-4.1-2025-04-14', temperature=0)\n",
    "        \n",
    "#         decision_system_message = '''\n",
    "# You are an assistant for the 'Supply Copia' company. Your task is to answer questions based on previous interactions and memory in a polite and human-readable tone.\n",
    "\n",
    "# ### Instructions:\n",
    "# 1. If the User's query is not available from past interactions or memory, respond with '0' only.\n",
    "# 2. If the User's query is available from past interactions or memory, provide the answer in a clear, human-readable format.\n",
    "# 3. If you are uncertain about the answer or cannot find relevant data, respond with '0' only.\n",
    "# 4. Do not generate or fabricate any information (no hallucinations). Always prioritize accuracy.\n",
    "# 5. If the required data is not retrievable from memory, always return '0' without adding any explanations or additional information.\n",
    "\n",
    "# ### Output Format:\n",
    "# - If the answer is available: Provide it in natural language.\n",
    "# - If not available: '0'\n",
    "# '''\n",
    "\n",
    "#         # Create the prompt template\n",
    "#         decision_prompt = ChatPromptTemplate.from_messages([\n",
    "#             MessagesPlaceholder(variable_name=\"history\"),\n",
    "#             (\"system\", decision_system_message),\n",
    "#             (\"human\", \"{question}\")\n",
    "#         ])\n",
    "\n",
    "#         # Create the chain with history management\n",
    "#         decision_chain = decision_prompt | llm\n",
    "#         decision_chain_with_history = RunnableWithMessageHistory(\n",
    "#             decision_chain,\n",
    "#             self.get_by_session_id,\n",
    "#             input_messages_key=\"question\",\n",
    "#             history_messages_key=\"history\"\n",
    "#         )\n",
    "\n",
    "#         # Invoke the chain\n",
    "#         decision_chain_result = decision_chain_with_history.invoke(\n",
    "#             {\"question\": user_prompt},\n",
    "#             config={\"configurable\": {\"session_id\": user_id}}\n",
    "#         ).content\n",
    "\n",
    "#         # If the result is '0', remove the last two messages\n",
    "#         if decision_chain_result == '0':\n",
    "#             self.store[user_id].remove_last_two_if_human_ai()\n",
    "\n",
    "#         return decision_chain_result\n",
    "\n",
    "#     def combined_chain(self, user_prompt: str, user_id: str) -> (str, str):\n",
    "#         \"\"\"\n",
    "#         Combines the decision and refinement chains to process the user's query.\n",
    "\n",
    "#         Args:\n",
    "#             user_prompt (str): The user's query.\n",
    "#             user_id (str): The session ID of the user.\n",
    "\n",
    "#         Returns:\n",
    "#             tuple:\n",
    "#                 - str: The refined query to be used.\n",
    "#                 - str or None: The result from memory if available, otherwise None.\n",
    "#         \"\"\"\n",
    "#         user_prompt_to_use = user_prompt\n",
    "#         result_from_memory = None\n",
    "\n",
    "#         # Check if there is any message history\n",
    "#         if self.store[user_id].messages:\n",
    "#             # Use the decision chain to check if the answer is available in memory\n",
    "#             decision_result = self.decision_chain(user_prompt, user_id)\n",
    "\n",
    "#             if decision_result == '0':\n",
    "#                 # If not available, refine the query\n",
    "#                 user_prompt_to_use = self.refinement_chain(user_prompt, user_id)\n",
    "#                 print(f'Refined Prompt: {user_prompt_to_use}\\n')\n",
    "#             else:\n",
    "#                 # If available, use the result from memory\n",
    "#                 result_from_memory = decision_result\n",
    "\n",
    "#         return user_prompt_to_use, result_from_memory\n",
    "\n",
    "# memory_manager = MemoryChainManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H: Whats the total cost monthwise for the year 2024? \n",
    "# A: List of month and total cost associated on each month \n",
    "\n",
    "# H: Whats the total cost found on June? \n",
    "# A: In June, the cost is .... \n",
    "\n",
    "\n",
    "# H: Whats the total number of encounters on the second month from the above list? \n",
    "# Intermediary Work: Whats the total number of encountesrs on Februrary, 2024? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [IN USE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        if len(self.messages) >= 12:\n",
    "            self.messages = self.messages[2:]\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages.clear()\n",
    "\n",
    "    def remove_last_two_if_human_ai(self) -> None:\n",
    "        if len(self.messages) >= 2:\n",
    "            if isinstance(self.messages[-2], HumanMessage) and isinstance(self.messages[-1], AIMessage):\n",
    "                self.messages = self.messages[:-2]\n",
    "\n",
    "\n",
    "class MemoryChainManager:\n",
    "    def __init__(self):\n",
    "        self.store: Dict[str, InMemoryHistory] = {}\n",
    "\n",
    "    def get_by_session_id(self, session_id: str) -> InMemoryHistory:\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = InMemoryHistory()\n",
    "            print(f\"\\n===========\\n**New User Found: {session_id}.\\n===========\\n\")\n",
    "        return self.store[session_id]\n",
    "    \n",
    "\n",
    "    def remove_user_by_session_id(self, session_id: str) -> dict:\n",
    "        \"\"\"\n",
    "        Removes a user and their message history from the store.\n",
    "\n",
    "        Args:\n",
    "            session_id (str): The session ID of the user to remove.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the status and message of the operation.\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        if session_id in self.store:\n",
    "            del self.store[session_id]\n",
    "            remaining_users = ', '.join(self.store.keys())\n",
    "            result['status'] = 'success'\n",
    "            result['message'] = f'User Removed: \"{session_id}\".'\n",
    "            result['remaining_users_count'] = len(self.store)\n",
    "            result['remaining_users'] = remaining_users\n",
    "        else:\n",
    "            remaining_users = ', '.join(self.store.keys())\n",
    "            result['status'] = 'error'\n",
    "            result['message'] = f'User Not Found: \"{session_id}\".'\n",
    "            result['remaining_users_count'] = len(self.store)\n",
    "            result['remaining_users'] = remaining_users\n",
    "        return result\n",
    "    \n",
    "    def get_all_histories_grouped(self) -> Dict[str, List[Dict[str, str]]]:\n",
    "        \"\"\"\n",
    "        Return all session_ids with their associated messages,\n",
    "        grouped so each human prompt and its AI reply are in the same dictionary element.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List[Dict[str, str]]]: {\n",
    "                \"session_id\": [\n",
    "                    {\"human\": \"prompt text\", \"ai\": \"response text\"},\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "        \"\"\"\n",
    "        all_histories = {}\n",
    "        for session_id, history in self.store.items():\n",
    "            grouped = []\n",
    "            temp_pair = {}\n",
    "            for msg in history.messages:\n",
    "                if isinstance(msg, HumanMessage):\n",
    "                    temp_pair = {\"human\": msg.content}\n",
    "                elif isinstance(msg, AIMessage):\n",
    "                    if \"human\" in temp_pair:\n",
    "                        temp_pair[\"ai\"] = msg.content\n",
    "                        grouped.append(temp_pair)\n",
    "                        temp_pair = {}\n",
    "            all_histories[session_id] = grouped\n",
    "        return all_histories\n",
    "\n",
    "    def extract_latest_explicit_date_from_history(self, user_id: str) -> Optional[str]:\n",
    "        history = self.store[user_id].messages\n",
    "        for msg in reversed(history):\n",
    "            if isinstance(msg, (HumanMessage, AIMessage)):\n",
    "                month_year_match = re.search(r\"(0[1-9]|1[0-2])-(20\\\\d{2})\", msg.content)\n",
    "                if month_year_match:\n",
    "                    return month_year_match.group(0)\n",
    "\n",
    "                named_month_match = re.search(\n",
    "                    r\"(January|February|March|April|May|June|July|August|September|October|November|December) (\\\\d{4})\",\n",
    "                    msg.content,\n",
    "                    re.IGNORECASE\n",
    "                )\n",
    "                if named_month_match:\n",
    "                    month = named_month_match.group(1)\n",
    "                    year = named_month_match.group(2)\n",
    "                    month_num = list(calendar.month_name).index(month.capitalize())\n",
    "                    return f\"{month_num:02}-{year}\"\n",
    "        return None\n",
    "\n",
    "    def extract_latest_entity_from_history(self, user_id: str, entity_keywords: List[str]) -> Optional[str]:\n",
    "        history = self.store[user_id].messages\n",
    "        for msg in reversed(history):\n",
    "            if isinstance(msg, (HumanMessage, AIMessage)):\n",
    "                for keyword in entity_keywords:\n",
    "                    match = re.search(fr\"{keyword}[:\\s-]*([A-Z ,.'\\-]+)\", msg.content, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        return match.group(1).strip()\n",
    "        return None\n",
    "\n",
    "    def refinement_chain(self, user_prompt: str, user_id: str) -> str:\n",
    "        llm = ChatOpenAI(model='gpt-4.1', temperature=0)\n",
    "\n",
    "        fallback_date = datetime.now().strftime('%m-%Y')\n",
    "        latest_date = self.extract_latest_explicit_date_from_history(user_id) or fallback_date\n",
    "        \n",
    "        refinement_prompt = ChatPromptTemplate.from_messages([\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"system\", f\"\"\"You are a query refinement assistant. Your job is to refine vague or incomplete user questions using historical context to form a complete, executable query for SQL generation.\n",
    "\n",
    "Guidelines:\n",
    "1. When the user uses vague time references like \\\"this month\\\", \\\"this year\\\", \\\"last quarter\\\", resolve them only if:\n",
    "   a. The question appears to involve a time-dependent metric (e.g., cost, trend, encounters).\n",
    "   b. AND there is a clear, recent time reference (e.g., \\\"October 2024\\\") in memory.\n",
    "\n",
    "2. DO NOT carry time references from previous memory if the current user query shifts focus to a new entity (e.g., a physician, market, or facility) — unless the time is explicitly stated again.\n",
    "\n",
    "3. When vague entity references like \\\"this physician\\\", \\\"this market\\\", or \\\"this facility\\\" are used, replace them using the most recent specific mention in memory:\n",
    "   - Example: \\\"this physician\\\" → \\\"ERTEM, FURKAN U\\\" if that was the last mentioned physician.\n",
    "\n",
    "4. For open-ended queries like:\n",
    "   - \\\"Which month had the highest encounters?\\\"\n",
    "   - \\\"Show supply cost trend over time\\\"\n",
    "   Do NOT inject any default or historical date — preserve the open nature.\n",
    "\n",
    "5. If no context is found and fallback is needed (e.g., \\\"this month\\\" without prior reference), use: {latest_date}\n",
    "\n",
    "6. If the input question is already fully formed, return it unchanged.\n",
    "\n",
    "7. Do NOT generate explanations, comments, or extra formatting. Only return the refined query.\n",
    "\n",
    "8. Clarify Entity Types: If a user mentions a specific value (e.g., a name or location) without specifying its entity type (like market, facility, physician, etc.), prepend the appropriate entity label based on context.\n",
    "Example: “What is the total cost for Horizon West?” → “What is the total cost for market Horizon West?”\n",
    "\n",
    "Output Format:\n",
    "refined_query\n",
    "\"\"\"),\n",
    "            (\"human\", \"{user_prompt}\")\n",
    "        ])\n",
    "\n",
    "        chain = refinement_prompt | llm\n",
    "        with_history = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            self.get_by_session_id,\n",
    "            input_messages_key=\"user_prompt\",\n",
    "            history_messages_key=\"history\"\n",
    "        )\n",
    "\n",
    "        result = with_history.invoke(\n",
    "            {\"user_prompt\": user_prompt},\n",
    "            config={\"configurable\": {\"session_id\": user_id}}\n",
    "        ).content\n",
    "\n",
    "        self.store[user_id].remove_last_two_if_human_ai()\n",
    "        return result\n",
    "\n",
    "    def decision_chain(self, user_prompt: str, user_id: str) -> str:\n",
    "        llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "        system_message = '''\n",
    "You are an assistant for the 'Supply Copia' company. Your task is to answer questions based on previous interactions and memory in a polite and human-readable tone.\n",
    "\n",
    "Instructions:\n",
    "1. If the User's query is not available from past interactions or memory, respond with '0' only.\n",
    "2. If the User's query is available from past interactions or memory, provide the answer in a clear, human-readable format.\n",
    "3. If you are uncertain about the answer or cannot find relevant data, respond with '0' only.\n",
    "4. Do not generate or fabricate any information (no hallucinations). Always prioritize accuracy.\n",
    "5. If the required data is not retrievable from memory, always return '0' without adding any explanations or additional information.\n",
    "\n",
    "Output Format:\n",
    "- If the answer is available: Provide it in natural language.\n",
    "- If not available: '0'\n",
    "'''\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\"system\", system_message),\n",
    "            (\"human\", \"{question}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | llm\n",
    "        with_history = RunnableWithMessageHistory(\n",
    "            chain,\n",
    "            self.get_by_session_id,\n",
    "            input_messages_key=\"question\",\n",
    "            history_messages_key=\"history\"\n",
    "        )\n",
    "\n",
    "        result = with_history.invoke(\n",
    "            {\"question\": user_prompt},\n",
    "            config={\"configurable\": {\"session_id\": user_id}}\n",
    "        ).content\n",
    "\n",
    "        if result == '0':\n",
    "            self.store[user_id].remove_last_two_if_human_ai()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def combined_chain(self, user_prompt: str, user_id: str):\n",
    "        user_prompt_to_use = user_prompt\n",
    "        result_from_memory = None\n",
    "\n",
    "        if self.store[user_id].messages:\n",
    "            decision = self.decision_chain(user_prompt, user_id)\n",
    "            if decision == '0':\n",
    "                user_prompt_to_use = self.refinement_chain(user_prompt, user_id)\n",
    "                print(f'[Refined Query]: {user_prompt_to_use}')\n",
    "            else:\n",
    "                result_from_memory = decision\n",
    "\n",
    "        return user_prompt_to_use, result_from_memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # >>>>>>>>>>>>>>>>>>>>>>>>  Memory Chain | combined_memory_chain_function <<<<<<<<<<<<<<<<<<<<<<<\n",
    "# user_prompt_to_use, result_from_memory = memory_manager.combined_chain(user_prompt, user_id)\n",
    "# prompt_to_save = user_prompt\n",
    "# if user_prompt != user_prompt_to_use:\n",
    "#     prompt_to_save = f\"{user_prompt} [REFINED QUERY: ({user_prompt_to_use})]\"\n",
    "# # # >>>>>>>>>>>>>>>>>>>>>>>>  Memory Chain | combined_memory_chain_function <<<<<<<<<<<<<<<<<<<<<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  QueryTypeClassifier | General or Specific ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTypeClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the QueryTypeClassifier with a language model.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "\n",
    "        self.decision_chain = self._create_decision_chain()\n",
    "        self.out_of_context_chain = self._create_out_of_context_chain()\n",
    "\n",
    "    def _create_decision_chain(self):\n",
    "        \"\"\"\n",
    "        Creates a decision-making chain to classify user queries.\n",
    "        \"\"\"\n",
    "        system_message = '''# System Instructions:\n",
    "This system evaluates the user's query and classifies it into one of two categories:\n",
    "\n",
    "1. General Knowledge Query (Return '1'):\n",
    "    - These queries seek conceptual understanding or general explanations.\n",
    "    - Typically answerable without access to structured or tabular data.\n",
    "    - Examples include: \"What is blood utilization?\" or \"Explain how cost-saving works in healthcare.\"\n",
    "\n",
    "2. Specific Data Query (Return '2'):\n",
    "    - These queries involve comparisons, trends, metrics, differences, aggregations, or numerical insights.\n",
    "    - Typically require access to structured data, such as databases, tables, or performance metrics.\n",
    "    - Keywords like \"how many\", \"difference\", \"compare\", \"utilization across\", \"average\", \"total\", or references to specific dimensions (e.g., by physician, by market, over time) indicate a specific data query.\n",
    "    - Questions asking *\"how something varies or differs across categories\"* (e.g., departments, service lines, markets, timeframes) are **always** specific data queries.\n",
    "\n",
    "# Special Notes:\n",
    "- If no question is provided or the query is clearly general knowledge, return '1'.\n",
    "- Default to '2' when there is any data-specific context, even if the question seems open-ended.\n",
    "\n",
    "# Immediate Instruction to LLM:\n",
    "- Analyze the query content precisely.\n",
    "- Return:\n",
    "    - '1' for general knowledge queries.\n",
    "    - '2' for specific data queries.\n",
    "'''\n",
    "\n",
    "        chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(system_message),\n",
    "            HumanMessagePromptTemplate.from_template(\"{user_query}\")\n",
    "        ])\n",
    "\n",
    "        return chat_prompt_template | self.llm | StrOutputParser()\n",
    "\n",
    "    def _create_out_of_context_chain(self):\n",
    "        \"\"\"\n",
    "        Creates a decision-making chain to detect out-of-context general knowledge queries.\n",
    "        \"\"\"\n",
    "        system_message = '''# Task:\n",
    "Evaluate whether the following general knowledge query (type '1') is still relevant to the same domain as our specific data queries.\n",
    "\n",
    "# Reference Domain:\n",
    "Our system handles queries related to healthcare operations, clinical procedures, outcomes, cost/utilization analysis, provider performance, and product/vendor data.\n",
    "\n",
    "# Response:\n",
    "- Return '1' → The question is general but still aligned with the above domain.\n",
    "- Return '0' → The question is unrelated (e.g., about food, space, jokes, or geography).\n",
    "\n",
    "Return only '0' or '1'. No explanation.\n",
    "'''\n",
    "\n",
    "        chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(system_message),\n",
    "            HumanMessagePromptTemplate.from_template(\"{user_query}\")\n",
    "        ])\n",
    "\n",
    "        return chat_prompt_template | self.llm | StrOutputParser()\n",
    "\n",
    "    def classify_query(self, user_query: str) -> str:\n",
    "        \"\"\"\n",
    "        Classifies the user query as either '1' (general knowledge) or '2' (specific data).\n",
    "        \"\"\"\n",
    "        result = self.decision_chain.invoke({\"user_query\": user_query})\n",
    "        return result.strip()\n",
    "\n",
    "    def is_out_of_context_general_knowledge(self, user_query: str) -> str:\n",
    "        \"\"\"\n",
    "        Identifies whether a general knowledge query is out of domain context.\n",
    "\n",
    "        Returns:\n",
    "            str: '0' if out of context, '1' if contextually relevant.\n",
    "        \"\"\"\n",
    "        result = self.out_of_context_chain.invoke({\"user_query\": user_query})\n",
    "        return result.strip()\n",
    "\n",
    "    def generate_out_of_scope_response_with_llm(self, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Uses the LLM to generate a polite, contextual response when a question is out of scope.\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The user's original query.\n",
    "\n",
    "        Returns:\n",
    "            str: A respectful, human-like message explaining that the query is out of scope.\n",
    "        \"\"\"\n",
    "        system_message = \"\"\"You are an assistant \"Ask the Bee\" of \"Supply Copia\" designed to answer questions related to healthcare analytics, cost, quality, clinical performance, and operational metrics.\n",
    "\n",
    "    If a user's question is out of scope (e.g., unrelated to healthcare, clinical data, or medical analysis), respond respectfully with a friendly message that:\n",
    "\n",
    "    1. Acknowledges the user’s query.\n",
    "    2. Explains briefly and kindly that it’s outside the assistant’s domain.\n",
    "    3. Encourages them to ask something within the assistant’s expertise.\n",
    "\n",
    "    Do not mention the word “out of scope” or say \"I cannot help with that\" in a negative tone.\n",
    "    Do not hallucinate an answer to the actual query.\n",
    "    Just stay polite and helpful.\n",
    "    \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(system_message),\n",
    "            HumanMessagePromptTemplate.from_template(\"{user_prompt}\")\n",
    "        ])\n",
    "\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        return chain.invoke({\"user_prompt\": user_prompt}).strip()\n",
    "\n",
    "query_type_classifier = QueryTypeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_type = query_type_classifier.classify_query(user_prompt_to_use)\n",
    "# print(f'{query_type}: {user_prompt_to_use}')\n",
    "\n",
    "# # If its a general question, then identify if its out of context or not\n",
    "# if query_type == '1':\n",
    "#     context_check = query_type_classifier.is_out_of_context_general_knowledge(query)\n",
    "#     print(\"Out of Context Check:\", context_check)\n",
    "\n",
    "#     if context_check == '0':\n",
    "#         response = query_type_classifier.generate_out_of_scope_response_with_llm(user_prompt_to_use)\n",
    "#         print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnalysisChainManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "#### Analysis Chain - To Analyze the Data | Analysis Chain - To Analyze the Data\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "\n",
    "class AnalysisChainManager:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "\n",
    "    def generate_analysis_chain_system_message(self, question, df):\n",
    "        \"\"\"\n",
    "        Creates a language model chain to analyze data based on natural language requests, providing insights in a clear and engaging manner.\n",
    "\n",
    "        Returns:\n",
    "            LLMChain: A chain object for conducting data analysis and generating insights based on the user's query and provided data.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(df) > 100:\n",
    "            display_size_note = f\"**Note: The Dataframe includes the first 50 entries out of a total of {len(df)} rows. Let the user know about it.\"\n",
    "        else:\n",
    "            display_size_note = \"\"\n",
    "\n",
    "        system_message_prompt = f'''You are an AI Chatbot for Supplycopia, experienced in data analysis. Your task is to interpret natural language requests and provide precise **data analysis** based on the given User's Question, and Dataframe given below.\n",
    "\n",
    "## For straightforward data, highlight key findings succinctly.\n",
    "## For complex data, conduct an in-depth analysis:\n",
    "    - Identify trends and patterns in detail, ensuring that any monthly trends are presented in proper chronological order.\n",
    "    - Detect and discuss any outliers with thorough explanations.\n",
    "    - If a specific month is mentioned, present it in full format (e.g., \"July 2023\" instead of \"7th month\") and provide the most detailed possible analysis relevant to that month.\n",
    "## Communicate insights in clear, simple language suitable for non-experts.\n",
    "\n",
    "{display_size_note}\n",
    "\n",
    "# Tone of Response:\n",
    "    - Friendly, professional, engaging, clear, and knowledgeable.\n",
    "\n",
    "------------\n",
    "USER'S QUESTION: {question}\n",
    "------------\n",
    "DataFrame: {df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "------------\n",
    "'''\n",
    "        return system_message_prompt\n",
    "        \n",
    "    def run_analysis_chain(self, question, df):\n",
    "        system_message_prompt = self.generate_analysis_chain_system_message(question, df)\n",
    "        chat_prompt_template = PromptTemplate.from_template(system_message_prompt)\n",
    "        analysis_chain = chat_prompt_template | self.llm | StrOutputParser()\n",
    "        query_response = analysis_chain.invoke({})\n",
    "        query_response = query_response.strip()\n",
    "        return query_response\n",
    "    \n",
    "analysis_chain_manager = AnalysisChainManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_chain_response = analysis_chain_manager.run_analysis_chain(user_prompt_to_use, df)\n",
    "# print(analysis_chain_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: moccasin; font-weight: BOLD;\">SQLQueryGenerator | MultiStageSQLGenerator</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: SQLQueryGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLQueryGenerator:\n",
    "    def __init__(self, connection_pool, table_info_from_db, embedding_task_helper):\n",
    "        \"\"\"\n",
    "        Initializes the SQLQueryGenerator with the specified Redshift table and language model.\n",
    "\n",
    "        Args:\n",
    "            redshift_table (str): The name of the Redshift SQL table to query.\n",
    "            model_name (str, optional): The name of the language model to use. Default is 'gpt-4o-2024-08-06'.\n",
    "        \"\"\"\n",
    "        # self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.llm = ChatOpenAI(model='gpt-4.1-2025-04-14', temperature=0)\n",
    "\n",
    "\n",
    "        ########## ------------- #############\n",
    "        # self.llm_for_correction = ChatOpenAI(model='gpt-4o-mini', temperature=0) \n",
    "        self.llm_for_correction = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        ########## ------------- #############\n",
    "\n",
    "        self.connection_pool = connection_pool\n",
    "        self.embedding_task_helper = embedding_task_helper\n",
    "\n",
    "        ## Identifying the Product level table for further use\n",
    "        self.product_level_table = table_info_from_db[table_info_from_db['table_for']=='Product Level Utilization Data']['table_name'].values[0]\n",
    "    \n",
    "    # ------------------------------------- Finding Correct Sql Query -------------------------------------------------- # \n",
    "    def generate_prompt_for_sql_correction(self, user_query: str, sql_query: str, sql_error_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a prompt for the LLM to correct and refine the SQL query.\n",
    "\n",
    "        Args:\n",
    "            user_query (str): The user's input query.\n",
    "            sql_query (str): The initial SQL query.\n",
    "            sql_error_message (str): The error message encountered during SQL query execution.\n",
    "\n",
    "        Returns:\n",
    "            str: The prompt for the LLM.\n",
    "        \"\"\"\n",
    "    \n",
    "        prompt = f\"\"\"You are an expert SQL assistant. Your task is to correct the provided SQL query based on the error message, user's intent, and SQL best practices. Specifically:\n",
    "\n",
    "**Instructions:**\n",
    "    - Analyze the provided SQL query and error message to identify the root cause.\n",
    "    - Correct the query to fix all technical issues while maintaining SQL best practices.\n",
    "    - Return only the corrected SQL query, formatted as code, and nothing else.\n",
    "\n",
    "Example of issues you must fix:\n",
    "    - Missing `GROUP BY` for non-aggregated columns in the `SELECT` clause.\n",
    "    - Proper use of aliases, subqueries, and aggregate functions.\n",
    "    - Syntax errors like division by zero risks, invalid column names, or casing issues.\n",
    "\n",
    "-----------------------------\n",
    "User Query: {user_query}\n",
    "\n",
    "Incorrect SQL Query:\n",
    "\n",
    "```\n",
    "{sql_query}\n",
    "```\n",
    "\n",
    "Error Message: {sql_error_message}\n",
    "-----------------------------\n",
    "\n",
    "## Analyze the error message and correct the faulty sql query given.\n",
    "## Prioritize identifying and fixing technical errors over stylistic corrections.\n",
    "\"\"\"\n",
    "        \n",
    "        return prompt.strip()\n",
    "\n",
    "    def correct_sql_query(self, user_query, sql_query: str, sql_error_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Corrects the SQL query using the LLM.\n",
    "\n",
    "        Args:\n",
    "            user_query (str): The user's input query.\n",
    "            sql_query (str): The initial SQL query.\n",
    "            sql_error_message (str): The error message encountered during SQL query execution.\n",
    "\n",
    "        Returns:\n",
    "            str: The corrected SQL query.\n",
    "        \"\"\"\n",
    "        # Create the system message prompt\n",
    "        system_message = self.generate_prompt_for_sql_correction(user_query, sql_query, sql_error_message)\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "\n",
    "        # Create the human message prompt to include the user query\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(\"Correct the SQL query.\")\n",
    "\n",
    "        # Combine system and human prompts into a chat prompt template\n",
    "        chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            system_message_prompt,\n",
    "            human_message_prompt\n",
    "        ])\n",
    "\n",
    "        response_chain = chat_prompt_template | self.llm_for_correction | StrOutputParser()\n",
    "        # Get the corrected SQL query\n",
    "        corrected_query = response_chain.invoke({})\n",
    "        corrected_query = corrected_query.replace('```sql','').replace('```','').strip()\n",
    "        return corrected_query.strip()\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------- # \n",
    "\n",
    "\n",
    "    def _create_table_definition_prompt(self, user_prompt, column_list, redshift_table) -> str:\n",
    "        \"\"\"\n",
    "        Generates a prompt for GPT to create SQL queries for the specified Redshift table,\n",
    "        focusing only on the given columns.\n",
    "\n",
    "        Args:\n",
    "            column_list (list or str): A list of column names or a string of column names to be included in the SQL query.\n",
    "\n",
    "        Returns:\n",
    "            str: A string prompt that details the requirements for generating SQL queries with specific constraints.\n",
    "        \"\"\"\n",
    "        # Get today's date\n",
    "        today_date = datetime.now()\n",
    "        # Format the date as YYYY-MM-DD\n",
    "        formatted_date = today_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Format the columns\n",
    "        if isinstance(column_list, str):\n",
    "            columns_formatted = column_list\n",
    "        else:\n",
    "            columns_formatted = \", \".join(column_list)\n",
    "      \n",
    "        ######################## grouping_instruction - P-EVENT Level ########################\n",
    "        grouping_instruction = ''\n",
    "        if redshift_table == self.product_level_table:\n",
    "            grouping_instruction = f\"\"\"\\n## Grouping Requirements for the `{self.product_level_table}` Table\n",
    "            \n",
    "- Mandatory Grouping:\n",
    "    - ***Always group by `emr_p_event` when calculating Outcome and Quality metrics, including:\n",
    "        - Outcome Score\n",
    "        - Length of Stay\n",
    "        - Geometric Mean Length of Stay\n",
    "        - Patient Age Bucket\n",
    "        - Cost Per Case\n",
    "        - Implant Cost Per Case\n",
    "        - Medical Supply Cost Per Case\n",
    "------------------------------------------------------------------------\\n\"\"\"\n",
    "        # ======================================================================== # \n",
    "\n",
    "        ######################## System Benchmark Context - Using: embedding_task_helper########################\n",
    "        system_benchmark_context_to_add = ''\n",
    "        if self.embedding_task_helper.check_prompt_conditions_for_system_benchmark(user_prompt):\n",
    "            system_benchmark_context_to_add = \"\"\"\\n## System Benchmark Context:\n",
    "- When the query includes \"System Benchmark\", special rules apply to column filters.\n",
    "\n",
    "### Rules:\n",
    "1. **Do NOT apply WHERE conditions** to specific columns if **\"System Benchmark\"** is present.\n",
    "2. **Excluded Columns**:\n",
    "    - Facility, *Market, Account Number, Age, ASA Score, BMI Bucket, Detailed Patient Type, Diabetic Status, Patient Ethnicity, Patient Gender, Patient Type, Physician, Payor Group, Smoking Status, SSI Flag, Mortality Flag, Readmission Flag, Blood Transfusion Flag, Product Name, Distributor, Contract Type, Outlier Classification.\n",
    "\n",
    "### **Example Behavior**:\n",
    "- User Query: \"What are the System Benchmark total encounters across top 10 UNSPSC in @Market X and @Primary Procedure 'Y'?\"\n",
    "- Expected SQL Behavior: Do not apply WHERE filters on the excluded columns listed above, even if other conditions like UNSPSC or Market are present.\n",
    "\n",
    "### Key Notes:\n",
    "- Always prioritize the \"System Benchmark\" context and leave the excluded columns unfiltered.\n",
    "\n",
    "------------------------------------------------------------------------\\n\"\"\"\n",
    "\n",
    "        # print(f'system_benchmark_context_to_add:\\n {system_benchmark_context_to_add}\\n')\n",
    "         # ======================================================================== # \n",
    "\n",
    "\n",
    "         ## Time Related Prompt\n",
    "        time_related_prompt = ''\n",
    "        if 'time' in user_prompt.lower() or 'trend' in user_prompt.lower() or 'month' in user_prompt.lower() or 'year' in user_prompt.lower() or 'quarter' in user_prompt.lower():\n",
    "            time_related_prompt = '''- Interpret 'spend over time' as a **monthly spend trend**.'''\n",
    "\n",
    "        prompt = f'''\n",
    "Craft SQL queries for the \"{redshift_table}\" table in Redshift, following these instructions:\n",
    "- Only use these columns for crafting the SQL query: {columns_formatted}.\n",
    "- Begin each query with \"SELECT\".\n",
    "- *Response should only contain the SQL query.\n",
    "- Include a \"WHERE\" clause for specific data retrieval.\n",
    "    -- *Maintain the exact format of user-provided strings within the \"WHERE\" condition without altering or adding spaces, punctuation, or abbreviations.\n",
    "- Today's date is: {formatted_date}. It's the {today_date.day} day of the month '{today_date.strftime('%B')}' in the year {today_date.year}.\n",
    "- If the user's query does not specify or hint at a date or year, there is no need to add date filters inside the SQL query.\n",
    "\n",
    "- Dates are given inside the columns: 'emr_month_yr', 'emr_discharge_date'.\n",
    "    - emr_discharge_date: YYYY-MM-DD (e.g., 2023-05-14, use LIKE '2023-%')\n",
    "    - emr_month_yr: MM-YYYY (e.g., 05-2023, use LIKE '%-2023' for year filtering)\n",
    "    - Use the LIKE % operator while dealing with the dates.\n",
    "    \n",
    "    - **Grouping Date Filters**:\n",
    "        - Always **group date filters properly** using parentheses when asked \"month-wise breakdown\", \"trend over time\", \"monthly CPC in 2023\" or similar to ensure correct logical grouping with other conditions in the `WHERE` clause.\n",
    "            - Example:\n",
    "                ```sql\n",
    "                WHERE \n",
    "                    (emr_discharge_date LIKE '2023-%' AND emr_discharge_date >= '2023-11-01')\n",
    "                    OR (emr_discharge_date LIKE '2024-%' AND emr_discharge_date <= '2024-10-31')\n",
    "                ```\n",
    "        - This ensures that date conditions are applied together and evaluated accurately with other filters.\n",
    "        \n",
    "        {time_related_prompt}\n",
    "\n",
    "- Use the LIKE % operator to search for substrings within a column.\n",
    "- Must use SUM(), COUNT(), AVG(), MIN(), MAX() functions appropriately while handling queries about: Cost, Charge, Spend, Score, Revenue, etc.\n",
    "- Important Note: Ensure case insensitivity within the 'WHERE' condition by converting all string columns to lowercase before performing comparisons.\n",
    "\n",
    "- Include an \"ORDER BY\" clause in the SQL query to ensure the results are sorted as follows:\n",
    "    - Case 1: If the question involves only one parameter, sort the response in descending order of that parameter.\n",
    "    - Case 2: If the question involves multiple parameters and includes either total encounter or total spend or CPC, sort the response in descending order of total encounter or total spend or cost per case (CPC).\n",
    "    - Case 3: If the question involves multiple parameters without total encounter or total spend or CPC, sort the response in descending order of the first parameter mentioned in the question.\n",
    "    - Case 4: If the question involves Date parameter, return results sorted by Date in ascending order.\n",
    "    - Case 5: Always add NULLS LAST in the ORDER BY clause to push nulls to the bottom.\n",
    "\n",
    "- **Ensure all fields referenced in the outer query**:\n",
    "    - Include in the inner query’s `SELECT` statement any field used in the outer query’s `SELECT`, `WHERE`, `CASE`, or `GROUP BY` clauses.\n",
    "    - This includes fields required for calculations (e.g., `AVG`, `SUM`), filters, or aggregations to avoid errors like \"column does not exist.\"\n",
    "\n",
    "- When aggregating data, construct multilevel aggregation queries. This involves nested queries that perform calculations at multiple levels.\n",
    "- Only provide the SQL query.\n",
    "- End the SQL query with a semicolon (;).\n",
    "- Do not add any explanation before or after the SQL query response.\n",
    "\n",
    "Example Response Format:\n",
    "```\n",
    "SELECT \n",
    "    col1, \n",
    "    (SUM(col3) / COUNT(col3)) * 100 AS percentage\n",
    "FROM \n",
    "    (\n",
    "        SELECT \n",
    "            col1, \n",
    "            AVG(col3) AS col3\n",
    "        FROM \n",
    "            {redshift_table}\n",
    "        WHERE \n",
    "            LOWER(col4) = LOWER(<value provided by the user>) \n",
    "            AND LOWER(col5) = LOWER(<value provided by the user>)\n",
    "        GROUP BY \n",
    "            col1\n",
    "    ) subquery\n",
    "GROUP BY \n",
    "    col1\n",
    "ORDER BY \n",
    "    percentage DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "- Construct a SQL query adhering to these guidelines.\n",
    "- Generate multilevel aggregation SQL queries like the example above to make the SQL query more usable.\n",
    "\n",
    "{grouping_instruction}\n",
    "{system_benchmark_context_to_add}\n",
    "\n",
    "📌 **Date Range Conversion (Mandatory)**  \n",
    "    - ALWAYS include date range columns in every query:  \n",
    "    - For `emr_discharge_date`, ALWAYS convert using:\n",
    "        TO_CHAR(MIN(TO_DATE(emr_discharge_date, 'YYYY-MM-DD')), 'YYYY-MM-DD') AS start_date,\n",
    "        TO_CHAR(MAX(TO_DATE(emr_discharge_date, 'YYYY-MM-DD')), 'YYYY-MM-DD') AS end_date  \n",
    "    - This conversion MUST be applied even if the user does not explicitly request the date range.  \n",
    "\n",
    "- If you have used multilevel aggregation query, make sure \"PROPER AGGREGATION\" IS BEING USED.\n",
    "- Use WHERE to filter rows before aggregation.\n",
    "- Use HAVING to filter aggregated results or groups after aggregation.\n",
    "\n",
    "✅ **For comparison-based questions** (e.g., comparing metrics between different markets, service lines, physicians, facilities, etc.):  \n",
    "- You **must** include the dimension being compared in the final output of the `SELECT` clause.  \n",
    "- This **must** be done even if aggregation is performed at a different level.  \n",
    "- The compared dimension **must** be clearly visible in the final result to label and distinguish each row properly.  \n",
    "- You **must not** omit the compared dimension from the output — doing so will result in incomplete or misleading answers.  \n",
    "- This rule **must always be followed**. No exceptions.\n",
    "\n",
    "    ### ✅ Examples of comparison-based questions:\n",
    "    - Compare the top 10 procedures by average cost for two different markets in 2024  \n",
    "    - Compare average case costs across service lines  \n",
    "    - Compare conversion rates between facilities over time\n",
    "'''\n",
    "        \n",
    "        return prompt.strip()\n",
    "\n",
    "    def generate_sql_chain(self, user_prompt, column_list, redshift_table):\n",
    "        \"\"\"\n",
    "        Creates a language model chain to generate SQL queries based on the provided columns.\n",
    "\n",
    "        Args:\n",
    "            column_list (list or str): A list of column names or a string of column names to be included in the SQL query.\n",
    "\n",
    "        Returns:\n",
    "            Chain: A chain object for generating SQL queries.\n",
    "        \"\"\"\n",
    "        # Generate the system prompt\n",
    "        system_message = self._create_table_definition_prompt(user_prompt, column_list, redshift_table)\n",
    "\n",
    "        # Create system message prompt template\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "\n",
    "        # Human prompt template\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "            \"A query to answer:\\n{human_prompt}\"\n",
    "        )\n",
    "\n",
    "        # Combine prompts into a chat prompt template\n",
    "        chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            system_message_prompt,\n",
    "            human_message_prompt\n",
    "        ])\n",
    "\n",
    "        # Create the chain\n",
    "        sql_chain = chat_prompt_template | self.llm | StrOutputParser()\n",
    "\n",
    "        return sql_chain\n",
    "\n",
    "    def extract_sql_query_from_string(self, query_string):\n",
    "        # Regular expression to find the start of the SQL query\n",
    "        sql_pattern = re.compile(r\"(?is)(?<!\\S)(SELECT|WITH)\\b.*\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        # Search for the SQL query using regex (start point)\n",
    "        match = sql_pattern.search(query_string)\n",
    "        if not match:\n",
    "            return None\n",
    "\n",
    "        # Get the query from the start\n",
    "        query_start = match.start()\n",
    "\n",
    "        # Find the index of the last semicolon in the query string\n",
    "        last_semicolon_index = query_string.rfind(';')\n",
    "\n",
    "        # If no semicolon is found, return None (invalid query)\n",
    "        if last_semicolon_index == -1:\n",
    "            return None\n",
    "\n",
    "        # Extract the query from the start of the match to the last semicolon\n",
    "        sql_query = query_string[query_start:last_semicolon_index + 1].strip()\n",
    "\n",
    "        return sql_query \n",
    "    \n",
    "    def get_dataframe_from_sql(self, generated_sql_query):\n",
    "        \"\"\"\n",
    "        Executes a given SQL query and returns the results as a pandas DataFrame.\n",
    "\n",
    "        Args:\n",
    "            generated_sql_query (str): The SQL query to execute.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A pandas DataFrame containing the query results and an error message (if any).\n",
    "        \"\"\"\n",
    "        # List of allowed aggregation functions and query clauses\n",
    "        aggregation_functions_list = [\"SUM\", \"COUNT\", \"AVG\", \"MIN\", \"MAX\", \"DISTINCT\"]\n",
    "        query_upper = generated_sql_query.upper()\n",
    "        error = None\n",
    "\n",
    "        # Validate the query before execution\n",
    "        if (any(func in query_upper for func in aggregation_functions_list) or \n",
    "            \"WHERE\" in query_upper or \"LIMIT\" in query_upper) and \\\n",
    "            (query_upper.startswith(\"SELECT\") or query_upper.startswith(\"WITH\") or query_upper.startswith(\"-- \")):\n",
    "            \n",
    "            df = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "            conn = None\n",
    "\n",
    "            try:\n",
    "                # Get a connection from the pool\n",
    "                conn = self.connection_pool.getconn()\n",
    "\n",
    "                # Execute the query\n",
    "                with conn.cursor() as cursor:\n",
    "                    cursor.execute(generated_sql_query)\n",
    "                    data = cursor.fetchall()\n",
    "                    columns = [desc[0] for desc in cursor.description]  # Fetch column names\n",
    "\n",
    "                # Create DataFrame from the query results\n",
    "                df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "            except Exception as e:\n",
    "                error = str(e)\n",
    "                # Attempt reconnection and re-execution\n",
    "                try:\n",
    "                    if conn:\n",
    "                        self.connection_pool.putconn(conn)  # Return the connection to the pool\n",
    "                    conn = self.connection_pool.getconn()  # Get a new connection\n",
    "                    with conn.cursor() as cursor:\n",
    "                        cursor.execute(generated_sql_query)\n",
    "                        data = cursor.fetchall()\n",
    "                        columns = [desc[0] for desc in cursor.description]  # Fetch column names\n",
    "\n",
    "                    # Create DataFrame from the query results\n",
    "                    df = pd.DataFrame(data, columns=columns)\n",
    "                    error = None  # Clear the error if reconnection is successful\n",
    "                except Exception as reconnect_error:\n",
    "                    error = str(reconnect_error)\n",
    "                    # print(f\"Error after reconnecting: {error}\")\n",
    "\n",
    "            finally:\n",
    "                # Ensure the connection is returned to the pool\n",
    "                if conn:\n",
    "                    self.connection_pool.putconn(conn)\n",
    "        else:\n",
    "            error = \"Invalid or unsupported SQL query.\"\n",
    "            df = pd.DataFrame()  # Return an empty DataFrame for invalid queries\n",
    "\n",
    "        # if error:\n",
    "        #     print(f\"Error during query execution: {error}\")\n",
    "        return df, error\n",
    "\n",
    "    def get_sql_query(self, user_prompt, column_list, redshift_table):\n",
    "\n",
    "        ## Generating Sql Chain\n",
    "        sql_chain = self.generate_sql_chain(user_prompt, column_list, redshift_table)\n",
    "        \n",
    "        ## Running the SQL Chain\n",
    "        result = sql_chain.invoke({'human_prompt': user_prompt})\n",
    "        sql_chain_query = result.replace('```sql','').replace('```','').replace('`', '').strip()\n",
    "        sql_chain_query = self.extract_sql_query_from_string(sql_chain_query)\n",
    "\n",
    "        return sql_chain_query\n",
    "    \n",
    "    def get_result_from_sql_query(\n",
    "        self,\n",
    "        user_prompt = None,\n",
    "        column_list = None,\n",
    "        redshift_table = None,\n",
    "        generated_sql_query = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns results from a SQL query using one of two modes:\n",
    "\n",
    "        Mode 1: Dynamic SQL Generation Mode\n",
    "            - Required Inputs: user_prompt, column_list, redshift_table\n",
    "            - If 'generated_sql_query' is not provided, it will be created using get_sql_query()\n",
    "        \n",
    "        Mode 2: Direct SQL Execution Mode\n",
    "            - Required Input: generated_sql_query\n",
    "            - Directly uses the provided SQL to fetch results\n",
    "        \n",
    "        Returns:\n",
    "            Final results from executing the SQL query.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If required parameters are missing for either mode.\n",
    "        \"\"\"\n",
    "        \n",
    "        if generated_sql_query is None:\n",
    "            # Mode 1: Dynamic SQL generation from user prompt\n",
    "            if not all([user_prompt, column_list, redshift_table]):\n",
    "                raise ValueError(\"To generate SQL, you must provide user_prompt, column_list, and redshift_table.\")\n",
    "            \n",
    "            print(\"[MODE 1:] No SQL provided. Generating SQL from prompt, column list, and table...\")\n",
    "            generated_sql_query = self.get_sql_query(\n",
    "                user_prompt=user_prompt,\n",
    "                column_list=column_list,\n",
    "                redshift_table=redshift_table\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # Mode 2: Use the provided SQL query directly\n",
    "            print(\"[MODE 2:] Using the provided SQL query.\")\n",
    "\n",
    "\n",
    "        # Run to get the dataframe\n",
    "        df, sql_error_message = self.get_dataframe_from_sql(generated_sql_query)\n",
    "\n",
    "        # Handle SQL errors with a retry mechanism\n",
    "        if sql_error_message:\n",
    "            print(f\"--- Initial SQL Error Encountered ---\\nError Message: {sql_error_message}\\nFaulty Query: {generated_sql_query}\\n──────────────────────────────────────────────────────────────────────────────────────────\\n\")\n",
    "            \n",
    "            input_error_message = sql_error_message\n",
    "            input_error_query = generated_sql_query\n",
    "\n",
    "            for attempt, _ in enumerate(range(4)):  # Retry up to 3 times\n",
    "                corrected_query = self.correct_sql_query(user_prompt, str(input_error_query), input_error_message)\n",
    "                df, new_error_message = self.get_dataframe_from_sql(corrected_query)\n",
    "\n",
    "                if new_error_message:\n",
    "                    input_error_query = corrected_query\n",
    "                    input_error_message = new_error_message\n",
    "                    print(f\"Attempt {attempt + 1} Failed:\\nError Message: {new_error_message}\\nFaulty Query: {input_error_query}\\n──────────────────────────────────────────────────────────────────────────────────────────\\n\")\n",
    "                else:\n",
    "                    generated_sql_query = corrected_query\n",
    "                    print(f\"--- Success on Attempt {attempt + 1} ---\\nCorrected Query Executed Successfully: {generated_sql_query}\\n──────────────────────────────────────────────────────────────────────────────────────────\\n\")\n",
    "                    break  # Exit the loop if no errors\n",
    "        \n",
    "        ## Rename Columns \n",
    "        if not df.empty and len(df)>100:\n",
    "            df.columns = (\n",
    "                df.columns\n",
    "                .str.replace(r\"^emr_\", \"\", regex=True)   # remove 'emr_' prefix\n",
    "                .str.replace(\"_\", \" \")                  # replace underscores with space\n",
    "                .str.title()                            # capitalize each word\n",
    "            )\n",
    "        return generated_sql_query, df\n",
    "\n",
    "## SQL QUERY GENERATOR - OBJECT\n",
    "sql_query_generator = SQLQueryGenerator(connection_pool, table_info_from_db, embedding_task_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# Inner Approach ####################\n",
    "# generated_sql_query = sql_query_generator.get_sql_query(user_prompt_to_use, columns_info_to_generate_sql, table_to_use)\n",
    "# print(generated_sql_query)\n",
    "# ################# #################### ####################\n",
    "\n",
    "# # generated_sql_query, df = sql_query_generator.get_result_from_sql_query(user_prompt_to_use, columns_info_to_generate_sql, table_to_use)\n",
    "# # df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(user_prompt_to_use)\n",
    "# print(generated_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: MultiStageSQLGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Class: MultiStageSQLGenerator [NOT IN USE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiStageSQLGenerator:\n",
    "#     def __init__(self):\n",
    "#         self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "#         self.parser = StrOutputParser()\n",
    "\n",
    "#     def prompt_chain_step1(self, user_prompt: str, schema: Dict[str, list], \n",
    "#                         additional_context: str = \"\", context_type: str = \"general\") -> str:\n",
    "#         \"\"\"\n",
    "#         First step: Generate initial SQL query using table schema and focused context.\n",
    "        \n",
    "#         Args:\n",
    "#             user_prompt: Natural language query from user\n",
    "#             schema: Dictionary with table_name and columns list\n",
    "#             additional_context: Optional specialized context (data dictionary, domain knowledge)\n",
    "#             context_type: Type of context provided (\"general\", \"domain_knowledge\", \"data_dictionary\", \"complex_instruction\")\n",
    "        \n",
    "#         Returns:\n",
    "#             Initial SQL query\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Format context based on type for better interpretation\n",
    "#         context_section = \"\"\n",
    "#         if additional_context:\n",
    "#             context_title = context_type.replace('_', ' ').title()\n",
    "#             context_section = f\"\\n\\n### {context_title}:\\n{additional_context.strip()}\"\n",
    "        \n",
    "#         # Customize system message based on context type\n",
    "#         if context_type == \"data_dictionary\":\n",
    "#             system = \"\"\"You are an SQL translator specialized in data dictionary interpretation.\n",
    "#     Focus exclusively on using the provided data dictionary to create precise column mappings.\n",
    "#     Return only the SQL query with no explanations.\"\"\"\n",
    "#         elif context_type == \"domain_knowledge\":\n",
    "#             system = \"\"\"You are a domain expert SQL generator.\n",
    "#     Apply the provided domain knowledge to create contextually appropriate filters and joins.\n",
    "#     Return only the SQL query with no explanations.\"\"\"\n",
    "#         elif context_type == \"complex_instruction\":\n",
    "#             system = \"\"\"You are an advanced SQL engineer specializing in complex query patterns.\n",
    "#     Apply the provided instructions to structure the query appropriately.\n",
    "#     Return only the SQL query with no explanations.\"\"\"\n",
    "#         else:\n",
    "#             system = \"\"\"You are a basic SQL translator.\n",
    "#     Generate a clear, focused SQL query using only the provided schema.\n",
    "#     Return only the SQL query with no explanations.\n",
    "#     The SQL query must end with a semicolon (;).\"\"\"\n",
    "\n",
    "#         # Keep user message focused and structured\n",
    "#         user = f\"\"\"User Query: {user_prompt}\n",
    "\n",
    "#     Table: {schema['table_name']}\n",
    "#     Columns: {', '.join(schema['columns'])}{context_section}\n",
    "\n",
    "#     Generate SQL:\"\"\"\n",
    "\n",
    "#         # Create and execute the chain\n",
    "#         prompt = ChatPromptTemplate.from_messages([\n",
    "#             SystemMessagePromptTemplate.from_template(system),\n",
    "#             HumanMessagePromptTemplate.from_template(\"{msg}\")\n",
    "#         ])\n",
    "#         chain = prompt | self.llm | self.parser\n",
    "#         return chain.invoke({\"msg\": user.strip()})\n",
    "\n",
    "#     def extract_sql_query_from_string(self, query_string):\n",
    "#         \"\"\"\n",
    "#         Extracts SQL query from a string response using regex pattern matching.\n",
    "        \n",
    "#         Args:\n",
    "#             query_string (str): String potentially containing SQL query\n",
    "            \n",
    "#         Returns:\n",
    "#             str or None: Extracted SQL query or None if no valid query found\n",
    "#         \"\"\"\n",
    "#         # Regular expression to find the start of the SQL query\n",
    "#         sql_pattern = re.compile(r\"(?is)(?<!\\S)(SELECT|WITH)\\b.*\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "#         # Search for the SQL query using regex (start point)\n",
    "#         match = sql_pattern.search(query_string)\n",
    "#         if not match:\n",
    "#             return None\n",
    "\n",
    "#         # Get the query from the start\n",
    "#         query_start = match.start()\n",
    "\n",
    "#         # Find the index of the last semicolon in the query string\n",
    "#         last_semicolon_index = query_string.rfind(';')\n",
    "\n",
    "#         # If no semicolon is found, return None (invalid query)\n",
    "#         if last_semicolon_index == -1:\n",
    "#             return None\n",
    "\n",
    "#         # Extract the query from the start of the match to the last semicolon\n",
    "#         sql_query = query_string[query_start:last_semicolon_index + 1].strip()\n",
    "\n",
    "#         return sql_query\n",
    "\n",
    "#     def feedback_agent(self, user_prompt: str, sql: str, schema: Dict[str, list], additional_context: str = \"\") -> str:\n",
    "#         \"\"\"\n",
    "#         Validate the SQL query against specific rules and provide targeted feedback.\n",
    "\n",
    "#         Args:\n",
    "#             user_prompt: Original user query\n",
    "#             sql: Current SQL query to validate\n",
    "#             schema: Dictionary with table_name and columns\n",
    "#             additional_context: Domain-specific validation rules\n",
    "\n",
    "#         Returns:\n",
    "#             Validation feedback as a string - \"YES\" if valid, otherwise specific issues\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Extract schema components\n",
    "#         columns = schema['columns']\n",
    "#         table_name = schema['table_name']\n",
    "\n",
    "#         # Date context for time-based queries\n",
    "#         today_date = datetime.now()\n",
    "#         date_context = (f\"Today's date is: {today_date.strftime('%Y-%m-%d')} — the {today_date.day} of \"\n",
    "#                     f\"{today_date.strftime('%B')}, {today_date.year}.\")\n",
    "\n",
    "#         # Format additional context if provided\n",
    "#         context_section = f\"\\n\\n### Additional Context:\\n{additional_context.strip()}\" if additional_context else \"\"\n",
    "\n",
    "#         # System message focused on validation rules\n",
    "#         system_message = f\"\"\"You are an SQL validator with expertise in Redshift syntax and best practices.\n",
    "\n",
    "#     Output format:\n",
    "#     - Return ONLY \"YES\" if the query is fully correct\n",
    "#     - Otherwise, return ONLY a numbered list of specific issues\n",
    "\n",
    "#     Validation Rules:\n",
    "#     1. Use only columns that exist in the provided schema\n",
    "#     2. Use correct SQL syntax and appropriate Redshift features\n",
    "#     3. Match filtering requirements in the user query\n",
    "#     4. Use case-insensitive comparisons with LOWER() where appropriate\n",
    "#     5. Use date filtering only when relevant to the user query:\n",
    "#         - Dates are given inside the columns: 'emr_month_yr', 'emr_discharge_date'.\n",
    "#             - emr_discharge_date: YYYY-MM-DD (e.g., 2023-05-14, use LIKE '2023-%')\n",
    "#             - emr_month_yr: MM-YYYY (e.g., 05-2023, use LIKE '%-2023' for year filtering)\n",
    "#         - Use the LIKE % operator while dealing with the dates. \n",
    "#         - **Grouping Date Filters**:\n",
    "#             - Always **group date filters properly** using parentheses to ensure correct logical grouping with other conditions in the `WHERE` clause.\n",
    "#                 - Example:\n",
    "#                     ```sql\n",
    "#                     WHERE \n",
    "#                         (emr_month_yr LIKE '%-2023' AND emr_month_yr >= '11-2023')\n",
    "#                         OR (emr_month_yr LIKE '%-2024' AND emr_month_yr <= '10-2024')\n",
    "#                     ```\n",
    "#             - This ensures that date conditions are applied together and evaluated accurately with other filters.\n",
    "#     6. Include all dimensions mentioned in the user query\n",
    "#     7. Add appropriate aggregations (SUM, COUNT, AVG, MIN, MAX) for metrics mentioned in the query\n",
    "#     8. Include ORDER BY only if relevant (for sorting, ranking, etc.)\n",
    "#     9. Structure subqueries and CTEs with proper grouping\n",
    "#     10. For comparison queries, include all compared dimensions\n",
    "\n",
    "#     {date_context}\n",
    "#     {context_section}\n",
    "#     \"\"\"\n",
    "\n",
    "#         # Human message with query details\n",
    "#         human_message = f\"\"\"\n",
    "#     User Query: {user_prompt}\n",
    "\n",
    "#     Table: {table_name}\n",
    "\n",
    "#     Available Columns: {', '.join(columns)}\n",
    "\n",
    "#     SQL to Validate:\n",
    "#     {sql}\n",
    "#     \"\"\"\n",
    "\n",
    "#         # Create and execute validation chain\n",
    "#         prompt = ChatPromptTemplate.from_messages([\n",
    "#             SystemMessagePromptTemplate.from_template(system_message),\n",
    "#             HumanMessagePromptTemplate.from_template(\"{msg}\")\n",
    "#         ])\n",
    "#         chain = prompt | self.llm | self.parser\n",
    "#         return chain.invoke({\"msg\": human_message}).strip()\n",
    "\n",
    "#     def refine_sql_based_on_feedback(self, user_prompt: str, previous_sql: str, \n",
    "#                                     feedback: str, schema: Dict[str, list]) -> str:\n",
    "#         \"\"\"\n",
    "#         Refine the SQL query based on specific feedback from the validation step.\n",
    "        \n",
    "#         Args:\n",
    "#             user_prompt: Original user query\n",
    "#             previous_sql: Current SQL query being refined\n",
    "#             feedback: Specific issues identified by the feedback agent\n",
    "#             schema: Dictionary with table_name and columns\n",
    "            \n",
    "#         Returns:\n",
    "#             Refined SQL query addressing the identified issues\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Extract schema components\n",
    "#         columns = schema['columns']\n",
    "#         table_name = schema['table_name']\n",
    "        \n",
    "#         # Date context for time-based queries\n",
    "#         today_date = datetime.now()\n",
    "#         date_context = f\"Today's date: {today_date.strftime('%Y-%m-%d')}, day {today_date.day} of {today_date.strftime('%B')} {today_date.year}\"\n",
    "        \n",
    "#         # System prompt focused on targeted refinement\n",
    "#         system_message = f\"\"\"You are an SQL refinement specialist.\n",
    "        \n",
    "#     Your task is to fix ONLY the specific issues in the feedback while preserving the query's core functionality.\n",
    "\n",
    "#     Guidelines:\n",
    "#     1. Address each numbered issue in the feedback explicitly\n",
    "#     2. Maintain the original query structure where possible\n",
    "#     3. Apply SQL best practices and Redshift-specific optimizations\n",
    "#     4. Ensure all required columns are used correctly\n",
    "#     5. Preserve the original business intent of the query\n",
    "\n",
    "#     {date_context}\n",
    "\n",
    "#     Return only the corrected SQL query with no explanations.\n",
    "#     \"\"\"\n",
    "\n",
    "#         # Human message with detailed context\n",
    "#         human_message = f\"\"\"\n",
    "#     Original User Query: {user_prompt}\n",
    "\n",
    "#     Available Schema:\n",
    "#     - Table: {table_name}\n",
    "#     - Columns: {', '.join(columns)}\n",
    "\n",
    "#     Previous SQL Query:\n",
    "#     {previous_sql}\n",
    "\n",
    "#     Issues to Fix:\n",
    "#     {feedback}\n",
    "\n",
    "#     Return the corrected SQL query:\n",
    "#     \"\"\"\n",
    "\n",
    "#         # Create and execute refinement chain\n",
    "#         prompt = ChatPromptTemplate.from_messages([\n",
    "#             SystemMessagePromptTemplate.from_template(system_message),\n",
    "#             HumanMessagePromptTemplate.from_template(\"{msg}\")\n",
    "#         ])\n",
    "#         chain = prompt | self.llm | self.parser\n",
    "#         return chain.invoke({\"msg\": human_message}).strip()\n",
    "\n",
    "#     def generate_and_iteratively_refine_sql(\n",
    "#         self,\n",
    "#         user_prompt: str,\n",
    "#         schema: Dict[str, list],\n",
    "#         domain_context: str = \"\",\n",
    "#         complex_instructions: str = \"\",\n",
    "#         max_iterations: int = 2,\n",
    "#         verbose: bool = True\n",
    "#     ) -> str:\n",
    "#         \"\"\"\n",
    "#         Enhanced SQL generation with iterative refinement and optional debug outputs.\n",
    "\n",
    "#         Args:\n",
    "#             user_prompt: Natural language query from the user.\n",
    "#             schema: Dictionary containing table names and columns.\n",
    "#             domain_context: Optional domain-specific knowledge.\n",
    "#             complex_instructions: Optional advanced instructions.\n",
    "#             max_iterations: Maximum feedback‑refinement loops.\n",
    "#             verbose: Whether to print detailed step-by-step output.\n",
    "\n",
    "#         Returns:\n",
    "#             Optimized SQL query string.\n",
    "#         \"\"\"\n",
    "#         if verbose:\n",
    "#             print(\"\\n========== USER PROMPT ==========\")\n",
    "#             print(user_prompt)\n",
    "\n",
    "#         # ---------- Phase 1 | Initial SQL ----------\n",
    "#         if verbose:\n",
    "#             print(\"\\n========== PHASE 1: INITIAL SQL GENERATION ==========\")\n",
    "#         initial_sql = self.prompt_chain_step1(\n",
    "#             user_prompt,\n",
    "#             schema,\n",
    "#             additional_context=\"\",\n",
    "#             context_type=\"general\",\n",
    "#         )\n",
    "#         current_sql = self.extract_sql_query_from_string(initial_sql) or initial_sql\n",
    "#         if verbose:\n",
    "#             print(\"Initial SQL Generated:\")\n",
    "#             print(current_sql)\n",
    "\n",
    "#         # ---------- Phase 2 | Add Domain Knowledge ----------\n",
    "#         if domain_context:\n",
    "#             if verbose:\n",
    "#                 print(\"\\n========== PHASE 2: DOMAIN KNOWLEDGE ENHANCEMENT ==========\")\n",
    "#             domain_enhanced_sql = self.prompt_chain_step1(\n",
    "#                 user_prompt,\n",
    "#                 schema,\n",
    "#                 additional_context=f\"\"\"\\\n",
    "#     Existing SQL:\n",
    "#     {current_sql}\n",
    "\n",
    "#     Domain Knowledge:\n",
    "#     {domain_context}\n",
    "#     \"\"\",\n",
    "#                 context_type=\"domain_knowledge\",\n",
    "#             )\n",
    "#             current_sql = self.extract_sql_query_from_string(domain_enhanced_sql) or domain_enhanced_sql\n",
    "#             if verbose:\n",
    "#                 print(\"Domain-Enhanced SQL:\")\n",
    "#                 print(current_sql)\n",
    "\n",
    "#             feedback = self.feedback_agent(\n",
    "#                 user_prompt,\n",
    "#                 current_sql,\n",
    "#                 schema,\n",
    "#                 additional_context=domain_context,\n",
    "#             )\n",
    "#             if verbose:\n",
    "#                 print(\"\\nDomain SQL Feedback:\")\n",
    "#                 print(feedback)\n",
    "\n",
    "#             if feedback.upper() != \"YES\":\n",
    "#                 if verbose:\n",
    "#                     print(\"\\nRefining SQL based on domain feedback...\")\n",
    "#                 current_sql = (\n",
    "#                     self.extract_sql_query_from_string(\n",
    "#                         self.refine_sql_based_on_feedback(\n",
    "#                             user_prompt, current_sql, feedback, schema\n",
    "#                         )\n",
    "#                     )\n",
    "#                     or current_sql\n",
    "#                 )\n",
    "#                 if verbose:\n",
    "#                     print(\"Refined Domain SQL:\")\n",
    "#                     print(current_sql)\n",
    "#             elif verbose:\n",
    "#                 print(\"✅ Domain-enhanced SQL validated.\")\n",
    "\n",
    "#         # ---------- Phase 3 | Add Complex Instructions ----------\n",
    "#         if complex_instructions:\n",
    "#             if verbose:\n",
    "#                 print(\"\\n========== PHASE 3: COMPLEX INSTRUCTION ENHANCEMENT ==========\")\n",
    "#             instruction_enhanced_sql = self.prompt_chain_step1(\n",
    "#                 user_prompt,\n",
    "#                 schema,\n",
    "#                 additional_context=f\"\"\"\\\n",
    "#     Existing SQL:\n",
    "#     {current_sql}\n",
    "\n",
    "#     Complex Instructions:\n",
    "#     {complex_instructions}\n",
    "#     \"\"\",\n",
    "#                 context_type=\"complex_instruction\",\n",
    "#             )\n",
    "#             current_sql = (\n",
    "#                 self.extract_sql_query_from_string(instruction_enhanced_sql)\n",
    "#                 or instruction_enhanced_sql\n",
    "#             )\n",
    "#             if verbose:\n",
    "#                 print(\"Instruction-Enhanced SQL:\")\n",
    "#                 print(current_sql)\n",
    "\n",
    "#             feedback = self.feedback_agent(\n",
    "#                 user_prompt,\n",
    "#                 current_sql,\n",
    "#                 schema,\n",
    "#                 additional_context=complex_instructions,\n",
    "#             )\n",
    "#             if verbose:\n",
    "#                 print(\"\\nInstruction SQL Feedback:\")\n",
    "#                 print(feedback)\n",
    "\n",
    "#             if feedback.upper() != \"YES\":\n",
    "#                 if verbose:\n",
    "#                     print(\"\\nRefining SQL based on instruction feedback...\")\n",
    "#                 current_sql = (\n",
    "#                     self.extract_sql_query_from_string(\n",
    "#                         self.refine_sql_based_on_feedback(\n",
    "#                             user_prompt, current_sql, feedback, schema\n",
    "#                         )\n",
    "#                     )\n",
    "#                     or current_sql\n",
    "#                 )\n",
    "#                 if verbose:\n",
    "#                     print(\"Refined Instruction SQL:\")\n",
    "#                     print(current_sql)\n",
    "#             elif verbose:\n",
    "#                 print(\"✅ Instruction-enhanced SQL validated.\")\n",
    "\n",
    "#         # ---------- Phase 4 | Combined Context & Final Loops ----------\n",
    "#         if domain_context and complex_instructions:\n",
    "#             if verbose:\n",
    "#                 print(\"\\n========== PHASE 4: COMBINED CONTEXT ENHANCEMENT ==========\")\n",
    "#             combined_context = f\"{domain_context}\\n\\n{complex_instructions}\"\n",
    "#             combined_enhanced_sql = self.prompt_chain_step1(\n",
    "#                 user_prompt,\n",
    "#                 schema,\n",
    "#                 additional_context=f\"\"\"\\\n",
    "#     Existing SQL:\n",
    "#     {current_sql}\n",
    "\n",
    "#     Combined Context:\n",
    "#     {combined_context}\n",
    "#     \"\"\",\n",
    "#                 context_type=\"complex_instruction\",\n",
    "#             )\n",
    "#             current_sql = (\n",
    "#                 self.extract_sql_query_from_string(combined_enhanced_sql)\n",
    "#                 or combined_enhanced_sql\n",
    "#             )\n",
    "#             if verbose:\n",
    "#                 print(\"Combined-Enhanced SQL:\")\n",
    "#                 print(current_sql)\n",
    "\n",
    "#             for i in range(1, max_iterations + 1):\n",
    "#                 if verbose:\n",
    "#                     print(f\"\\n========== FINAL FEEDBACK LOOP {i} ==========\")\n",
    "#                 feedback = self.feedback_agent(\n",
    "#                     user_prompt,\n",
    "#                     current_sql,\n",
    "#                     schema,\n",
    "#                     additional_context=combined_context,\n",
    "#                 )\n",
    "#                 if verbose:\n",
    "#                     print(f\"Feedback: {feedback}\")\n",
    "\n",
    "#                 if feedback.upper() == \"YES\":\n",
    "#                     if verbose:\n",
    "#                         print(f\"✅ SQL validated at iteration {i}\")\n",
    "#                     break\n",
    "\n",
    "#                 if verbose:\n",
    "#                     print(\"Refining SQL based on final feedback...\")\n",
    "#                 current_sql = (\n",
    "#                     self.extract_sql_query_from_string(\n",
    "#                         self.refine_sql_based_on_feedback(\n",
    "#                             user_prompt, current_sql, feedback, schema\n",
    "#                         )\n",
    "#                     )\n",
    "#                     or current_sql\n",
    "#                 )\n",
    "#                 if verbose:\n",
    "#                     print(f\"Refined SQL (Iteration {i}):\")\n",
    "#                     print(current_sql)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(\"\\n========== FINAL SQL OUTPUT ==========\")\n",
    "#             print(current_sql)\n",
    "\n",
    "#         return current_sql\n",
    "    \n",
    "# multi_stage_sql_generator = MultiStageSQLGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Approach - Single Pass Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStageSQLGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-4.1-mini\",          # Faster default model\n",
    "        temperature: float = 0,\n",
    "        request_timeout: int = 20,           # Lower timeout to fail fast on network stalls\n",
    "        max_retries: int = 1,                # Reduce retry loops to cut latency\n",
    "        max_tokens: Optional[int] = 2400     # Cap output to keep responses snappy\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Speed-oriented setup. You can switch back to 'gpt-4.1-mini' if you prefer.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            timeout=request_timeout,\n",
    "            max_retries=max_retries,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        self.parser = StrOutputParser()\n",
    "\n",
    "    # -------- Core: single generator prompt (already considers all contexts) --------\n",
    "    def prompt_chain_single(self, user_prompt: str, schema: Dict[str, list],\n",
    "                            domain_context: str = \"\", complex_instructions: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        One concise generation pass that integrates domain knowledge + complex instructions if provided.\n",
    "        Returns ONLY SQL (per instructions), ending with a semicolon.\n",
    "        \"\"\"\n",
    "\n",
    "        # Build a tight context block to minimize tokens but keep precision\n",
    "        blocks = []\n",
    "        if domain_context:\n",
    "            blocks.append(f\"### Domain Knowledge:\\n{domain_context.strip()}\")\n",
    "        if complex_instructions:\n",
    "            blocks.append(f\"### Complex Instructions:\\n{complex_instructions.strip()}\")\n",
    "        extra = (\"\\n\\n\" + \"\\n\\n\".join(blocks)) if blocks else \"\"\n",
    "\n",
    "        system = \"\"\"You are a senior Redshift SQL generator.\n",
    "- Use ONLY the provided table and columns.\n",
    "- Apply domain/context instructions if given.\n",
    "- Use case-insensitive comparisons with LOWER() for string filters where appropriate.\n",
    "- Dates:\n",
    "  * emr_discharge_date: YYYY-MM-DD  (e.g., 2023-05-14; year filter: emr_discharge_date LIKE '2023-%')\n",
    "  * emr_month_yr: MM-YYYY          (e.g., 05-2023; year filter: emr_month_yr LIKE '%-2023')\n",
    "  * Use LIKE with % for flexible date filtering where needed.\n",
    "  * Group date conditions in parentheses when mixing with other filters.\n",
    "\n",
    "📌 **Date Range Conversion (Mandatory)**  \n",
    "    - ALWAYS include date range columns in every query:  \n",
    "    - For `emr_discharge_date`, ALWAYS convert using:\n",
    "        TO_CHAR(MIN(TO_DATE(emr_discharge_date, 'YYYY-MM-DD')), 'YYYY-MM-DD') AS start_date,\n",
    "        TO_CHAR(MAX(TO_DATE(emr_discharge_date, 'YYYY-MM-DD')), 'YYYY-MM-DD') AS end_date  \n",
    "    - This conversion MUST be applied even if the user does not explicitly request the date range.  \n",
    "\n",
    "- Include aggregations (SUM, COUNT, AVG, MIN, MAX) when the user asks for metrics.\n",
    "- Include ORDER BY only if relevant.\n",
    "- Structure CTEs/subqueries cleanly when helpful.\n",
    "Return ONLY the SQL query, ending with a semicolon.\n",
    "\"\"\"\n",
    "\n",
    "        # Keep the user message compact to reduce tokens\n",
    "        user = f\"\"\"User Query: {user_prompt}\n",
    "\n",
    "Table: {schema['table_name']}\n",
    "Columns: {', '.join(schema['columns'])}{extra}\n",
    "\n",
    "Generate SQL:\"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(system),\n",
    "            HumanMessagePromptTemplate.from_template(\"{msg}\")\n",
    "        ])\n",
    "        chain = prompt | self.llm | self.parser\n",
    "        return chain.invoke({\"msg\": user.strip()})\n",
    "\n",
    "    # -------- Collapsed: validator OR fixer (one call does both) --------\n",
    "    def validator_or_fix(self, user_prompt: str, sql: str, schema: Dict[str, list],\n",
    "                         additional_context: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        If query is fully correct → return EXACTLY 'YES'\n",
    "        Else → return ONLY the corrected SQL query (no explanations).\n",
    "        This compresses the 'feedback + refine' cycle into a single call.\n",
    "        \"\"\"\n",
    "\n",
    "        columns = schema['columns']\n",
    "        table_name = schema['table_name']\n",
    "        today_date = datetime.now()\n",
    "        date_context = (f\"Today's date is: {today_date.strftime('%Y-%m-%d')} — \"\n",
    "                        f\"the {today_date.day} of {today_date.strftime('%B')}, {today_date.year}.\")\n",
    "\n",
    "        context_block = f\"\\n\\n### Additional Context:\\n{additional_context.strip()}\" if additional_context else \"\"\n",
    "\n",
    "        system = f\"\"\"You are an expert Redshift SQL validator and fixer.\n",
    "\n",
    "Output rules:\n",
    "- If the SQL is fully correct, respond EXACTLY with: YES\n",
    "- Otherwise, respond ONLY with the corrected SQL query (no comments), ending with a semicolon.\n",
    "\n",
    "Validation Rules:\n",
    "1) Use only columns from the provided schema\n",
    "2) Correct Redshift syntax\n",
    "3) Match the user's filtering intent\n",
    "4) Case-insensitive comparisons via LOWER() when appropriate\n",
    "5) Date filtering (only if relevant):\n",
    "   - emr_discharge_date: YYYY-MM-DD (use LIKE 'YYYY-%' for year)\n",
    "   - Use LIKE with % when needed\n",
    "   - Group date conditions in parentheses with other filters\n",
    "    📌 **Date Range Conversion (Mandatory)**  \n",
    "        - ALWAYS include date range columns in every query:  \n",
    "        - For `emr_discharge_date`, ALWAYS convert using:\n",
    "            TO_CHAR(MIN(TO_DATE(emr_discharge_date, 'YYYY-MM-DD')), 'YYYY-MM-DD') AS start_date,\n",
    "            TO_CHAR(MAX(TO_DATE(emr_discharge_date, 'YYYY-MM-DD')), 'YYYY-MM-DD') AS end_date  \n",
    "        - This conversion MUST be applied even if the user does not explicitly request the date range.  \n",
    "\n",
    "6) Include all dimensions mentioned by the user\n",
    "7) Use aggregations for metric asks\n",
    "8) Include ORDER BY only if relevant\n",
    "9) Proper grouping for CTEs/subqueries\n",
    "10) For comparisons, include all compared dimensions\n",
    "\n",
    "{date_context}{context_block}\n",
    "\"\"\"\n",
    "\n",
    "        human = f\"\"\"User Query: {user_prompt}\n",
    "\n",
    "Table: {table_name}\n",
    "Available Columns: {', '.join(columns)}\n",
    "\n",
    "SQL to Validate:\n",
    "{sql}\n",
    "\"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(system),\n",
    "            HumanMessagePromptTemplate.from_template(\"{msg}\")\n",
    "        ])\n",
    "        chain = prompt | self.llm | self.parser\n",
    "        return chain.invoke({\"msg\": human}).strip()\n",
    "\n",
    "    # -------- Utility: robust SQL extractor (kept from your version) --------\n",
    "    def extract_sql_query_from_string(self, query_string):\n",
    "        \"\"\"\n",
    "        Extract SQL starting at SELECT/WITH up to the last semicolon.\n",
    "        \"\"\"\n",
    "        sql_pattern = re.compile(r\"(?is)(?<!\\S)(SELECT|WITH)\\b.*\", re.IGNORECASE | re.DOTALL)\n",
    "        match = sql_pattern.search(query_string)\n",
    "        if not match:\n",
    "            return None\n",
    "        last_semicolon_index = query_string.rfind(';')\n",
    "        if last_semicolon_index == -1:\n",
    "            return None\n",
    "        return query_string[match.start(): last_semicolon_index + 1].strip()\n",
    "\n",
    "    # -------- Public API: drastically fewer LLM calls while keeping quality --------\n",
    "    def generate_and_iteratively_refine_sql(\n",
    "        self,\n",
    "        user_prompt: str,\n",
    "        schema: Dict[str, list],\n",
    "        domain_context: str = \"\",\n",
    "        complex_instructions: str = \"\",\n",
    "        max_iterations: int = 2,     # You can set 1 for even faster\n",
    "        verbose: bool = True\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        New flow:\n",
    "          1) Single-pass generation (already integrates domain + complex context)\n",
    "          2) Single validator_or_fix call per iteration (YES or return corrected SQL)\n",
    "          → Default path: ~3 calls total vs. many in the original\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n========== USER PROMPT ==========\")\n",
    "            print(user_prompt)\n",
    "\n",
    "        # ---- Phase 1: Single-pass generation with combined context ----\n",
    "        if verbose:\n",
    "            print(\"\\n========== PHASE 1: SINGLE-PASS GENERATION ==========\")\n",
    "\n",
    "        initial_sql = self.prompt_chain_single(\n",
    "            user_prompt=user_prompt,\n",
    "            schema=schema,\n",
    "            domain_context=domain_context,\n",
    "            complex_instructions=complex_instructions,\n",
    "        )\n",
    "        current_sql = self.extract_sql_query_from_string(initial_sql) or initial_sql\n",
    "        if verbose:\n",
    "            print(\"Initial SQL:\")\n",
    "            print(current_sql)\n",
    "\n",
    "        # Combine contexts once for validation to keep prompts tiny\n",
    "        combined_context = \"\\n\\n\".join([c for c in [domain_context, complex_instructions] if c])\n",
    "\n",
    "        # ---- Phase 2+: Compact validation/fix loops (1 call per iteration) ----\n",
    "        for i in range(1, max_iterations + 1):\n",
    "            if verbose:\n",
    "                print(f\"\\n========== VALIDATE/FIX LOOP {i} ==========\")\n",
    "            verdict_or_fix = self.validator_or_fix(\n",
    "                user_prompt=user_prompt,\n",
    "                sql=current_sql,\n",
    "                schema=schema,\n",
    "                additional_context=combined_context,\n",
    "            )\n",
    "\n",
    "            if verdict_or_fix.strip().upper() == \"YES\":\n",
    "                if verbose:\n",
    "                    print(f\"✅ SQL validated at iteration {i}\")\n",
    "                break\n",
    "\n",
    "            # Otherwise we received corrected SQL — extract and continue\n",
    "            maybe_sql = self.extract_sql_query_from_string(verdict_or_fix) or verdict_or_fix\n",
    "            if verbose:\n",
    "                print(\"Received corrected SQL:\")\n",
    "                print(maybe_sql)\n",
    "            current_sql = maybe_sql\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n========== FINAL SQL OUTPUT ==========\")\n",
    "            print(current_sql)\n",
    "\n",
    "        return current_sql\n",
    "\n",
    "# Instantiate the faster generator (compatible name)\n",
    "multi_stage_sql_generator = MultiStageSQLGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rough to test: MultiStageSQLGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # user_prompt = \"What is the '42321610-SPINAL SCREWS OR SCREW EXTENSIONS' UNSPSC product utilization across physicians used in '0SG10AJ-FUSION 2-4 L JT W INTBD FUS DEV, POST APPR A COL, OPEN' procedure?\"\n",
    "# # user_prompt = 'Whats the total acquisition cost for the year 2024?'\n",
    "# # user_prompt = 'Which Market has the highest total cost?'\n",
    "# user_prompt = \"compare the top 10 primary procedures by average cpc for Market Lorain and Market Youngstown for Service line WOMEN'S HEALTH for 2024\"\n",
    "# # user_prompt = input()\n",
    "\n",
    "# print(f'user_prompt: {user_prompt}')\n",
    "\n",
    "# ## Running the Chain | TableSelection\n",
    "# selected_table_text = table_selection_object.get_table_name(user_prompt)\n",
    "\n",
    "# #### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "# #### Retrieving best matched column info for SQL generation based on user prompt and selected table\n",
    "# #### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "\n",
    "# table_to_use, filtered_df, best_match_columns, table_columns, column_info_from_knowledge_base, prompt_to_use_for_complex_question, columns_info_to_generate_sql = embedding_task_helper.get_column_info_for_sql_generation(user_prompt, selected_table_text)\n",
    "\n",
    "# schema = {\n",
    "#     \"table_name\": table_to_use,\n",
    "#     \"columns\": table_columns\n",
    "# }\n",
    "\n",
    "# domain_dict = column_info_from_knowledge_base\n",
    "# addons = prompt_to_use_for_complex_question\n",
    "\n",
    "#### Running the SQL Multi Stage Generation Way ######\n",
    "# Generate SQL using the function-based approach\n",
    "# final_sql = multi_stage_sql_generator.generate_and_iteratively_refine_sql(\n",
    "#     user_prompt=user_prompt,\n",
    "#     schema=schema,\n",
    "#     domain_context=domain_dict,\n",
    "#     complex_instructions=prompt_to_use_for_complex_question,\n",
    "#     max_iterations = 2)\n",
    "\n",
    "# generated_sql_query, wf = sql_query_generator.get_result_from_sql_query(generated_sql_query=final_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLResponseGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLResponseGenerator:\n",
    "    \n",
    "    def __init__(self, s3_utils):\n",
    "\n",
    "        ## __ GPT-4.1 Response __\n",
    "        model_name = 'gpt-4.1-2025-04-14'\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "        # ____ GPT 5 Response ____\n",
    "        # model_name = 'gpt-5-2025-08-07'\n",
    "        # model_name = \"gpt-5-mini-2025-08-07\"\n",
    "        # model_name = \"gpt-5-nano-2025-08-07\"\n",
    "        # self.llm = ChatOpenAI(model=model_name, temperature=1)\n",
    "\n",
    "        self.get_redirect_link_from_df = s3_utils.get_redirect_link_from_df\n",
    "\n",
    "        self.generic_response_guideline = \"\"\"\n",
    "# Generic Guidelines:\n",
    "- If the data in the DataFrame are in abbreviated form, do not expand them.\n",
    "- If the DataFrame is empty, politely let the user know that there is no available answer for their question.\n",
    "- **Avoid mentioning any details related to the table used in the SQL query.\n",
    "- **Do not reuse the exact column names from the SQL query when presenting data. Instead, use suitable synonyms or natural-language alternatives that convey the same meaning.**\n",
    "- **State the data exactly as it appears in the DataFrame without summarizing, interpreting, or making any assumptions about trends.** Only list each value as observed.\n",
    "- Review the SQL query to determine the basis of sorting, and include this information in the response to clarify the order of results.\n",
    "- Only mention sorting or ordering if the user’s question explicitly asks about ranking, order, top, or lowest; otherwise, do not include it.\n",
    "- *Use appropriate commas when formatting any cost-related numbers to make them easier to read.\n",
    "- If the user requests any 'CHARTS' (e.g., Pie Chart, Bar Chart), do not mention it in your response.\n",
    "- If all values for a parameter are identical across entities (e.g., identical scores across Entity), explicitly list the names of these entities to give the user clarity.\n",
    "- Tone of Conversation: Polite and humble, professional, concise and to the point.\n",
    "- Be as detailed and informative as possible while adhering strictly to these guidelines.\n",
    "- ***When referencing data from the DataFrame in the response, do not cut, modify, or remove any part of the values. This includes retaining all numeric identifiers, leading information, and any symbols or abbreviations as they appear in the DataFrame.\n",
    "- Do NOT add any closing or follow-up phrases such as \"Let me know if you need more information\", \"You can ask for a breakdown\", \"Feel free to request\", or similar open-ended invitations.\n",
    "- End the response right after presenting the last fact or data point. No extra commentary or summary should follow.\n",
    "\n",
    "## Data Analysis Response Rules (VERY IMPORTANT)\n",
    "- Priority Order: Question > DataFrame > SQL Query\n",
    "- **Core Rule: Only include what's explicitly requested. If a parameter isn't asked for, exclude it entirely.**\n",
    "- Before responding, verify: Does this directly answer only what was asked?\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        self.formatting_requirements = \"\"\"\n",
    "## Date and Time Formatting Requirements:\n",
    "\t1.\tDate Formats:\n",
    "        • Always preserve the full date granularity available in the DataFrame.\n",
    "        • If day is present → output in YYYY-MM-DD (Example: 2023-05-14)\n",
    "        • If only month/year is present → output in Month YYYY (Example: July 2023)\n",
    "        • Never drop the day when it exists.\n",
    "\t2.\tMonth Representation:\n",
    "\t    •\tUse full month names with year (Example: July 2023)\n",
    "\t    •\tConvert any numeric months to full text format.\n",
    "\t3.\tChronological Ordering:\n",
    "\t    •\tWhen DataFrame contains temporal data (dates/months):\n",
    "            a. Primary sort: Year (ascending)\n",
    "            b. Secondary sort: Month (ascending)\n",
    "            c. Tertiary sort: Day (ascending, if available)\n",
    "                •\tExample order: 2023-01-15, 2023-01-30, 2023-02-10\n",
    "                •\tApply this sorting regardless of how data appears in the DataFrame.\n",
    "                •\tMaintain this order even if the SQL query specifies a different ordering.\n",
    "\t4.\tMonth-Specific Analysis:\n",
    "\t    •\tWhen a specific month is referenced, provide focused insights for that timeframe using the full month format.\n",
    "\t    •\tIf the data contains day-level granularity for that month, include the exact days.\n",
    "\n",
    "## Formatting Rules for Responses:\n",
    "    1. Currency Formatting:\n",
    "    - Use $ for CPC, Spend, Price, Cost, or Charge.\n",
    "    - Numbers must:\n",
    "        - Include commas (`,`) for readability.\n",
    "        - Be rounded to two decimal places.\n",
    "        - Have the currency symbol prefixed (e.g., `$36,614.58`).\n",
    "        - Eg: For Percentage: 32.56%\n",
    "\n",
    "    2. Non-Currency Numbers:\n",
    "    - Exclude the currency symbol for metrics like Outcome Scores or Encounters.\n",
    "    - Format with commas and two decimal places (e.g., `36,614.58`).\n",
    "\n",
    "    3. Presentation:\n",
    "    - Ensure numbers align with context (e.g., CPC is always currency).\n",
    "\n",
    "    Example Response:\n",
    "    - December 2023: $34,494.08\n",
    "    - November 2023: $36,826.91\n",
    "    - Percentage: 32.56%\n",
    "    - Treat market share values as percentages (e.g., 0.46 → 46%).\"\"\"\n",
    "        \n",
    "        self.comparison_related_guideline = \"\"\"\n",
    "-----------------------\n",
    "## Comparison Guideline\n",
    "    - For comparison requests, always present results in a proper *Markdown tabular format*,\n",
    "    - Each row = entity; each column = metric.\n",
    "    - Show data exactly as in the DataFrame, no edits.\n",
    "    - Mention sorting basis, use commas for numbers.\n",
    "    - Keep tone polite, humble, professional, concise.\n",
    "-----------------------\n",
    "\"\"\"\n",
    "\n",
    "    def get_date_range(self, df: Optional[pd.DataFrame]) -> Tuple[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Returns a full date range instruction prompt for the LLM and the updated DataFrame\n",
    "        with 'start_date' and 'end_date' columns removed. If unavailable, returns a blank \n",
    "        string and the original DataFrame.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        if df is None or not isinstance(df, pd.DataFrame):\n",
    "            return \"\", df\n",
    "\n",
    "\n",
    "        ## Make a Copy of DataFrame\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # Normalize column names (lowercase, remove spaces, underscores)\n",
    "        normalized_cols = {col: col.strip().lower().replace(\" \", \"_\") for col in df_copy.columns}\n",
    "        df_copy.rename(columns=normalized_cols, inplace=True)\n",
    "\n",
    "        # If required columns are missing or DataFrame is empty, return unchanged\n",
    "        if 'start_date' not in df_copy.columns or 'end_date' not in df_copy.columns or df_copy.empty:\n",
    "            return \"\", df_copy\n",
    "\n",
    "        df_copy['start_date'] = pd.to_datetime(df_copy['start_date'], errors='coerce')\n",
    "        df_copy['end_date'] = pd.to_datetime(df_copy['end_date'], errors='coerce')\n",
    "\n",
    "        if df_copy['start_date'].dropna().empty or df_copy['end_date'].dropna().empty:\n",
    "            # Drop the columns even if invalid data\n",
    "            df_copy = df_copy.drop(columns=['start_date', 'end_date'], errors='ignore')\n",
    "            return \"\", df_copy\n",
    "\n",
    "        # Find min and max dates\n",
    "        min_date = df_copy['start_date'].min()\n",
    "        max_date = df_copy['end_date'].max()\n",
    "\n",
    "        # Format dates as \"Month Day, Year\" (remove leading zeros)\n",
    "        min_str = min_date.strftime(\"%B %d, %Y\").replace(\" 0\", \" \")\n",
    "        max_str = max_date.strftime(\"%B %d, %Y\").replace(\" 0\", \" \")\n",
    "\n",
    "        if not min_str or not max_str:\n",
    "            df_copy = df_copy.drop(columns=['start_date', 'end_date'], errors='ignore')\n",
    "            return \"\", df_copy\n",
    "\n",
    "        # Drop date columns before returning\n",
    "        df_copy = df_copy.drop(columns=['start_date', 'end_date'], errors='ignore')\n",
    "\n",
    "        date_instruction = (\n",
    "            f\"📅 **Data Range Instruction**:\\n\"\n",
    "            f\"\\t- The data used for this query spans from **{min_str}** to **{max_str}**.\\n\"\n",
    "            f\"\\t- Always mention the **starting date** and **ending date** of the data used for this query \"\n",
    "            f\"at the very **BOTTOM** of your response, even if the user did not explicitly ask for it.\\n\"\n",
    "            f\"\\t- Always show the start and end dates exactly as this format: Full_Month DAY, YEAR(YYYY).\"\n",
    "        )\n",
    "\n",
    "        ## Inverse Normalization\n",
    "        df_copy.columns = (\n",
    "                df_copy.columns\n",
    "                .str.replace(\"_\", \" \")                  # replace underscores with space\n",
    "                .str.title()                            # capitalize each word\n",
    "            )\n",
    "\n",
    "        return date_instruction, df_copy\n",
    "\n",
    "    def _generate_standard_prompt_for_small_df(self, question, sqlQuery, df, date_instruction):\n",
    "        \n",
    "        empty_df_instruction = empty_df_instruction = \"\"\"\n",
    "When the DataFrame is empty, inform the user that no data is available, which may be due to missing information or because the requested criteria did not match any records.\n",
    "\"\"\" if df.empty else \"\"\n",
    "        \n",
    "        prompt = f'''You are a friendly assistant working for SupplyCopia. Based on the user’s question, SQL Query, and DataFrame, provide a detailed, conversational response that directly addresses the user’s question.\n",
    "        \n",
    "{self.generic_response_guideline}\n",
    "\n",
    "{self.formatting_requirements}\n",
    "\n",
    "{self.comparison_related_guideline}\n",
    "\n",
    "------------  \n",
    "USER'S QUESTION: {question}  \n",
    "------------  \n",
    "SQL QUERY: {sqlQuery}  \n",
    "------------  \n",
    "DataFrame: {df.to_json().replace('{', '{{').replace('}', '}}')}  \n",
    "------------  \n",
    "\n",
    "{empty_df_instruction}\n",
    "\n",
    "{date_instruction}\n",
    "\n",
    "RESPONSE TO USER:  \n",
    "'''\n",
    "        return prompt\n",
    "\n",
    "    def _generate_top_rows_with_download_link_prompt(self, question, sqlQuery, df, redirect_link, date_instruction):\n",
    "\n",
    "        top_df = df.head(20)\n",
    "        total_rows = len(df)\n",
    "\n",
    "        columns_to_use = df.columns.tolist()\n",
    "\n",
    "        prompt = f'''You are a friendly assistant working for SupplyCopia. Based on the user’s question, SQL Query, and DataFrame, provide a detailed, professional response that shows the top results and provides a download link for all data.\n",
    "\n",
    "# Specific Guidelines:\n",
    "    - Only the top 20 rows out of a total of {total_rows} rows are shown below. Mention that the full dataset is available via the download link.\n",
    "\n",
    "Download Link: [Click here to download full results]({redirect_link})\n",
    "\n",
    "{self.generic_response_guideline}\n",
    "\n",
    "# Formatting:\n",
    "    {self.formatting_requirements}\n",
    "\n",
    "{self.comparison_related_guideline}    \n",
    "\n",
    "------------\n",
    "USER'S QUESTION: {question}\n",
    "------------\n",
    "SQL QUERY: {sqlQuery}\n",
    "------------\n",
    "Showing Top 20 of {total_rows} Rows (Full dataset is downloadable via the download link given below.)\n",
    "------------\n",
    "DataFrame: {top_df.to_json().replace('{', '{{').replace('}', '}}')}\n",
    "------------\n",
    "\n",
    "Columns in DataFrame: {', '.join(columns_to_use)}\n",
    "\n",
    "{date_instruction}\n",
    "\n",
    "RESPONSE TO USER:\n",
    "    '''\n",
    "        return prompt\n",
    "\n",
    "    def get_sql_response(self, question, sqlQuery, df):\n",
    "\n",
    "        ## Date Instruction\n",
    "        date_instruction, df_copy = self.get_date_range(df)\n",
    "        if date_instruction:\n",
    "            df = df_copy.copy()\n",
    "\n",
    "        # print(f'Date Instruction: {date_instruction}')\n",
    "        # print(f'COlumns: {df.columns}')\n",
    "        \n",
    "        if len(df) <= 100:\n",
    "            # Use simple non-chunked prompt\n",
    "            prompt = self._generate_standard_prompt_for_small_df(question, sqlQuery, df, date_instruction)\n",
    "            chain = PromptTemplate.from_template(prompt) | self.llm | StrOutputParser()\n",
    "            return chain.invoke({}).strip()\n",
    "\n",
    "        # Handle large DataFrame: show top 20 rows + download link\n",
    "        try:\n",
    "            _, _, redirect_link = self.get_redirect_link_from_df(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to get redirect link: {str(e)[:300]}\")\n",
    "            redirect_link = None\n",
    "\n",
    "        if redirect_link:\n",
    "            prompt = self._generate_top_rows_with_download_link_prompt(\n",
    "                question, sqlQuery, df, redirect_link, date_instruction\n",
    "            )\n",
    "        else:\n",
    "            # Fallback if redirect link fails — show top 20 rows without download message\n",
    "            print(\"[Warning] Redirect link not available. Showing top 20 results only.\")\n",
    "            prompt = self._generate_standard_prompt_for_small_df(question, sqlQuery, df.head(20), date_instruction)\n",
    "\n",
    "        chain = PromptTemplate.from_template(prompt) | self.llm | StrOutputParser()\n",
    "        return chain.invoke({}).strip().replace('http://', 'https://')\n",
    "\n",
    "\n",
    "sql_response_generator = SQLResponseGenerator(s3_utils=s3_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # s3_utils.get_redirect_link_from_df(df)\n",
    "# sql_response_chain_result = sql_response_generator.get_sql_response(user_prompt_to_use, generated_sql_query, df)\n",
    "\n",
    "# print(f'Question: {user_prompt_to_use}\\n')\n",
    "# Markdown(sql_response_chain_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Question: {user_prompt_to_use}')\n",
    "# Markdown(sql_response_chain_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeneralQueryAPIHandler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralQueryAPIHandler:\n",
    "    def __init__(self, api_url: str):\n",
    "        \"\"\"\n",
    "        Initializes the GeneralQueryAPIHandler class with the specified API URL.\n",
    "\n",
    "        Args:\n",
    "            api_url (str): The URL of the API endpoint.\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "\n",
    "    def get_result(self, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Sends a POST request to the API with the provided user prompt and returns the text response.\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The user's prompt to send to the API.\n",
    "\n",
    "        Returns:\n",
    "            str: The 'text' value from the API response, or an error message if the request fails.\n",
    "        \"\"\"\n",
    "        payload = {\"question\": user_prompt}\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.api_url, json=payload)\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "            return response.json().get('text', 'No text found in response')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle potential exceptions, such as network issues or invalid responses\n",
    "            print(f\"Error during API request: {e}\")\n",
    "            return \"Failed to send payload to the API.\"\n",
    "\n",
    "API_URL = \"https://chat.supplycopia.com/api/v1/prediction/b19b865b-1ac7-448b-940f-e78857b3fe76\"\n",
    "general_query_api = GeneralQueryAPIHandler(API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HighchartModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighchartModule:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def function_to_check_for_chart_possibility(self, df):\n",
    "        charting = False\n",
    "        if df is not None and not df.empty and len(df) != 1 and len(df.columns) != 1:\n",
    "            charting = True\n",
    "        return charting\n",
    "                \n",
    "    def context_generation_for_js_config(self, human_message, df):\n",
    "        \"\"\"\n",
    "        Generates the context for JavaScript configuration by creating a formatted string\n",
    "        containing the user's question and a preview of the provided DataFrame.\n",
    "\n",
    "        Args:\n",
    "            human_message (str): The user's question.\n",
    "            df (DataFrame): The DataFrame containing the data.\n",
    "\n",
    "        Returns:\n",
    "            str: A formatted string with the user's question and the DataFrame preview.\n",
    "        \"\"\"\n",
    "\n",
    "        output_context = f'''\n",
    "        ------\n",
    "        USER'S QUESTION: `{human_message}`\n",
    "        ------\n",
    "        DATAFRAME: \\n{df.head(50).to_csv(index=False)}\n",
    "        ------'''\n",
    "        return output_context.strip()\n",
    "\n",
    "    def getting_prompt_for_js_highcharts(self, context_for_chart_generation):\n",
    "\n",
    "        \"\"\"\n",
    "        Constructs a system message prompt for generating Highcharts JS code based on the user's query and SQL response.\n",
    "\n",
    "        Args:\n",
    "            context_for_chart_generation (str): The SQL response to be used for generating the Highcharts JS code.\n",
    "\n",
    "        Returns:\n",
    "            str: A string containing the system message prompt for Highcharts JS code generation.\n",
    "        \"\"\"\n",
    "\n",
    "        ## Color Codes to Use\n",
    "\n",
    "        color_codes = [\n",
    "            \"#006292\", \"#1A729D\", \"#98D7FC\", \"#1AC5FB\", \"#1AABD9\", \"#1A73CB\", \"#4A90E5\", \"#6DEDD1\", \"#BEFBBF\", \"#83C38A\", \"#1A9D6D\",\n",
    "            \"#34C670\", \"#4ED1B6\", \"#28B78F\", \"#FFE886\", \"#F8EEBE\", \"#D59D50\", \"#EC8047\", \"#C36C41\", \"#FFA56F\", \"#EA654D\", \"#C64949\",\n",
    "            \"#00BFFA\", \"#00A2D5\", \"#0063C5\", \"#3684E2\", \"#5DEBCC\"]\n",
    "\n",
    "        system_message = f'''As a seasoned NodeJS developer with expertise in the 'highcharts' library, you are tasked with providing assistance on chart/plot related queries. Your responses should focus solely on delivering complete HTML, CSS, and JavaScript code snippets tailored to the specific requirements of each chart query based on the provided USER'S QUESTION AND DATAFRAME below.\n",
    "\n",
    "- Use the JavaScript code generated to render and display a graph/chart/table.\n",
    "- Provide a complete HTML document including HTML, CSS, and JavaScript code.\n",
    "\n",
    "- Strictly Follow the Response Template:\n",
    "    ```\n",
    "    <html>\n",
    "    <head>\n",
    "        <script src=\"https://code.highcharts.com/highcharts.js\"></script>\n",
    "        <script src=\"https://code.highcharts.com/modules/exporting.js\"></script>\n",
    "        <script src=\"https://code.highcharts.com/modules/export-data.js\"></script>\n",
    "        <script src=\"https://code.highcharts.com/modules/accessibility.js\"></script>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div id=\"container_id\"></div>\n",
    "        <script>\n",
    "        eval(Highcharts.chart('container_id', {{{{\n",
    "            credits: {{{{ enabled: false }}}},\n",
    "            exporting: {{{{\n",
    "                enabled: true,\n",
    "                buttons: {{{{\n",
    "                    contextButton: {{{{\n",
    "                        menuItems: ['downloadPNG', 'downloadPDF']\n",
    "                    }}}}\n",
    "                }}}}\n",
    "            }}}},\n",
    "            ...\n",
    "        }}}}));\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    ```\n",
    "# Render the graph after the entire text is loaded.\n",
    "# Replace the placeholder ... with appropriate elements.\n",
    "\n",
    "# Instruction for the Use of Colors: \n",
    "    ** Use distinct colors for each data point to enhance visual appeal.\n",
    "    ** Colors must be exclusively selected from the following list: {', '.join(color_codes)}.\n",
    "    ** Do not repeat colors unless the number of data points exceeds the color options.\n",
    "    ** The objective is to ensure the chart is visually engaging and easy to differentiate at a glance.\n",
    "\n",
    "# Chart Types: Line, Bar, Column, Pie, Scatter, Bubble etc. Use the most appropriate chart type based on the given data.\n",
    "\n",
    "# When dealing with Numbers inside the data: \n",
    "    - Must use the currency symbol appropriately ($, €) \n",
    "        - (valuePrefix: '$') or (valuePrefix: '€')\n",
    "    - Round numbers to two decimal places (valueDecimals: 2).\n",
    "    - Use commas (,) in between digits inside numbers (For this, Use - Highcharts.numberFormat).\n",
    "    - *Example Numbers: \n",
    "        - 36,614,583.26 (Encounter, Outcome Score etc where we are not talking currency symbol is not needed.)\n",
    "        - $36,614,583.26 (Spend/Price/Cost/Charge etc, where we need $ currecty symbol.)\n",
    "        - €36,614,583.26 (Spend/Price/Cost/Charge etc, where we need € currecty symbol.)\n",
    "\n",
    "# Use full month name whenever possible.\n",
    "# Must Use Proper Titles inside the tooltips.\n",
    "\n",
    "# Define the `tooltip` section inside Highcharts using a custom formatter function.\n",
    "# Follow these clear rules step-by-step:\n",
    "    1. **Always start the tooltip with the category label:**\n",
    "        - Use: `this.key` — never use `this.x` or fallback logic.\n",
    "        - It should be the first line of the tooltip.\n",
    "        - Example line of code:\n",
    "            ```javascript\n",
    "            let tooltip = `<b>$this.key</b><br/>`;\n",
    "            ```\n",
    "        - Only exception is for the date. If Date is given on X Axis - then use appropriate tooltip.\n",
    "\n",
    "    2. For each point in the tooltip (loop over this.points), use the following logic to decide the correct display format based on the series name:\n",
    "\n",
    "        - If the series name contains **'Spend'**, **'Cost'**, **'Price'**, or **'Charge'** (case-insensitive):\n",
    "            - Use a **$ currency symbol**, format to two decimal places, and add commas.\n",
    "            - Example: `Total Spend: <b>$36,614,583.26</b>`\n",
    "\n",
    "        - For all other types:\n",
    "            - Format as a plain number with two decimal places and commas.\n",
    "            - Example: `Volume Index: <b>14,284.68</b>`\n",
    "\n",
    "    3. Use `Highcharts.numberFormat(point.y, decimals, '.', ',')` to format numbers properly.\n",
    "    4. Return the final tooltip string after the loop.\n",
    "    5. Wrap the whole formatter inside:\n",
    "        ```\n",
    "        tooltip: \n",
    "            formatter: function () \n",
    "                // your logic here\n",
    "            shared: true\n",
    "        ```\n",
    "\n",
    "# Do not skip any condition or return incomplete tooltip content.\n",
    "# Be sure to apply the correct number formatting and symbol for each type.\n",
    "# Always ensure tooltips are clean, aligned with the series meaning, and easily readable.\n",
    "# If the question is asked in different language, use the same language to provide the response and also use relevant currency symbol.\n",
    "# Your response should only contain the HTML code following the provided template.\n",
    "------------------\n",
    "{context_for_chart_generation}\n",
    "----------------\n",
    "HIGHCHARTS GRAPH:\n",
    "'''\n",
    "        return system_message.strip()\n",
    "\n",
    "    def highcharts_js_chain_function(self, context_for_chart_generation):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a language model chain to generate Highcharts JS code based on the provided SQL response.\n",
    "\n",
    "        Args:\n",
    "            context_for_chart_generation (str): The SQL response to be used for generating the Highcharts JS code.\n",
    "\n",
    "        Returns:\n",
    "            LLMChain: A chain object for generating Highcharts JS code.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the language model\n",
    "        # llm = ChatOpenAI(model = 'gpt-4o-2024-08-06', temperature=0)\n",
    "        llm = ChatOpenAI(model='gpt-4.1-2025-04-14', temperature=0)\n",
    "\n",
    "\n",
    "        # System Prompt\n",
    "        system_message_prompt = self.getting_prompt_for_js_highcharts(context_for_chart_generation)\n",
    "        \n",
    "        # Combining system and human prompts\n",
    "        final_prompt_template_for_highcharts_js = PromptTemplate.from_template(system_message_prompt)\n",
    "\n",
    "        # Create a basic single chain\n",
    "        final_chain = final_prompt_template_for_highcharts_js | llm | StrOutputParser()\n",
    "\n",
    "        return final_chain\n",
    "\n",
    "    def saving_visualization_content_as_html(self, html_content, user_id):\n",
    "        \"\"\"\n",
    "        Saves the provided HTML content to a file with a unique name.\n",
    "\n",
    "        Args:\n",
    "            html_content (str): The HTML content to be saved.\n",
    "\n",
    "        Returns:\n",
    "            str: The file path of the saved HTML file.\n",
    "        \"\"\"\n",
    "\n",
    "        ## Create the Directory if not Available yet\n",
    "        folder_directory = f'visualization/{user_id}'\n",
    "        os.makedirs(folder_directory, exist_ok=True)    \n",
    "\n",
    "        ## Exporting Image\n",
    "        unique_identifier = str(uuid.uuid4()).split('-')[-1]\n",
    "        image_file_name_with_path = f\"{folder_directory}/viz_{unique_identifier}.html\"\n",
    "\n",
    "        html_content = html_content.replace('let tooltip = `<b>${this.x}</b><br/>`','let tooltip = `<b>${this.key}</b><br/>`').strip()\n",
    "        # Open a new file in write mode and write the HTML content\n",
    "        with open(image_file_name_with_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(html_content)\n",
    "\n",
    "        image_name = image_file_name_with_path.split('/')[-1]\n",
    "        return image_file_name_with_path, image_name\n",
    "    \n",
    "    def get_highchart_info(self, human_message, df, user_id):\n",
    "\n",
    "        if self.function_to_check_for_chart_possibility(df):\n",
    "            context_for_chart_generation = self.context_generation_for_js_config(human_message, df)\n",
    "            highcharts_js_chain = self.highcharts_js_chain_function(context_for_chart_generation)\n",
    "            html_content = highcharts_js_chain.invoke({})\n",
    "            html_content = html_content.replace('`html','').strip('`').strip()\n",
    "            saved_image_file_path, saved_image_name = self.saving_visualization_content_as_html(html_content, user_id)\n",
    "            return saved_image_file_path, saved_image_name, html_content\n",
    "        else:\n",
    "            return None, None, None\n",
    "      \n",
    "hichart_module = HighchartModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_image_file_path, saved_image_name, html_content = hichart_module.get_highchart_info(user_prompt_to_use, df, user_id)\n",
    "# saved_image_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DynamicRecommendationChainManager | Endpoint: /recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicRecommendationChainManager:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "\n",
    "    def classify_role_and_generate_context(self, role_input):\n",
    "        \"\"\"\n",
    "        Uses the LLM to classify the user's role into a hierarchy and derive responsibilities dynamically.\n",
    "        If role_input is blank, returns a general recommendation context.\n",
    "\n",
    "        Args:\n",
    "            role_input (str): The user's role or title (e.g., \"CMO\", \"Chief Marketing Officer\").\n",
    "\n",
    "        Returns:\n",
    "            str: A classification response or general recommendation context.\n",
    "        \"\"\"\n",
    "        if not role_input.strip():\n",
    "            # Return general recommendation context if no role is provided\n",
    "            return \"- Management Level: General Management\\n\" \\\n",
    "                \"- Role Responsibilities: Responsible for general data analysis and insights to identify trends, \" \\\n",
    "                \"opportunities, and risks. Recommendations should focus on actionable insights derived from the data.\"\n",
    "\n",
    "        # Classification prompt for non-blank role inputs\n",
    "        classification_prompt = f'''\n",
    "        You are an AI assistant specializing in organizational hierarchy and role classification.\n",
    "        Based on the user-provided role or title, identify the appropriate management level \n",
    "        (e.g., Executive Leadership, Senior Management, Mid-Level Management, Operational Management, etc.) \n",
    "        and provide a concise description of the responsibilities associated with this role.\n",
    "\n",
    "        Role Provided: \"{role_input}\"\n",
    "\n",
    "        Respond with:\n",
    "        - Management Level: (e.g., \"Executive Leadership\")\n",
    "        - Role Responsibilities: (e.g., \"Responsible for overseeing high-level strategic operations...\")\n",
    "        '''\n",
    "\n",
    "        chat_prompt_template_for_classification = PromptTemplate.from_template(classification_prompt)\n",
    "        dynamic_recommendation_chain_classification = chat_prompt_template_for_classification | self.llm | StrOutputParser()\n",
    "        classification_response = dynamic_recommendation_chain_classification.invoke({}).strip()\n",
    "        return classification_response\n",
    "\n",
    "    def generate_dynamic_recommendation_prompt(self, question, df, role_input):\n",
    "        \"\"\"\n",
    "        Generates a system message prompt for dynamic role-based recommendations.\n",
    "\n",
    "        Args:\n",
    "            question (str): User's query or question.\n",
    "            df (DataFrame): The DataFrame containing data for analysis.\n",
    "            role_input (str): The user's role or title.\n",
    "\n",
    "        Returns:\n",
    "            str: A system message prompt for generating dynamic recommendations.\n",
    "        \"\"\"\n",
    "        role_context = self.classify_role_and_generate_context(role_input)\n",
    "\n",
    "        lines = role_context.split(\"\\n\")\n",
    "        management_level = next((line.split(\":\")[1].strip() for line in lines if line.startswith(\"- Management Level:\")), \"General Management\")\n",
    "        role_responsibilities = next((line.split(\":\")[1].strip() for line in lines if line.startswith(\"- Role Responsibilities:\")), \"General responsibilities for this role.\")\n",
    "\n",
    "        print(f'\\n==============================\\nmanagement_level: {management_level}')\n",
    "        print(f'role_responsibilities: {role_responsibilities}\\n==============================\\n')\n",
    "\n",
    "        if len(df) > 100:\n",
    "            display_size_note = f\"**Note: The DataFrame includes the first 50 entries out of a total of {len(df)} rows. Let the user know about it.\"\n",
    "        else:\n",
    "            display_size_note = \"\"\n",
    "\n",
    "        system_message_prompt = f'''You are an AI Chatbot for Supplycopia that provides data-driven recommendations based on user roles and responsibilities.\n",
    "\n",
    "Role Context:\n",
    "- Management Level: {management_level}\n",
    "- Role Responsibilities: {role_responsibilities}\n",
    "\n",
    "Recommendation Guidelines:\n",
    "1. Focus on actionable recommendations derived from DataFrame analysis\n",
    "2. Each recommendation must:\n",
    "   - Link directly to data evidence\n",
    "   - Align with user's role/responsibilities\n",
    "   - Address specific question context\n",
    "   - Include quantifiable metrics where possible\n",
    "\n",
    "3. Prioritize by:\n",
    "   - Impact (based on data)\n",
    "   - Implementation feasibility \n",
    "   - Role relevance\n",
    "   - Urgency (if applicable)\n",
    "\n",
    "4. Format:\n",
    "   - Start with clear recommendation (Elaborately)\n",
    "   - Support with specific data points\n",
    "   - End with expected outcome\n",
    "\n",
    "{display_size_note}\n",
    "\n",
    "Tone: Professional and direct\n",
    "\n",
    "Query: {question}\n",
    "DataFrame: {df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "'''\n",
    "        \n",
    "        return system_message_prompt\n",
    "\n",
    "    def run_dynamic_recommendation_chain(self, question, df, role_input=''):\n",
    "        \"\"\"\n",
    "        Runs the dynamic recommendation chain to generate tailored recommendations.\n",
    "\n",
    "        Args:\n",
    "            question (str): User's query or question.\n",
    "            df (DataFrame): The DataFrame containing data for analysis.\n",
    "            role_input (str): The user's role or title.\n",
    "\n",
    "        Returns:\n",
    "            str: Recommendations based on the provided query and data.\n",
    "        \"\"\"\n",
    "        system_message_prompt = self.generate_dynamic_recommendation_prompt(question, df, role_input)\n",
    "        chat_prompt_template = PromptTemplate.from_template(system_message_prompt)\n",
    "        dynamic_recommendation_chain = chat_prompt_template | self.llm | StrOutputParser()\n",
    "        query_response = dynamic_recommendation_chain.invoke({})\n",
    "        query_response = query_response.strip()\n",
    "        return query_response\n",
    "\n",
    "recommendation_module = DynamicRecommendationChainManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendations = recommendation_module.run_dynamic_recommendation_chain(user_prompt_to_use, df, '')\n",
    "# # # recommendations = dynamic_recommendation_chain_manager.run_dynamic_recommendation_chain(user_prompt_to_use, df, 'Software')\n",
    "\n",
    "# # print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FollowUpQuestions - Recommending two questions based on the previously asked question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Approach -> FollowUpQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Define the FollowUpQuestions model\n",
    "# class FollowUpQuestions(BaseModel):\n",
    "#     questions: List[str]\n",
    "\n",
    "#     def to_list(self) -> List[str]:\n",
    "#         return self.questions\n",
    "    \n",
    "# class QuestionSuggestionsGenerator:\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         Initializes the QuestionSuggestionsGenerator with a language model.\n",
    "#         \"\"\"\n",
    "#         self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "#         self.structured_llm = self.llm.with_structured_output(FollowUpQuestions)\n",
    "\n",
    "#     def generate_system_message(self, user_prompt: str, generated_sql_query: str, table_to_use: str, table_columns: list) -> str:\n",
    "#         \"\"\"\n",
    "#         Generates the system message prompt for suggesting questions.\n",
    "\n",
    "#         Args:\n",
    "#             user_prompt (str): The original user's prompt.\n",
    "#             table_to_use (str): The name of the table being used.\n",
    "#             table_columns (list): List of available columns in the table.\n",
    "#             generated_sql_query (str): Sql query generated to get the result.\n",
    "\n",
    "\n",
    "#         Returns:\n",
    "#             str: The formatted system message prompt.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Get today's date\n",
    "#         today_date = datetime.now()\n",
    "#         # Format the date as YYYY-MM-DD\n",
    "#         formatted_date = today_date.strftime('%Y-%m-%d')\n",
    "\n",
    "#         system_message = f'''\n",
    "# You are an AI assistant tasked with generating two relevant follow-up questions based on the user's current query and the SQL query executed to obtain the results.\n",
    "\n",
    "# **Context:**\n",
    "# - **Table:** {table_to_use}\n",
    "# - **Columns:** {', '.join(table_columns)}\n",
    "# - **Generated SQL Query:** {generated_sql_query}\n",
    "\n",
    "# **Guidelines:**\n",
    "# 1. **Relevance:** Each follow-up question must directly relate to the user's original query and the data retrieved by the SQL query.\n",
    "# 2. **Simplicity:** Generate simple and clear questions that are easy to understand.\n",
    "# 3. **Column Transformation:** Transform column names to their full business names when generating follow-up questions:\n",
    "#    - `order_id` → `Order Number`\n",
    "#    - `cust_name` → `Customer Name`\n",
    "#    - `ord_date` → `Order Date`\n",
    "# 4. **Focus Areas:** Concentrate on basic counting, summing, or simple aggregations related to the data.\n",
    "# 5. **Conciseness:** Keep each question under 10 words.\n",
    "# 6. **Direct Answerability:** Ensure questions can be answered by direct lookups without requiring complex computations.\n",
    "# 7. **Single Metrics/Periods:** Target single metrics or specific time periods to maintain focus.\n",
    "# 8. **Avoid Transformations:** Do not include data transformations or complex operations in the questions.\n",
    "# 9. **Use Available Columns:** Base questions solely on the columns available in the specified table.\n",
    "# 10. **Date Reference:** Today's Date: {formatted_date}\n",
    "# 11. **SQL Utilization:** Use the `generated_sql_query` to inform and enhance the relevance of the follow-up questions.\n",
    "\n",
    "# **User Query:** {user_prompt}\n",
    "\n",
    "# **Format:**\n",
    "# [\"...\", \"...\"]\n",
    "\n",
    "# **Note:** Provide only the numbered questions without any explanations or additional text.\n",
    "# '''\n",
    "\n",
    "#         return system_message.strip()\n",
    "\n",
    "#     def generate_question_chain(self, user_prompt: str, generated_sql_query: str, table_to_use: str, table_columns: list):\n",
    "#         \"\"\"\n",
    "#         Creates a language model chain to generate question suggestions.\n",
    "\n",
    "#         Args:\n",
    "#             user_prompt (str): The user's original query.\n",
    "#             table_to_use (str): The name of the table being used.\n",
    "#             table_columns (list): List of available columns in the table.\n",
    "\n",
    "#         Returns:\n",
    "#             Chain: A chain object for generating question suggestions.\n",
    "#         \"\"\"\n",
    "#         system_message = self.generate_system_message(user_prompt, generated_sql_query, table_to_use, table_columns)\n",
    "        \n",
    "#         # Create the system message prompt template\n",
    "#         system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "        \n",
    "#         # Create the human message prompt\n",
    "#         human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "#             \"Generate two question suggestions based on this query.\"\n",
    "#         )\n",
    "\n",
    "#         # Combine prompts into a chat prompt template\n",
    "#         chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "#             system_message_prompt,\n",
    "#             human_message_prompt\n",
    "#         ])\n",
    "\n",
    "#         # Create the chain\n",
    "#         question_chain = chat_prompt_template | self.structured_llm\n",
    "\n",
    "#         return question_chain\n",
    "\n",
    "#     def get_suggestions(self, user_prompt: str, generated_sql_query: str, table_to_use: str, table_columns: list) -> FollowUpQuestions:\n",
    "#         \"\"\"\n",
    "#         Gets question suggestions based on the user's prompt and table information.\n",
    "\n",
    "#         Args:\n",
    "#             user_prompt (str): The user's original query.\n",
    "#             table_to_use (str): The name of the table being used.\n",
    "#             table_columns (list): List of available columns in the table.\n",
    "\n",
    "#         Returns:\n",
    "#             str: Two suggested follow-up questions.\n",
    "#         \"\"\"\n",
    "\n",
    "#         random.shuffle(table_columns)\n",
    "\n",
    "#         question_chain = self.generate_question_chain(user_prompt, generated_sql_query, table_to_use, table_columns)\n",
    "#         suggestions = question_chain.invoke({})\n",
    "\n",
    "#         return cast(FollowUpQuestions, suggestions)\n",
    "    \n",
    "# # Initialize the Question Suggestions Generator\n",
    "# question_suggestions_generator = QuestionSuggestionsGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Approach -> FollowUpQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# Define the FollowUpQuestions model\n",
    "class FollowUpQuestions(BaseModel):\n",
    "    questions: List[str]\n",
    "\n",
    "    def to_list(self) -> List[str]:\n",
    "        return self.questions\n",
    "\n",
    "class QuestionSuggestionsGenerator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.llm = ChatOpenAI(model='gpt-4.1', temperature=0.5)\n",
    "        self.structured_llm = self.llm.with_structured_output(FollowUpQuestions)\n",
    "\n",
    "    # 🔄 Updated to accept past_questions\n",
    "    def generate_system_message(\n",
    "        self, \n",
    "        user_prompt: str, \n",
    "        generated_sql_query: str, \n",
    "        table_to_use: str, \n",
    "        table_columns: list,\n",
    "        past_questions: List[str]  # 🔄\n",
    "    ) -> str:\n",
    "        today_date = datetime.now()\n",
    "        formatted_date = today_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        past_questions_str = \"\\n\".join([f\"- {q}\" for q in past_questions]) if past_questions else \"None\"\n",
    "\n",
    "        system_message = f\"\"\"\n",
    "You are an AI assistant tasked with generating **two concise, relevant, and professional follow-up questions** based on the user's current query and the SQL query executed to retrieve the results.\n",
    "\n",
    "### Context\n",
    "- **Table:** {table_to_use}\n",
    "- **Columns:** {', '.join(table_columns)}\n",
    "- **Executed SQL Query:** {generated_sql_query}\n",
    "\n",
    "### Previously Asked Questions\n",
    "{past_questions_str}\n",
    "\n",
    "### Guidelines for Generating Follow-Up Questions\n",
    "\n",
    "1. **Relevance:** Each question must be directly relevant to the user's current query and the SQL results.\n",
    "2. **No Repetition:** Do NOT repeat or rephrase any questions already listed above.\n",
    "3. **Clarity:** Use clear, direct, and simple language. Avoid overly technical or verbose phrasing.\n",
    "4. **Professional Tone:** Use a formal, business-appropriate tone. Avoid casual or conversational style.\n",
    "5. **Column Name Mapping:** Translate technical column names to user-friendly business terms where applicable:\n",
    "   - `order_id` → `Order Number`\n",
    "   - `cust_name` → `Customer Name`\n",
    "   - `ord_date` → `Order Date`\n",
    "6. **Answerability:** Ensure each question can be answered using simple aggregations (e.g., COUNT, SUM) or direct lookups from the data. No advanced calculations or transformations should be assumed.\n",
    "7. **Single Focus:** Target one metric or time period per question for clarity and specificity.\n",
    "8. **Use Only Available Columns:** Do not introduce new concepts or columns that are not in the provided list.\n",
    "9. **No Hardcoded Entities:** Avoid using specific entity names (e.g., “orthopedic”, “SELLERS, MATTHEW B”, “Texas”) **unless they appear in the user's current or past questions.**\n",
    "10. **Hierarchy-Driven Suggestions:** Follow the semantic drill-down hierarchy to guide deeper exploration of the dataset:\n",
    "    → Market  \n",
    "    → Facility  \n",
    "    → Service Line / Business Line  \n",
    "    → Procedure / Physician  \n",
    "    → Contract Category / UNSPSC  \n",
    "    → Products  \n",
    "\n",
    "    - If the user asks about a higher-level dimension (e.g., Market), generate questions at the next lower level (e.g., Facility).\n",
    "    - If the user focuses on Physicians, suggest questions about their Procedures or Products.\n",
    "    - Do not suggest questions that go *up* the hierarchy.\n",
    "11. **Avoid Redundancy:** Suggested questions must **not** be semantically similar, layered, or hierarchical variants of one another (e.g., “total encounter for top service line” vs. “top facility in top service line”).\n",
    "12. **Avoid Transformations:** Do not suggest derived metrics or complex computed features.\n",
    "13. **Leverage SQL Output:** Use the `generated_sql_query` to inform what metrics or dimensions have already been used, and build logically connected but distinct follow-ups.\n",
    "\n",
    "14. Clarify Ambiguous Entities:\n",
    "When a question includes a specific value (e.g., a name or location) without explicitly stating its entity type (such as market, facility, physician, etc.), prepend the appropriate entity label to ensure clarity and avoid ambiguity. This helps downstream systems understand and map the query correctly.\n",
    "Example:\n",
    "❌ “What is the total cost for Horizon West?”\n",
    "✅ “What is the total cost for market Horizon West?”\n",
    "\n",
    "15. **Current Date Reference:** Today's date is {formatted_date} – include it only if the question context requires a temporal reference.\n",
    "\n",
    "### Input\n",
    "**User Query:** {user_prompt}\n",
    "\n",
    "### Output Format\n",
    "A list of exactly two distinct, valid follow-up questions:\n",
    "[\"<question_1>\", \"<question_2>\"]\n",
    "\n",
    "### Important\n",
    "- Only return the list. No explanations, commentary, or additional formatting.\n",
    "\"\"\"\n",
    "        return system_message.strip()\n",
    "\n",
    "    # 🔄 Updated to accept past_questions\n",
    "    def generate_question_chain(\n",
    "        self, \n",
    "        user_prompt: str, \n",
    "        generated_sql_query: str, \n",
    "        table_to_use: str, \n",
    "        table_columns: list,\n",
    "        past_questions: List[str]  # 🔄\n",
    "    ):\n",
    "        system_message = self.generate_system_message(\n",
    "            user_prompt, generated_sql_query, table_to_use, table_columns, past_questions\n",
    "        )\n",
    "\n",
    "        system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "        human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "            \"Generate two question suggestions based on this query.\"\n",
    "        )\n",
    "\n",
    "        chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            system_message_prompt,\n",
    "            human_message_prompt\n",
    "        ])\n",
    "\n",
    "        return chat_prompt_template | self.structured_llm\n",
    "\n",
    "    # 🔄 Updated to accept past_questions\n",
    "    def get_suggestions(\n",
    "        self, \n",
    "        user_prompt: str, \n",
    "        generated_sql_query: str, \n",
    "        table_to_use: str, \n",
    "        table_columns: list,\n",
    "        past_questions: List[str] = []  # 🔄 optional\n",
    "    ) -> FollowUpQuestions:\n",
    "\n",
    "        random.shuffle(table_columns)\n",
    "\n",
    "        question_chain = self.generate_question_chain(\n",
    "            user_prompt, generated_sql_query, table_to_use, table_columns, past_questions\n",
    "        )\n",
    "        suggestions = question_chain.invoke({})\n",
    "\n",
    "        return cast(FollowUpQuestions, suggestions)\n",
    "\n",
    "## Question Suggestion Generator\n",
    "question_suggestions_generator = QuestionSuggestionsGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_questions = [\n",
    "#     msg.content\n",
    "#     for msg in memory_manager.store[user_id].messages\n",
    "#     if isinstance(msg, HumanMessage)\n",
    "# ][-8:]\n",
    "\n",
    "# suggestions = question_suggestions_generator.get_suggestions(\n",
    "#     user_prompt= user_prompt_to_use,\n",
    "#     generated_sql_query=generated_sql_query,\n",
    "#     table_to_use=table_to_use,\n",
    "#     table_columns=table_columns,\n",
    "#     past_questions=previous_questions # 👈 Prevents repeats\n",
    "# )\n",
    "\n",
    "# print(suggestions.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: yellow; font-weight: bold;\">Charting_Json | Endpoint: /charting_json</span> (Not In Use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "        \"#006292\", \"#1A729D\", \"#98D7FC\", \"#1AC5FB\", \"#1AABD9\", \"#1A73CB\", \"#4A90E5\",\n",
    "        \"#6DEDD1\", \"#BEFBBF\", \"#83C38A\", \"#1A9D6D\", \"#34C670\", \"#4ED1B6\", \"#28B78F\",\n",
    "        \"#FFE886\", \"#F8EEBE\", \"#D59D50\", \"#EC8047\", \"#C36C41\", \"#FFA56F\", \"#EA654D\",\n",
    "        \"#C64949\", \"#00BFFA\", \"#00A2D5\", \"#0063C5\", \"#3684E2\", \"#5DEBCC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChartRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChartRecommender:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "\n",
    "    def get_column_types(self, df):\n",
    "        \"\"\"\n",
    "        Provides the number of numeric and categorical columns in the DataFrame,\n",
    "        using select_dtypes primarily, and a fallback mechanism when all columns are initially identified as categorical.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to analyze.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with counts of numeric and categorical columns.\n",
    "        \"\"\"\n",
    "        # Step 1: Primary method using select_dtypes\n",
    "        numeric_columns = df.select_dtypes(include=['float64', 'int64', 'uint8', 'int32', 'float32']).columns\n",
    "        categorical_columns = df.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "\n",
    "        # Step 2: Check if all columns are classified as categorical\n",
    "        if len(numeric_columns) == 0 and len(categorical_columns) == len(df.columns):\n",
    "            # Fallback mechanism: Iterate through all columns to detect numeric columns\n",
    "            numeric_columns = []\n",
    "            categorical_columns = []\n",
    "            for col in df.columns:\n",
    "                try:\n",
    "                    # Try converting the column to float\n",
    "                    df[col].astype(float)\n",
    "                    numeric_columns.append(col)\n",
    "                except ValueError:\n",
    "                    # If conversion fails, classify as categorical\n",
    "                    categorical_columns.append(col)\n",
    "\n",
    "        # Step 3: Return the counts\n",
    "        return {\n",
    "            \"numeric_columns\": len(numeric_columns),\n",
    "            \"categorical_columns\": len(categorical_columns)\n",
    "        }\n",
    "    \n",
    "    def get_suitable_charts_from_dataframe(self, df):\n",
    "        \"\"\"\n",
    "        Performs basic data validation to determine if charts are technically possible,\n",
    "        leaving final chart suitability decisions to the LLM.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to analyze.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing technically possible charts and column counts.\n",
    "        \"\"\"\n",
    "        # Critical validation: Empty DataFrame or single row\n",
    "        # Basic data validation to check minimum chart requirements\n",
    "        if (\n",
    "            df.empty or  # Empty dataframe\n",
    "            len(df) < 2 or  # Minimum 2 rows needed for visualization\n",
    "            len(df.columns) < 2  # Minimum 2 columns needed\n",
    "        ):\n",
    "            return {\n",
    "                'suitable_charts': [],\n",
    "                'numeric_columns': 0,\n",
    "                'categorical_columns': 0\n",
    "            }\n",
    "\n",
    "        column_types = self.get_column_types(df)\n",
    "        numeric_count = column_types['numeric_columns']\n",
    "        categorical_count = column_types['categorical_columns']\n",
    "        suitable_charts = []\n",
    "\n",
    "        # Bubble: needs at least 3 numeric columns for x, y, and size\n",
    "        if numeric_count >= 3:\n",
    "            suitable_charts.append('bubble')\n",
    "\n",
    "        # Scatter: needs at least 2 numeric columns for x and y\n",
    "        if numeric_count >= 2:\n",
    "            suitable_charts.append('scatter')\n",
    "\n",
    "        # Basic minimum requirements for technical feasibility\n",
    "        # Line charts: need at least 2 points to form a line\n",
    "        if numeric_count >= 1 and len(df) >= 2:\n",
    "            suitable_charts.extend(['area', 'line'])\n",
    "\n",
    "        # Pie: needs exactly 1 numeric for values and 1 categorical for segments\n",
    "        if numeric_count >= 1 and categorical_count >= 1:\n",
    "            suitable_charts.append('pie')\n",
    "\n",
    "        # Bar/Column: need at least 1 category and 1 numeric value\n",
    "        if numeric_count >= 1 and categorical_count >= 1:\n",
    "            suitable_charts.extend(['column', 'bar'])\n",
    "\n",
    "\n",
    "        # Critical data quality check: remove all charts if any column is entirely NULL\n",
    "        if df.isna().all().any():\n",
    "            suitable_charts = []\n",
    "\n",
    "        return {\n",
    "            'suitable_charts': suitable_charts,\n",
    "            'numeric_columns': numeric_count,\n",
    "            'categorical_columns': categorical_count\n",
    "        }\n",
    "    \n",
    "    def generate_chart_selection_guidelines(self, suitable_charts):\n",
    "        \"\"\"\n",
    "        Generates dynamic chart selection guidelines based on the list of suitable charts.\n",
    "        \"\"\"\n",
    "        chart_guidelines = {\n",
    "            'line': \"\"\"**Line Charts**:\n",
    "- **Use When**: Displaying trends or changes over time or continuous intervals.\n",
    "- **Data Requirements**:\n",
    "    - X-axis: Continuous or time-based variable with regular intervals.\n",
    "    - Y-axis: Numeric variable (e.g., totals, averages).\"\"\",\n",
    "                \n",
    "            'area': \"\"\"**Area Charts**:\n",
    "- **When to Select**: \n",
    "    - To visualize cumulative totals over time.\n",
    "    - To emphasize the magnitude of trends across multiple categories.\n",
    "    - To compare relative contributions using percentage-stacked area charts.\n",
    "- **Required Data Characteristics**:\n",
    "    - X-axis: Time-based or sequential data.\n",
    "    - Y-axis: Numeric values that can be aggregated or compared across categories.\n",
    "- **Selection Guidelines**:\n",
    "    - Prefer for datasets with multiple numeric categories that can be stacked or compared.\n",
    "- **Avoid Selection If**:\n",
    "    - The dataset has a single numeric series (use a line or bar chart instead).\n",
    "    - Individual category values need precise interpretation (stacked charts may obscure this).\n",
    "- **Special Considerations**:\n",
    "    - Always include a zero baseline for accurate representation.\n",
    "    - Arrange stacked groups with the largest or most stable values at the bottom for better readability.\"\"\",\n",
    "                \n",
    "            'bar': \"\"\"**Bar Charts**:\n",
    "- **Use When**: Comparing categories with long names or a large number of categories.\n",
    "- **Data Requirements**:\n",
    "    - X-axis: Numeric values.\n",
    "    - Y-axis: Categorical variable (e.g., labels, distinct groups).\"\"\",\n",
    "                \n",
    "            'column': \"\"\"**Column Charts**:\n",
    "- **Use When**: Comparing discrete categories with numerical values, especially for ordered or sequential data.\n",
    "- **Data Requirements**:\n",
    "    - X-axis: Categorical variable or sequentially ordered categories (e.g., months, years).\n",
    "    - Y-axis: Numeric values.\"\"\",\n",
    "                \n",
    "            'pie': \"\"\"**Pie Charts**:\n",
    "- **Use When**: Showing part-to-whole relationships or percentages.\n",
    "- **Data Requirements**:\n",
    "    - Categories that sum to a whole (e.g., 100%) with corresponding numeric values.\"\"\",\n",
    "                \n",
    "            'scatter': \"\"\"**Scatter Plots**:\n",
    "- **Use When**: Exploring relationships or correlations between two numeric variables.\n",
    "- **Data Requirements**:\n",
    "    - X-axis: Numeric variable.\n",
    "    - Y-axis: Numeric variable.\"\"\",\n",
    "            \n",
    "            'bubble': \"\"\"**Bubble Charts**:\n",
    "- **Use When**: Visualizing relationships among three numeric variables, optionally grouped by a categorical variable, to compare magnitudes or identify correlations.\n",
    "- **Data Requirements**:\n",
    "    - **X-axis**: Numeric variable (independent or comparison metric).\n",
    "    - **Y-axis**: Numeric variable (response or comparison metric).\n",
    "    - **Bubble Size**: Third numeric variable (magnitude or intensity).\n",
    "    - **Optional**: Categorical variable for grouping (e.g., Markets, Facilities).\n",
    "- **Considerations**:\n",
    "    - Prioritize for queries with three numeric variables showing relationships or magnitudes.\n",
    "    - Ensure variation in bubble sizes and validate all variables are non-null.\n",
    "    - Avoid if data has excessive categories or fewer than three variables.\n",
    "    - Handle overlaps with transparency or tooltips if required.\"\"\"\n",
    "        }\n",
    "\n",
    "        guidelines = []\n",
    "        for idx, chart in enumerate(suitable_charts, start=1):\n",
    "            if chart in chart_guidelines:\n",
    "                guidelines.append(f\"{idx}. {chart_guidelines[chart]}\")\n",
    "\n",
    "        return \"\\n\\n\".join(guidelines)\n",
    "\n",
    "    def generate_chart_selection_prompt(self, user_query, sql_query, df):\n",
    "    # Get the column types\n",
    "        column_types = self.get_suitable_charts_from_dataframe(df)\n",
    "        # print(json.dumps(column_types, indent=2))\n",
    "        suitable_charts = column_types['suitable_charts']\n",
    "        \n",
    "        chart_selection_guideline_to_be_used = self.generate_chart_selection_guidelines(suitable_charts)\n",
    "        # print(chart_selection_guideline_to_be_used)\n",
    "\n",
    "        system_message = f'''\n",
    "You are a data visualization expert tasked with selecting the most appropriate chart types for visualizing data. Based on the user's query, SQL query, and data structure, recommend one or more of the following chart types: **column**, **bar**, **line**, **pie**, **bubble**, **scatter**. If none are suitable, return an empty list.\n",
    "\n",
    "**Chart Selection Guidelines**:\n",
    "{chart_selection_guideline_to_be_used}\n",
    "\n",
    "**Context Provided**:\n",
    "===============================================================================\n",
    "User Query: {user_query}\n",
    "SQL Query: {sql_query}\n",
    "DataFrame Preview:\n",
    "{df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "===============================================================================\n",
    "\n",
    "**Additional Information**:\n",
    "- Only return the appropriate chart or charts from this list: {suitable_charts}\n",
    "\n",
    "**Instructions**:\n",
    "1. **Understand the Context**:\n",
    "   - Analyze the user's query for intent.\n",
    "   - Examine the data structure from the DataFrame preview.\n",
    "   - Consider SQL query operations (e.g., aggregations, filters).\n",
    "   - Take into account the preliminary chart suggestions based on the column types.\n",
    "\n",
    "2. **Select Suitable Chart Types**:\n",
    "   - Refer to the **Chart Selection Guidelines** to choose the most appropriate chart type(s).\n",
    "   - Match the data structure and user intent with the chart requirements.\n",
    "\n",
    "3. **Output Format**:\n",
    "   - Return a Python list of chart type names in lowercase (e.g., `['column', 'line']`).\n",
    "   - If no chart type is suitable, return an empty list `[]`.\n",
    "   - Do not include explanations, comments, or any additional text.\n",
    "\n",
    "**Example Output**:\n",
    "['chart_type_1', 'chart_type_2']\n",
    "'''\n",
    "        return system_message.strip(), column_types\n",
    "\n",
    "    def get_suitable_charts(self, user_query, sql_query, df):\n",
    "        # Create the prompt template\n",
    "        system_message, column_types = self.generate_chart_selection_prompt(user_query, sql_query, df)\n",
    "        suitable_charts = column_types['suitable_charts']\n",
    "\n",
    "        if suitable_charts == []:\n",
    "            chart_types = []\n",
    "            return chart_types, column_types\n",
    "        \n",
    "        prompt_template = PromptTemplate.from_template(system_message)\n",
    "        \n",
    "        # Create and run the chain\n",
    "        chart_selection_chain = prompt_template | self.llm | StrOutputParser()\n",
    "        result = chart_selection_chain.invoke({})\n",
    "        \n",
    "        # Clean and process the result\n",
    "        result = result.lower().replace('[', '').replace(']', '').replace('\\'', '').replace('\"', '')\n",
    "        chart_types = [chart.strip() for chart in result.split(',')]\n",
    "\n",
    "        ## If we can create a line chart, then an area chart is also possible \n",
    "        if 'line' in chart_types:\n",
    "            chart_types.append('area')\n",
    "\n",
    "        chart_types = list(set(chart_types))    \n",
    "        return chart_types, column_types\n",
    "\n",
    "## Initializing ChartRecommender Module\n",
    "chart_recommender = ChartRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'user_prompt: {user_prompt}\\n')\n",
    "\n",
    "# # chart_prompt = chart_selection.generate_chart_selection_prompt(user_prompt, generated_sql_query, df)\n",
    "# # print(chart_prompt)\n",
    "\n",
    "# list_of_charts, suitable_chart_types = chart_recommender.get_suitable_charts(user_prompt, generated_sql_query, df)\n",
    "# print(json.dumps(suitable_chart_types, indent = 2))\n",
    "# print(f'\\nlist_of_charts: {list_of_charts}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  __ Charts Modules __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ColumnChartGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnChartGenerator:\n",
    "    def __init__(self, colors):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.colors = colors\n",
    "\n",
    "    def _generate_column_chart_prompt(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Generates the prompt for the LLM to analyze the data and suggest column chart configuration.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt for the LLM\n",
    "        \"\"\"\n",
    "        system_message = f'''\n",
    "You are an expert in data visualization specializing in creating Highcharts column chart configurations. Analyze the provided DataFrame and user query to generate the optimal column chart JSON configuration.\n",
    "\n",
    "DataFrame Preview:\n",
    "{df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Available Color Codes (use only these):\n",
    "{\", \".join(self.colors)}\n",
    "\n",
    "Follow this exact JSON template for column charts:\n",
    "{{{{\n",
    "  \"chart\": {{{{\n",
    "    \"type\": \"column\",\n",
    "    \"plotBorderWidth\": 1.5\n",
    "  }}}},\n",
    "  \"accessibility\": {{{{\n",
    "    \"description\": \"<accessibility_description>\"\n",
    "  }}}},\n",
    "  \"title\": {{{{\n",
    "    \"text\": \"<chart_title>\",\n",
    "    \"align\": \"<align>\"\n",
    "  }}}},\n",
    "  \"subtitle\": {{{{\n",
    "    \"text\": \"<chart_subtitle>\",\n",
    "    \"align\": \"<align>\"\n",
    "  }}}},\n",
    "  \"xAxis\": {{{{\n",
    "    \"categories\": [\"Category 1\", \"Category 2\", \"Category 3\", \"Category 4\"],\n",
    "    \"title\": {{{{ \"text\": \"<x_axis_title>\" }}}},\n",
    "    \"crosshair\": true,\n",
    "    \"gridLineWidth\": 1,\n",
    "    \"lineWidth\": 0,\n",
    "    \"allowDecimals\": false,\n",
    "    \"accessibility\": {{{{\n",
    "      \"rangeDescription\": \"<range_description>\",\n",
    "      \"description\": \"<x_axis_accessibility_description>\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"yAxis\": {{{{\n",
    "    \"title\": {{{{\n",
    "      \"text\": \"<y_axis_title>\",\n",
    "      \"align\": \"<align>\"\n",
    "    }}}},\n",
    "  }}}},\n",
    "  \"series\": [\n",
    "    {{{{\n",
    "      \"name\": \"<series_name>\",\n",
    "      \"data\": [<data_points>],\n",
    "      \"color\": \"<hex_color>\",\n",
    "      \"tooltip\": {{{{\n",
    "        \"pointFormat\": \"<point_format>\"\n",
    "      }}}},\n",
    "      \"dataLabels\": {{{{\n",
    "        \"enabled\": true,\n",
    "        \"format\": \"<format>\"\n",
    "      }}}}\n",
    "    }}}}\n",
    "  ],\n",
    "  \"plotOptions\": {{{{\n",
    "    \"column\": {{{{\n",
    "      \"pointPadding\": 0.2,\n",
    "      \"borderWidth\": 0,\n",
    "      \"groupPadding\": 0.1,\n",
    "      \"dataLabels\": {{{{\n",
    "        \"enabled\": true,\n",
    "        \"format\": \"{{{{point.y:,.2f}}}}\"\n",
    "      }}}}\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"legend\": {{{{\n",
    "    \"layout\": \"vertical\",\n",
    "    \"align\": \"right\"\n",
    "  }}}},\n",
    "  \"responsive\": {{{{\n",
    "    \"rules\": [\n",
    "      {{{{\n",
    "        \"condition\": {{{{ \"maxWidth\": 500 }}}},\n",
    "        \"chartOptions\": {{{{\n",
    "          \"legend\": {{{{\n",
    "            \"layout\": \"horizontal\",\n",
    "            \"align\": \"<align>\",\n",
    "            \"verticalAlign\": \"bottom\"\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}}\n",
    "    ]\n",
    "  }}}}\n",
    "}}}}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the DataFrame structure and user query to determine:\n",
    "   - Appropriate columns for x-axis (categories) and y-axis (values)\n",
    "   - Meaningful title and labels\n",
    "   - Proper series configuration\n",
    "\n",
    "2. Number Formatting Requirements:\n",
    "   - For currency values (Spend/Price/Cost/Charge):\n",
    "     * Use $ prefix (valuePrefix: '$')\n",
    "     * Format example: $36,614,583.26\n",
    "   - For non-currency values (like Encounter, Score):\n",
    "     * No currency symbol\n",
    "     * Format example: 36,614,583.26\n",
    "   - Always:\n",
    "     * Round to 2 decimal places\n",
    "     * Use commas for thousand separators\n",
    "     * Use Highcharts.numberFormat\n",
    "   - For percentages:\n",
    "     * Format to 1 decimal place\n",
    "     * Include % symbol\n",
    "\n",
    "3. Date and Text Requirements:\n",
    "   - Use full month names (e.g., \"January\" not \"Jan\")\n",
    "   - Use proper capitalizations in labels and titles\n",
    "\n",
    "4. The JSON must follow the exact template structure above\n",
    "5. Only use colors from the provided color list\n",
    "6. Return only the JSON, no explanations\n",
    "\n",
    "Generate the complete column chart JSON configuration:'''\n",
    "\n",
    "        return system_message.strip()\n",
    "\n",
    "    def _validate_and_clean_json(self, json_str: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validates and cleans the JSON string returned by the LLM.\n",
    "        \n",
    "        Args:\n",
    "            json_str (str): JSON string from LLM\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Cleaned and validated JSON dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove any markdown code block syntax if present\n",
    "            json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            chart_config = json.loads(json_str)\n",
    "            \n",
    "            # Basic validation\n",
    "            required_keys = ['chart', 'title', 'xAxis', 'yAxis', 'series', 'plotOptions']\n",
    "            for key in required_keys:\n",
    "                if key not in chart_config:\n",
    "                    raise ValueError(f\"Missing required key: {key}\")\n",
    "            \n",
    "            # Ensure 'type' is 'column'\n",
    "            if chart_config['chart'].get('type') != 'column':\n",
    "                chart_config['chart']['type'] = 'column'\n",
    "                \n",
    "            return chart_config\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON format: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error processing chart configuration: {str(e)}\")\n",
    "\n",
    "    def generate_chart_json(\n",
    "        self,\n",
    "        query: str,\n",
    "        df: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates a column chart JSON configuration using LLM and data analysis.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete column chart JSON configuration\n",
    "        \"\"\"\n",
    "        # Generate prompt for LLM\n",
    "        prompt = self._generate_column_chart_prompt(query, df)\n",
    "        \n",
    "        # Create chain\n",
    "        prompt_template = PromptTemplate.from_template(prompt)\n",
    "        chart_generation_chain = prompt_template | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Get JSON from LLM\n",
    "        json_response = chart_generation_chain.invoke({})\n",
    "        \n",
    "        # Validate and clean the JSON\n",
    "        chart_config = self._validate_and_clean_json(json_response)\n",
    "        \n",
    "        return chart_config\n",
    "    \n",
    "## Initialization \n",
    "column_chart_generator = ColumnChartGenerator(colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. BarChartGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarChartGenerator:\n",
    "    def __init__(self, colors):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.colors = colors\n",
    "\n",
    "    def _generate_bar_chart_prompt(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Generates the prompt for the LLM to analyze the data and suggest bar chart configuration.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt for the LLM\n",
    "        \"\"\"\n",
    "        system_message = f'''\n",
    "You are an expert in data visualization specializing in creating Highcharts bar chart configurations. Analyze the provided DataFrame and user query to generate the optimal bar chart JSON configuration.\n",
    "\n",
    "DataFrame Preview:\n",
    "{df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Available Color Codes (use only these):\n",
    "{\", \".join(self.colors)}\n",
    "\n",
    "Follow this exact JSON template for bar charts:\n",
    "{{{{\n",
    "  \"chart\": {{{{\n",
    "    \"type\": \"bar\",\n",
    "    \"plotBorderWidth\": 1\n",
    "  }}}},\n",
    "  \"accessibility\": {{{{\n",
    "    \"description\": \"<accessibility_description>\"\n",
    "  }}}},\n",
    "  \"title\": {{{{\n",
    "    \"text\": \"<chart_title>\",\n",
    "    \"align\": \"left\",\n",
    "    \"style\": {{{{\n",
    "      \"fontSize\": \"16px\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"subtitle\": {{{{\n",
    "    \"text\": \"<chart_subtitle>\",\n",
    "    \"align\": \"left\"\n",
    "  }}}},\n",
    "  \"xAxis\": {{{{\n",
    "    \"categories\": [\"Category 1\", \"Category 2\", \"Category 3\", \"Category 4\"],\n",
    "    \"title\": {{{{ \"text\": \"<x_axis_title>\" }}}},\n",
    "    \"crosshair\": true,\n",
    "    \"gridLineWidth\": 1,\n",
    "    \"lineWidth\": 0,\n",
    "    \"allowDecimals\": false,\n",
    "    \"accessibility\": {{{{\n",
    "      \"rangeDescription\": \"<range_description>\",\n",
    "      \"description\": \"<x_axis_accessibility_description>\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"yAxis\": {{{{\n",
    "    \"title\": {{{{\n",
    "      \"text\": \"<y_axis_title>\",\n",
    "      \"align\": \"high\"\n",
    "    }}}},\n",
    "    \"min\": 0,\n",
    "    \"max\": null,\n",
    "    \"labels\": {{{{\n",
    "      \"overflow\": \"justify\"\n",
    "    }}}},\n",
    "    \"gridLineWidth\": 0\n",
    "  }}}},\n",
    "  \"series\": [\n",
    "    {{{{\n",
    "      \"name\": \"<series_name>\",\n",
    "      \"data\": [<data_points>],\n",
    "      \"color\": \"<hex_color>\",\n",
    "      \"tooltip\": {{{{\n",
    "        \"pointFormat\": \"<point_format>\"\n",
    "      }}}},\n",
    "      \"dataLabels\": {{{{\n",
    "        \"enabled\": true,\n",
    "        \"format\": \"<format>\"\n",
    "      }}}}\n",
    "    }}}}\n",
    "  ],\n",
    "  \"plotOptions\": {{{{\n",
    "    \"bar\": {{{{\n",
    "      \"borderRadius\": \"50%\",\n",
    "      \"groupPadding\": 0.1,\n",
    "      \"dataLabels\": {{{{\n",
    "        \"enabled\": true,\n",
    "        \"format\": \"{{{{point.y:,.2f}}}}\"\n",
    "      }}}},\n",
    "      \"colorByPoint\": false\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"legend\": {{{{\n",
    "    \"layout\": \"vertical\",\n",
    "    \"borderWidth\": 1,\n",
    "    \"backgroundColor\": \"#FFFFFF\",\n",
    "    \"shadow\": true\n",
    "  }}}},\n",
    "  \"responsive\": {{{{\n",
    "    \"rules\": [\n",
    "      {{{{\n",
    "        \"condition\": {{{{ \"maxWidth\": 500 }}}},\n",
    "        \"chartOptions\": {{{{\n",
    "          \"legend\": {{{{\n",
    "            \"layout\": \"horizontal\",\n",
    "            \"align\": \"center\",\n",
    "            \"verticalAlign\": \"bottom\"\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}}\n",
    "    ]\n",
    "  }}}}\n",
    "}}}}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the DataFrame structure and user query to determine:\n",
    "   - Appropriate columns for x-axis (categories) and y-axis (values)\n",
    "   - Meaningful title and labels\n",
    "   - Proper series configuration\n",
    "\n",
    "2. Number Formatting Requirements:\n",
    "   - For currency values (Spend/Price/Cost/Charge):\n",
    "     * Use $ prefix (valuePrefix: '$')\n",
    "     * Format example: $36,614,583.26\n",
    "   - For non-currency values (like Encounter, Score):\n",
    "     * No currency symbol\n",
    "     * Format example: 36,614,583.26\n",
    "   - Always:\n",
    "     * Round to 2 decimal places\n",
    "     * Use commas for thousand separators\n",
    "     * Use Highcharts.numberFormat\n",
    "   - For percentages:\n",
    "     * Format to 1 decimal place\n",
    "     * Include % symbol\n",
    "\n",
    "3. Date and Text Requirements:\n",
    "   - Use full month names (e.g., \"January\" not \"Jan\")\n",
    "   - Use proper capitalizations in labels and titles\n",
    "\n",
    "4. The JSON must follow the exact template structure above\n",
    "5. Only use colors from the provided color list\n",
    "6. Return only the JSON, no explanations\n",
    "\n",
    "Generate the complete bar chart JSON configuration:'''\n",
    "\n",
    "        return system_message.strip()\n",
    "\n",
    "    def _validate_and_clean_json(self, json_str: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validates and cleans the JSON string returned by the LLM.\n",
    "        \n",
    "        Args:\n",
    "            json_str (str): JSON string from LLM\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Cleaned and validated JSON dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove any markdown code block syntax if present\n",
    "            json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            chart_config = json.loads(json_str)\n",
    "            \n",
    "            # Basic validation\n",
    "            required_keys = ['chart', 'title', 'xAxis', 'yAxis', 'series', 'plotOptions']\n",
    "            for key in required_keys:\n",
    "                if key not in chart_config:\n",
    "                    raise ValueError(f\"Missing required key: {key}\")\n",
    "            \n",
    "            # Ensure 'type' is 'bar'\n",
    "            if chart_config['chart'].get('type') != 'bar':\n",
    "                chart_config['chart']['type'] = 'bar'\n",
    "                \n",
    "            return chart_config\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON format: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error processing chart configuration: {str(e)}\")\n",
    "\n",
    "    def generate_chart_json(\n",
    "        self,\n",
    "        query: str,\n",
    "        df: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates a bar chart JSON configuration using LLM and data analysis.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete bar chart JSON configuration\n",
    "        \"\"\"\n",
    "        # Generate prompt for LLM\n",
    "        prompt = self._generate_bar_chart_prompt(query, df)\n",
    "        \n",
    "        # Create chain\n",
    "        prompt_template = PromptTemplate.from_template(prompt)\n",
    "        chart_generation_chain = prompt_template | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Get JSON from LLM\n",
    "        json_response = chart_generation_chain.invoke({})\n",
    "        \n",
    "        # Validate and clean the JSON\n",
    "        chart_config = self._validate_and_clean_json(json_response)\n",
    "        \n",
    "        return chart_config\n",
    "\n",
    "\n",
    "# Initialize the generator\n",
    "bar_generator = BarChartGenerator(colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LineChartGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineChartGenerator:\n",
    "    def __init__(self, colors):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.colors = colors\n",
    "\n",
    "    def _generate_line_chart_prompt(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Generates the prompt for the LLM to analyze the data and suggest line chart configuration.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt for the LLM\n",
    "        \"\"\"\n",
    "        system_message = f'''\n",
    "You are an expert in data visualization specializing in creating Highcharts line chart configurations. Analyze the provided DataFrame and user query to generate the optimal line chart JSON configuration.\n",
    "\n",
    "DataFrame Preview:\n",
    "{df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Available Color Codes (use only these):\n",
    "{\", \".join(self.colors)}\n",
    "\n",
    "Follow this exact JSON template for line charts:\n",
    "{{{{\n",
    "  \"chart\": {{{{\n",
    "    \"type\": \"line\",\n",
    "    \"plotBorderWidth\": 1.5\n",
    "    \n",
    "  }}}},\n",
    "  \"accessibility\": {{{{\n",
    "    \"description\": \"<accessibility_description>\"\n",
    "  }}}},\n",
    "  \"title\": {{{{\n",
    "    \"text\": \"<chart_title>\",\n",
    "    \"align\": \"left\"\n",
    "  }}}},\n",
    "  \"subtitle\": {{{{\n",
    "    \"text\": \"<chart_subtitle>\",\n",
    "    \"align\": \"left\"\n",
    "  }}}},\n",
    "  \"xAxis\": {{{{\n",
    "    \"categories\": [\"Category 1\", \"Category 2\", \"Category 3\", \"Category 4\"],\n",
    "    \"title\": {{{{ \"text\": \"<x_axis_title>\" }}}},\n",
    "    \"crosshair\": false,\n",
    "    \"gridLineWidth\": 1,\n",
    "    \"lineWidth\": 1,\n",
    "    \"allowDecimals\": true,\n",
    "    \"accessibility\": {{{{\n",
    "      \"rangeDescription\": \"<range_description>\",\n",
    "      \"description\": \"<x_axis_accessibility_description>\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"yAxis\": {{{{\n",
    "    \"title\": {{{{\n",
    "      \"text\": \"<y_axis_title>\",\n",
    "      \"align\": \"high\"\n",
    "    }}}},\n",
    "    \"min\": 0,\n",
    "    \"max\": null,\n",
    "    \"labels\": {{{{\n",
    "      \"overflow\": \"justify\"\n",
    "    }}}},\n",
    "    \"gridLineWidth\": 1\n",
    "  }}}},\n",
    "  \"series\": [\n",
    "    {{{{\n",
    "      \"name\": \"<series_name>\",\n",
    "      \"data\": [<data_points>],\n",
    "      \"color\": \"<hex_color>\",\n",
    "      \"tooltip\": {{{{\n",
    "        \"pointFormat\": \"<point_format>\"\n",
    "      }}}},\n",
    "      \"dataLabels\": {{{{\n",
    "        \"enabled\": true,\n",
    "        \"format\": \"<format>\"\n",
    "      }}}}\n",
    "    }}}}\n",
    "  ],\n",
    "  \"plotOptions\": {{{{\n",
    "    \"series\": {{{{\n",
    "      \"label\": {{{{ \"connectorAllowed\": false }}}},\n",
    "      \"pointStart\": null,\n",
    "      \"dataLabels\": {{{{\n",
    "        \"enabled\": true,\n",
    "        \"format\": \"{{{{point.y:,.2f}}}}\"\n",
    "      }}}}\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"legend\": {{{{\n",
    "    \"layout\": \"vertical\",\n",
    "    \"align\": \"right\",\n",
    "    \"verticalAlign\": \"middle\",\n",
    "    \"floating\": false,\n",
    "    \"borderWidth\": 1,\n",
    "    \"backgroundColor\": \"<hex_color>\",\n",
    "    \"shadow\": true\n",
    "  }}}},\n",
    "  \"responsive\": {{{{\n",
    "    \"rules\": [\n",
    "      {{{{\n",
    "        \"condition\": {{{{ \"maxWidth\": 500 }}}},\n",
    "        \"chartOptions\": {{{{\n",
    "          \"legend\": {{{{\n",
    "            \"layout\": \"horizontal\",\n",
    "            \"align\": \"center\",\n",
    "            \"verticalAlign\": \"bottom\"\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}}\n",
    "    ]\n",
    "  }}}}\n",
    "}}}}\n",
    "\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the DataFrame structure and user query to determine:\n",
    "   - Appropriate columns for x-axis (categories) and y-axis (values)\n",
    "   - Meaningful title and labels\n",
    "   - Proper series configuration\n",
    "\n",
    "2. Number Formatting Requirements:\n",
    "   - For currency values (Spend/Price/Cost/Charge):\n",
    "     * Use $ prefix (valuePrefix: '$')\n",
    "     * Format example: $36,614,583.26\n",
    "   - For non-currency values (like Encounter, Score):\n",
    "     * No currency symbol\n",
    "     * Format example: 36,614,583.26\n",
    "   - Always:\n",
    "     * Round to 2 decimal places\n",
    "     * Use commas for thousand separators\n",
    "     * Use Highcharts.numberFormat\n",
    "   - For percentages:\n",
    "     * Format to 1 decimal place\n",
    "     * Include % symbol\n",
    "\n",
    "3. Date and Text Requirements:\n",
    "   - Use full month names (e.g., \"January\" not \"Jan\")\n",
    "   - Use proper capitalizations in labels and titles\n",
    "\n",
    "4. The JSON must follow the exact template structure above\n",
    "5. Only use colors from the provided color list\n",
    "6. Return only the JSON, no explanations\n",
    "\n",
    "Generate the complete line chart JSON configuration:'''\n",
    "\n",
    "        return system_message.strip()\n",
    "\n",
    "    def _validate_and_clean_json(self, json_str: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validates and cleans the JSON string returned by the LLM.\n",
    "        \n",
    "        Args:\n",
    "            json_str (str): JSON string from LLM\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Cleaned and validated JSON dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove any markdown code block syntax if present\n",
    "            json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            chart_config = json.loads(json_str)\n",
    "            \n",
    "            # Basic validation\n",
    "            required_keys = ['chart', 'title', 'xAxis', 'yAxis', 'series', 'plotOptions']\n",
    "            for key in required_keys:\n",
    "                if key not in chart_config:\n",
    "                    raise ValueError(f\"Missing required key: {key}\")\n",
    "            \n",
    "            # Ensure 'type' is 'line'\n",
    "            if chart_config['chart'].get('type') != 'line':\n",
    "                chart_config['chart']['type'] = 'line'\n",
    "                \n",
    "            return chart_config\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON format: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error processing chart configuration: {str(e)}\")\n",
    "\n",
    "    def generate_chart_json(\n",
    "        self,\n",
    "        query: str,\n",
    "        df: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates a line chart JSON configuration using LLM and data analysis.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete line chart JSON configuration\n",
    "        \"\"\"\n",
    "        # Generate prompt for LLM\n",
    "        prompt = self._generate_line_chart_prompt(query, df)\n",
    "        \n",
    "        # Create chain\n",
    "        prompt_template = PromptTemplate.from_template(prompt)\n",
    "        chart_generation_chain = prompt_template | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Get JSON from LLM\n",
    "        json_response = chart_generation_chain.invoke({})\n",
    "        \n",
    "        # Validate and clean the JSON\n",
    "        chart_config = self._validate_and_clean_json(json_response)\n",
    "        \n",
    "        return chart_config\n",
    "\n",
    "# Initialize the generator\n",
    "line_generator = LineChartGenerator(colors = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. PieChartGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PieChartGenerator:\n",
    "    def __init__(self, colors):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.colors = colors\n",
    "\n",
    "    def _generate_pie_chart_prompt(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Generates the prompt for the LLM to analyze the data and suggest pie chart configuration.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt for the LLM\n",
    "        \"\"\"\n",
    "        system_message = f'''\n",
    "You are an expert in data visualization specializing in creating Highcharts pie chart configurations. Analyze the provided DataFrame and user query to generate the optimal pie chart JSON configuration.\n",
    "\n",
    "DataFrame Preview:\n",
    "{df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Available Color Codes (use only these):\n",
    "{\", \".join(self.colors)}\n",
    "\n",
    "Follow this exact JSON template for pie charts:\n",
    "{{{{\n",
    "  \"chart\": {{{{\n",
    "    \"type\": \"pie\",\n",
    "    \"plotBorderWidth\": 1\n",
    "  }}}},\n",
    "  \"accessibility\": {{{{\n",
    "    \"description\": \"<accessibility_description>\",\n",
    "    \"announceNewData\": {{{{\n",
    "      \"enabled\": true\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"title\": {{{{\n",
    "    \"text\": \"<chart_title>\",\n",
    "    \"align\": \"left\",\n",
    "    \"style\": {{{{\n",
    "      \"fontSize\": \"<font_size>\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"subtitle\": {{{{\n",
    "    \"text\": \"<chart_subtitle>\",\n",
    "    \"align\": \"left\"\n",
    "  }}}},\n",
    "  \"plotOptions\": {{{{\n",
    "    \"series\": {{{{\n",
    "      \"allowPointSelect\": true,\n",
    "      \"cursor\": \"pointer\",\n",
    "      \"tooltip\": {{{{\n",
    "        \"pointFormat\": \"<point_format>\"\n",
    "      }}}},\n",
    "      \"dataLabels\": [\n",
    "        {{{{\n",
    "          \"enabled\": true,\n",
    "          \"distance\": <+distance>,\n",
    "          \"format\": \"<format>\"\n",
    "        }}}}\n",
    "      ]\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"series\": [\n",
    "    {{{{\n",
    "      \"name\": \"<series_name>\",\n",
    "      \"colorByPoint\": true,\n",
    "      \"data\": [<data_points>]\n",
    "    }}}}\n",
    "  ],\n",
    "  \"legend\": {{{{\n",
    "    \"enabled\": true,\n",
    "    \"layout\": \"vertical\",\n",
    "    \"align\": \"right\",\n",
    "    \"verticalAlign\": \"middle\",\n",
    "    \"itemMarginTop\": 10,\n",
    "    \"itemMarginBottom\": 10,\n",
    "    \"labelFormat\": \"{{{{name}}}} ({{{{percentage:.1f}}}}%)\"\n",
    "  }}}},\n",
    "  \"responsive\": {{{{\n",
    "    \"rules\": [\n",
    "      {{{{\n",
    "        \"condition\": {{{{ \"maxWidth\": 500 }}}},\n",
    "        \"chartOptions\": {{{{\n",
    "          \"legend\": {{{{\n",
    "            \"layout\": \"horizontal\",\n",
    "            \"align\": \"center\",\n",
    "            \"verticalAlign\": \"bottom\"\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}}\n",
    "    ]\n",
    "  }}}}\n",
    "}}}}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the DataFrame structure and user query to determine:\n",
    "   - Appropriate categories and values for pie segments\n",
    "   - Meaningful title and labels\n",
    "   - Proper series configuration\n",
    "\n",
    "2. Number Formatting Requirements:\n",
    "   - For currency values (Spend/Price/Cost/Charge):\n",
    "     * Use $ prefix (valuePrefix: '$')\n",
    "     * Format example: $36,614,583.26\n",
    "   - For non-currency values (like Encounter, Score):\n",
    "     * No currency symbol\n",
    "     * Format example: 36,614,583.26\n",
    "   - Always:\n",
    "     * Round to 2 decimal places\n",
    "     * Use commas for thousand separators\n",
    "     * Use Highcharts.numberFormat\n",
    "   - For percentages:\n",
    "     * Format to 1 decimal place\n",
    "     * Include % symbol\n",
    "\n",
    "3. Date and Text Requirements:\n",
    "   - Use full month names (e.g., \"January\" not \"Jan\")\n",
    "   - Use proper capitalizations in labels and titles\n",
    "   - Ensure segment labels are clear and readable\n",
    "\n",
    "4. Pie Chart Specific Requirements:\n",
    "   - Set largest segment as sliced out by default\n",
    "   - Enable point selection\n",
    "   - Show percentage in data labels for segments > 10%\n",
    "   - Include percentages in legend labels\n",
    "\n",
    "5. The JSON must follow the exact template structure above\n",
    "6. Only use colors from the provided color list\n",
    "7. Return only the JSON, no explanations\n",
    "\n",
    "Generate the complete pie chart JSON configuration:'''\n",
    "\n",
    "        return system_message.strip()\n",
    "\n",
    "    def _validate_and_clean_json(self, json_str: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validates and cleans the JSON string returned by the LLM.\n",
    "        \n",
    "        Args:\n",
    "            json_str (str): JSON string from LLM\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Cleaned and validated JSON dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove any markdown code block syntax if present\n",
    "            json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            chart_config = json.loads(json_str)\n",
    "            \n",
    "            # Basic validation\n",
    "            required_keys = ['chart', 'title', 'series', 'plotOptions']\n",
    "            for key in required_keys:\n",
    "                if key not in chart_config:\n",
    "                    raise ValueError(f\"Missing required key: {key}\")\n",
    "            \n",
    "            # Ensure 'type' is 'pie'\n",
    "            if chart_config['chart'].get('type') != 'pie':\n",
    "                chart_config['chart']['type'] = 'pie'\n",
    "            \n",
    "            # Ensure pie-specific settings\n",
    "            if 'series' in chart_config and chart_config['series']:\n",
    "                for series in chart_config['series']:\n",
    "                    series['colorByPoint'] = True\n",
    "                \n",
    "            if 'plotOptions' in chart_config:\n",
    "                if 'series' not in chart_config['plotOptions']:\n",
    "                    chart_config['plotOptions']['series'] = {}\n",
    "                chart_config['plotOptions']['series']['allowPointSelect'] = True\n",
    "                chart_config['plotOptions']['series']['cursor'] = 'pointer'\n",
    "                \n",
    "            return chart_config\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON format: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error processing chart configuration: {str(e)}\")\n",
    "\n",
    "    def generate_chart_json(\n",
    "        self,\n",
    "        query: str,\n",
    "        df: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates a pie chart JSON configuration using LLM and data analysis.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete pie chart JSON configuration\n",
    "        \"\"\"\n",
    "        # Generate prompt for LLM\n",
    "        prompt = self._generate_pie_chart_prompt(query, df)\n",
    "        \n",
    "        # Create chain\n",
    "        prompt_template = PromptTemplate.from_template(prompt)\n",
    "        chart_generation_chain = prompt_template | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Get JSON from LLM\n",
    "        json_response = chart_generation_chain.invoke({})\n",
    "        \n",
    "        # Validate and clean the JSON\n",
    "        chart_config = self._validate_and_clean_json(json_response)\n",
    "        \n",
    "        return chart_config\n",
    "\n",
    "# Initialization - Pie Chart\n",
    "pie_generator = PieChartGenerator(colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. BubbleChartGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BubbleChartGenerator:\n",
    "    def __init__(self, colors):\n",
    "        \"\"\"\n",
    "        Initializes the BubbleChartGenerator with a list of approved colors.\n",
    "        \n",
    "        Args:\n",
    "            colors (list): List of approved color codes for the chart\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.colors = colors\n",
    "\n",
    "    def _generate_bubble_chart_prompt(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Generates the prompt for the LLM to analyze the data and suggest bubble chart configuration.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt for the LLM\n",
    "        \"\"\"\n",
    "        system_message = f'''\n",
    "You are an expert in data visualization specializing in creating Highcharts bubble chart configurations. Analyze the provided DataFrame and user query to generate the optimal bubble chart JSON configuration.\n",
    "\n",
    "DataFrame Preview:\n",
    "{df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Available Color Codes (use only these):\n",
    "{\", \".join(self.colors)}\n",
    "\n",
    "Follow this exact JSON template for bubble charts:\n",
    "{{{{\n",
    "  \"chart\": {{{{\n",
    "    \"type\": \"bubble\",\n",
    "    \"plotBorderWidth\": 1,\n",
    "    \"zooming\": {{{{\n",
    "      \"type\": \"xy\"\n",
    "    }}}},\n",
    "    \"backgroundColor\": null\n",
    "  }}}},\n",
    "  \"title\": {{{{\n",
    "    \"text\": \"<chart_title>\",\n",
    "    \"align\": \"left\"\n",
    "  }}}},\n",
    "  \"subtitle\": {{{{\n",
    "    \"text\": \"<chart_subtitle>\",\n",
    "    \"align\": \"left\"\n",
    "  }}}},\n",
    "  \"accessibility\": {{{{\n",
    "    \"point\": {{{{\n",
    "      \"valueDescriptionFormat\": \"{{{{index}}}}. {{{{point.name}}}}, x: {{{{point.x}}}}g, y: {{{{point.y}}}}g, z: {{{{point.z}}}}%.\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"xAxis\": {{{{\n",
    "    \"gridLineWidth\": 1,\n",
    "    \"title\": {{{{\n",
    "      \"text\": \"<x_axis_title>\"\n",
    "    }}}},\n",
    "    \"labels\": {{{{\n",
    "      \"format\": \"{{{{value}}}} <unit>\"\n",
    "    }}}},\n",
    "    \"plotLines\": [\n",
    "      {{{{\n",
    "        \"color\": \"black\",\n",
    "        \"dashStyle\": \"dot\",\n",
    "        \"width\": 2,\n",
    "        \"value\": \"<reference_line_value>\",\n",
    "        \"label\": {{{{\n",
    "          \"rotation\": 0,\n",
    "          \"y\": 15,\n",
    "          \"style\": {{{{\n",
    "            \"fontStyle\": \"italic\"\n",
    "          }}}},\n",
    "          \"text\": \"<reference_line_label>\"\n",
    "        }}}},\n",
    "        \"zIndex\": 3\n",
    "      }}}}\n",
    "    ],\n",
    "    \"accessibility\": {{{{\n",
    "      \"rangeDescription\": \"Range: <x_range_description>\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"yAxis\": {{{{\n",
    "    \"startOnTick\": false,\n",
    "    \"endOnTick\": false,\n",
    "    \"title\": {{{{\n",
    "      \"text\": \"<y_axis_title>\"\n",
    "    }}}},\n",
    "    \"labels\": {{{{\n",
    "      \"format\": \"{{{{value}}}} <unit>\"\n",
    "    }}}},\n",
    "    \"maxPadding\": 0.2,\n",
    "    \"plotLines\": [\n",
    "      {{{{\n",
    "        \"color\": \"black\",\n",
    "        \"dashStyle\": \"dot\",\n",
    "        \"width\": 2,\n",
    "        \"value\": \"<reference_line_value>\",\n",
    "        \"label\": {{{{\n",
    "          \"align\": \"right\",\n",
    "          \"style\": {{{{\n",
    "            \"fontStyle\": \"italic\"\n",
    "          }}}},\n",
    "          \"text\": \"<reference_line_label>\",\n",
    "          \"x\": -10\n",
    "        }}}},\n",
    "        \"zIndex\": 3\n",
    "      }}}}\n",
    "    ],\n",
    "    \"accessibility\": {{{{\n",
    "      \"rangeDescription\": \"Range: <y_range_description>\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"tooltip\": {{{{\n",
    "    \"useHTML\": true,\n",
    "    \"headerFormat\": \"<table>\",\n",
    "    \"pointFormat\": \"<\"pointFormat\": \"<tr><th>Category:</th><td>{{{{point.name}}}}</td></tr>\\\\n<tr><th>Value X:</th><td>{{{{point.x}}}}</td></tr>\\\\n<tr><th>Value Y:</th><td>{{{{point.y}}}}</td></tr>\\\\n<tr><th>Metric Z:</th><td>{{{{point.z}}}}</td></tr>\">\",\n",
    "    \"footerFormat\": \"</table>\",\n",
    "    \"followPointer\": true\n",
    "  }}}},\n",
    "  \"plotOptions\": {{{{\n",
    "    \"series\": {{{{\n",
    "      \"dataLabels\": {{{{\n",
    "        \"enabled\": true,\n",
    "        \"format\": \"{{{{point.name}}}}\"\n",
    "      }}}},\n",
    "      \"marker\": {{{{\n",
    "        \"fillOpacity\": 0.7\n",
    "      }}}}\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"series\": [\n",
    "    {{{{\n",
    "      \"data\": [\n",
    "        {{{{\n",
    "          \"x\": \"<x_value>\",\n",
    "          \"y\": \"<y_value>\",\n",
    "          \"z\": \"<z_value>\",\n",
    "          \"name\": \"<point_name>\",\n",
    "          \"color\": \"<color_code>\"\n",
    "        }}}}\n",
    "      ],\n",
    "      \"name\": \"<series_name>\",\n",
    "      \"marker\": {{{{\n",
    "        \"symbol\": \"circle\",\n",
    "        \"lineColor\": null,\n",
    "        \"lineWidth\": 1\n",
    "      }}}}\n",
    "    }}}}\n",
    "  ],\n",
    "  \"legend\": {{{{\n",
    "    \"enabled\": true,\n",
    "    \"layout\": \"vertical\",\n",
    "    \"align\": \"right\",\n",
    "    \"verticalAlign\": \"middle\",\n",
    "    \"itemMarginTop\": 10,\n",
    "    \"itemMarginBottom\": 10,\n",
    "    \"bubbleLegend\": {{{{\n",
    "      \"enabled\": true,\n",
    "      \"borderColor\": \"black\",\n",
    "      \"borderWidth\": 2,\n",
    "      \"connectorWidth\": 2,\n",
    "      \"labels\": {{{{\n",
    "        \"align\": \"left\"\n",
    "      }}}}\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"responsive\": {{{{\n",
    "    \"rules\": [\n",
    "      {{{{\n",
    "        \"condition\": {{{{\n",
    "          \"maxWidth\": 500\n",
    "        }}}},\n",
    "        \"chartOptions\": {{{{\n",
    "          \"legend\": {{{{\n",
    "            \"layout\": \"horizontal\",\n",
    "            \"align\": \"center\",\n",
    "            \"verticalAlign\": \"bottom\"\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}}\n",
    "    ]\n",
    "  }}}}\n",
    "}}}}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the DataFrame structure and user query to determine:\n",
    "   - Appropriate x, y, and z values for bubble chart\n",
    "   - Meaningful axis titles and labels\n",
    "   - Proper series configuration and bubble sizes\n",
    "\n",
    "2. Number Formatting Requirements:\n",
    "   - For currency values (Spend/Price/Cost/Charge):\n",
    "     * Use $ prefix (valuePrefix: '$')\n",
    "     * Format example: $36,614,583.26\n",
    "   - For non-currency values (like Encounter, Score):\n",
    "     * No currency symbol\n",
    "     * Format example: 36,614,583.26\n",
    "   - Always:\n",
    "     * Round to 2 decimal places\n",
    "     * Use commas for thousand separators\n",
    "     * Use Highcharts.numberFormat\n",
    "   - For percentages:\n",
    "     * Format to 1 decimal place\n",
    "     * Include % symbol\n",
    "\n",
    "3. Bubble Chart Specific Requirements:\n",
    "   - Ensure bubble sizes (z values) are proportional\n",
    "   - Add meaningful tooltips with all three dimensions\n",
    "   - Enable bubble legend if appropriate\n",
    "   - Use semi-transparent bubbles to handle overlapping\n",
    "   - Include appropriate reference lines if data suggests thresholds\n",
    "\n",
    "4. The JSON must follow the exact template structure above\n",
    "5. Only use colors from the provided color list\n",
    "6. Return only the JSON, no explanations\n",
    "\n",
    "Generate the complete bubble chart JSON configuration:'''\n",
    "        return system_message.strip()\n",
    "\n",
    "    def _validate_and_clean_json(self, json_str: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validates and cleans the JSON string returned by the LLM.\n",
    "        Ensures all required fields are present and data points are properly formatted.\n",
    "        \n",
    "        Args:\n",
    "            json_str (str): JSON string from LLM\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Cleaned and validated JSON dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Clean and parse JSON\n",
    "            json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "            chart_config = json.loads(json_str)\n",
    "\n",
    "            # Ensure 'chart' block and chart type\n",
    "            if 'chart' not in chart_config:\n",
    "                chart_config['chart'] = {}\n",
    "            if 'type' not in chart_config['chart']:\n",
    "                chart_config['chart']['type'] = 'bubble'\n",
    "            if 'plotBorderWidth' not in chart_config['chart']:\n",
    "                chart_config['chart']['plotBorderWidth'] = 1\n",
    "\n",
    "            # Ensure xAxis and yAxis exist (basic placeholder if missing)\n",
    "            if 'xAxis' not in chart_config:\n",
    "                chart_config['xAxis'] = {\n",
    "                    \"gridLineWidth\": 1,\n",
    "                    \"title\": {\"text\": \"\"},\n",
    "                    \"labels\": {\"format\": \"{value}\"},\n",
    "                    \"plotLines\": [],\n",
    "                    \"accessibility\": {\"rangeDescription\": \"\"}\n",
    "                }\n",
    "            if 'yAxis' not in chart_config:\n",
    "                chart_config['yAxis'] = {\n",
    "                    \"startOnTick\": False,\n",
    "                    \"endOnTick\": False,\n",
    "                    \"title\": {\"text\": \"\"},\n",
    "                    \"labels\": {\"format\": \"{value}\"},\n",
    "                    \"maxPadding\": 0.2,\n",
    "                    \"plotLines\": [],\n",
    "                    \"accessibility\": {\"rangeDescription\": \"\"}\n",
    "                }\n",
    "\n",
    "            # Validate 'series' data\n",
    "            if 'series' not in chart_config:\n",
    "                chart_config['series'] = []\n",
    "            for series in chart_config['series']:\n",
    "                if 'data' not in series:\n",
    "                    series['data'] = []\n",
    "\n",
    "                for point in series['data']:\n",
    "                    # Convert x, y, z values to float if possible\n",
    "                    for coord in ['x', 'y', 'z']:\n",
    "                        if coord in point and point[coord] is not None:\n",
    "                            try:\n",
    "                                point[coord] = float(point[coord])\n",
    "                            except (ValueError, TypeError):\n",
    "                                point[coord] = 0.0\n",
    "\n",
    "                    # Validate color usage; fallback to first available color if invalid or missing\n",
    "                    if 'color' not in point or point['color'] not in self.colors:\n",
    "                        point['color'] = self.colors[0] if self.colors else \"#000000\"\n",
    "\n",
    "            return chart_config\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error processing chart configuration: {str(e)}\")\n",
    "\n",
    "    def generate_chart_json(\n",
    "        self,\n",
    "        query: str,\n",
    "        df: pd.DataFrame\n",
    "    ) -> Dict[str, Any] | str:\n",
    "        \"\"\"\n",
    "        Generates a bubble chart JSON configuration using LLM and data analysis.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete bubble chart JSON configuration\n",
    "        \"\"\"\n",
    "        # Generate prompt for LLM\n",
    "        prompt = self._generate_bubble_chart_prompt(query, df)\n",
    "\n",
    "        # Create chain\n",
    "        prompt_template = PromptTemplate.from_template(prompt)\n",
    "        chart_generation_chain = prompt_template | self.llm | StrOutputParser()\n",
    "\n",
    "        # Get JSON from LLM\n",
    "        json_response = chart_generation_chain.invoke({})\n",
    "\n",
    "        # Validate and clean the JSON\n",
    "        chart_config = self._validate_and_clean_json(json_response)\n",
    "        return chart_config\n",
    "\n",
    "# Example usage:\n",
    "bubble_generator = BubbleChartGenerator(colors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_config = bubble_generator.generate_chart_json(user_prompt, df)\n",
    "# # print(result_config)\n",
    "# json.loads(result_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. ScatterChartGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScatterChartGenerator:\n",
    "    def __init__(self, colors):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.colors = colors\n",
    "\n",
    "    def _generate_scatter_chart_prompt(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Generates the prompt for the LLM to analyze the data and suggest scatter chart configuration.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt for the LLM\n",
    "        \"\"\"\n",
    "        system_message = f'''\n",
    "You are an expert in data visualization specializing in creating Highcharts scatter plot configurations. Analyze the provided DataFrame and user query to generate the optimal scatter chart JSON configuration.\n",
    "\n",
    "DataFrame Preview:\n",
    "{df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Available Color Codes (use only these):\n",
    "{\", \".join(self.colors)}\n",
    "\n",
    "Follow this exact JSON template for scatter charts:\n",
    "{{{{\n",
    "  \"chart\": {{{{\n",
    "    \"type\": \"scatter\",\n",
    "    \"zooming\": {{{{\n",
    "      \"type\": \"xy\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"colors\": [\"<colors>\"],\n",
    "  \"title\": {{{{\n",
    "    \"text\": \"<chart_title>\",\n",
    "    \"align\": \"left\"\n",
    "  }}}},\n",
    "  \"subtitle\": {{{{\n",
    "    \"text\": \"<chart_subtitle>\",\n",
    "    \"align\": \"left\"\n",
    "  }}}},\n",
    "  \"xAxis\": {{{{\n",
    "    \"title\": {{{{\n",
    "      \"text\": \"<x_axis_title>\"\n",
    "    }}}},\n",
    "    \"labels\": {{{{\n",
    "      \"format\": \"<proper_format>\"\n",
    "    }}}},\n",
    "    \"startOnTick\": true,\n",
    "    \"endOnTick\": true,\n",
    "    \"showLastLabel\": true,\n",
    "    \"gridLineWidth\": 1\n",
    "  }}}},\n",
    "  \"yAxis\": {{{{\n",
    "    \"title\": {{{{\n",
    "      \"text\": \"<y_axis_title>\"\n",
    "    }}}},\n",
    "    \"labels\": {{{{\n",
    "      \"format\": \"<proper_format>\"\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"legend\": {{{{\n",
    "    \"enabled\": true,\n",
    "    \"layout\": \"vertical\",\n",
    "    \"align\": \"right\",\n",
    "    \"verticalAlign\": \"middle\",\n",
    "    \"borderWidth\": 1\n",
    "  }}}},\n",
    "  \"plotOptions\": {{{{\n",
    "    \"scatter\": {{{{\n",
    "      \"marker\": {{{{\n",
    "        \"radius\": 5,\n",
    "        \"symbol\": \"circle\",\n",
    "        \"states\": {{{{\n",
    "          \"hover\": {{{{\n",
    "            \"enabled\": true,\n",
    "            \"lineColor\": \"<line_color>\"\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}},\n",
    "      \"states\": {{{{\n",
    "        \"hover\": {{{{\n",
    "          \"marker\": {{{{\n",
    "            \"enabled\": false\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}},\n",
    "      \"jitter\": {{{{\n",
    "        \"x\": 0.005\n",
    "      }}}}\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"tooltip\": {{{{\n",
    "    \"pointFormat\": \"<point_format>\"\n",
    "  }}}},\n",
    "  \"series\": [\n",
    "    {{{{\n",
    "      \"name\": \"<series_name>\",\n",
    "      \"data\": [\n",
    "        {{{{\n",
    "          \"x\": \"<x_value>\",\n",
    "          \"y\": \"<y_value>\",\n",
    "          \"name\": \"<point_name>\",\n",
    "          \"color\": \"<color_code>\"\n",
    "        }}}}\n",
    "      ],\n",
    "      \"marker\": {{{{\n",
    "        \"symbol\": \"circle\"\n",
    "      }}}}\n",
    "    }}}}\n",
    "  ],\n",
    "  \"responsive\": {{{{\n",
    "    \"rules\": [\n",
    "      {{{{\n",
    "        \"condition\": {{{{\n",
    "          \"maxWidth\": 500\n",
    "        }}}},\n",
    "        \"chartOptions\": {{{{\n",
    "          \"legend\": {{{{\n",
    "            \"align\": \"center\",\n",
    "            \"layout\": \"horizontal\",\n",
    "            \"verticalAlign\": \"bottom\"\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}}\n",
    "    ]\n",
    "  }}}}\n",
    "}}}}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the DataFrame structure and user query to determine:\n",
    "   - Appropriate x and y variables for scatter plot\n",
    "   - Meaningful title and axis labels\n",
    "   - Proper series configuration\n",
    "\n",
    "2. Number Formatting Requirements:\n",
    "   - For currency values (Spend/Price/Cost/Charge):\n",
    "     * Use $ prefix (valuePrefix: '$')\n",
    "     * Format example: $36,614,583.26\n",
    "   - For non-currency values (like Encounter, Score):\n",
    "     * No currency symbol\n",
    "     * Format example: 36,614,583.26\n",
    "   - Always:\n",
    "     * Round to 2 decimal places\n",
    "     * Use commas for thousand separators\n",
    "     * Use Highcharts.numberFormat\n",
    "   - For percentages:\n",
    "     * Format to 1 decimal place\n",
    "     * Include % symbol\n",
    "\n",
    "3. Scatter Plot Specific Requirements:\n",
    "   - Choose appropriate axis scales\n",
    "   - Set meaningful marker sizes\n",
    "   - Enable zoom functionality\n",
    "   - Configure proper tooltips\n",
    "   - Handle point overlaps using jitter if needed\n",
    "\n",
    "4. The JSON must follow the exact template structure above\n",
    "5. Only use colors from the provided color list\n",
    "6. Return only the JSON, no explanations\n",
    "\n",
    "Generate the complete scatter chart JSON configuration:'''\n",
    "\n",
    "        return system_message.strip()\n",
    "\n",
    "    def _validate_and_clean_json(self, json_str: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validates and cleans the JSON string returned by the LLM.\n",
    "        \n",
    "        Args:\n",
    "            json_str (str): JSON string from LLM\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Cleaned and validated JSON dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove any markdown code block syntax if present\n",
    "            json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            chart_config = json.loads(json_str)\n",
    "            \n",
    "            # Basic validation\n",
    "            required_keys = ['chart', 'title', 'xAxis', 'yAxis', 'series']\n",
    "            for key in required_keys:\n",
    "                if key not in chart_config:\n",
    "                    raise ValueError(f\"Missing required key: {key}\")\n",
    "            \n",
    "            # Ensure 'type' is 'scatter'\n",
    "            if chart_config['chart'].get('type') != 'scatter':\n",
    "                chart_config['chart']['type'] = 'scatter'\n",
    "            \n",
    "            # Ensure scatter-specific settings\n",
    "            if 'plotOptions' in chart_config:\n",
    "                if 'scatter' not in chart_config['plotOptions']:\n",
    "                    chart_config['plotOptions']['scatter'] = {}\n",
    "                    \n",
    "                # Set default marker options if not present\n",
    "                if 'marker' not in chart_config['plotOptions']['scatter']:\n",
    "                    chart_config['plotOptions']['scatter']['marker'] = {\n",
    "                        'radius': 5,\n",
    "                        'symbol': 'circle'\n",
    "                    }\n",
    "            \n",
    "            # Ensure zooming is enabled\n",
    "            if 'zooming' not in chart_config['chart']:\n",
    "                chart_config['chart']['zooming'] = {'type': 'xy'}\n",
    "            \n",
    "\n",
    "            # Validate series colors\n",
    "            if 'series' in chart_config:\n",
    "                for series in chart_config['series']:\n",
    "                    if 'data' in series:\n",
    "                        for point in series['data']:\n",
    "                            if 'color' in point and point['color'] not in [f\"#{color}\" for color in self.colors]:\n",
    "                                point['color'] = f\"#{self.colors[0]}\"  # Default to first color if invalid\n",
    "                \n",
    "            return chart_config\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON format: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error processing chart configuration: {str(e)}\")\n",
    "\n",
    "    def generate_chart_json(\n",
    "        self,\n",
    "        query: str,\n",
    "        df: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates a scatter chart JSON configuration using LLM and data analysis.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete scatter chart JSON configuration\n",
    "        \"\"\"\n",
    "        # Generate prompt for LLM\n",
    "        prompt = self._generate_scatter_chart_prompt(query, df)\n",
    "        \n",
    "        # Create chain\n",
    "        prompt_template = PromptTemplate.from_template(prompt)\n",
    "        chart_generation_chain = prompt_template | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Get JSON from LLM\n",
    "        json_response = chart_generation_chain.invoke({})\n",
    "        \n",
    "        # Validate and clean the JSON\n",
    "        chart_config = self._validate_and_clean_json(json_response)\n",
    "        \n",
    "        return chart_config\n",
    "\n",
    "# Initialization - Scatter Chart\n",
    "scatter_generator = ScatterChartGenerator(colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. AreaChartGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AreaChartGenerator:\n",
    "    def __init__(self, colors):\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-2024-08-06', temperature=0)\n",
    "        self.colors = colors\n",
    "\n",
    "    def _generate_area_chart_prompt(self, query: str, df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Generates the prompt for the LLM to analyze the data and suggest area chart configuration.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt for the LLM\n",
    "        \"\"\"\n",
    "        system_message = f'''You are an expert in data visualization specializing in creating Highcharts area chart configurations. Analyze the provided DataFrame and user query to generate the optimal area chart JSON configuration.\n",
    "\n",
    "DataFrame Preview:\n",
    "{df.head(50).to_json().replace('{', '{{').replace('}', '}}')}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Available Color Codes (use only these):\n",
    "{\", \".join(self.colors)}\n",
    "\n",
    "Follow this exact JSON template for area charts:\n",
    "{{{{\n",
    "  \"chart\": {{{{\n",
    "    \"type\": \"area\"\n",
    "  }}}},\n",
    "  \"accessibility\": {{{{\n",
    "    \"description\": \"<accessibility_description>\"\n",
    "  }}}},\n",
    "  \"title\": {{{{\n",
    "    \"text\": \"<chart_title>\",\n",
    "    \"align\": \"<align>\"\n",
    "  }}}},\n",
    "  \"subtitle\": {{{{\n",
    "    \"text\": \"<chart_subtitle>\",\n",
    "    \"align\": \"<align>\"\n",
    "  }}}},\n",
    "  \"xAxis\": {{{{\n",
    "    \"categories\": null,\n",
    "    \"title\": {{{{ \"text\": \"<x_axis_title>\" }}}},\n",
    "    \"crosshair\": false,\n",
    "    \"gridLineWidth\": null,\n",
    "    \"lineWidth\": null,\n",
    "    \"allowDecimals\": false,\n",
    "    \"accessibility\": {{{{\n",
    "      \"rangeDescription\": \"<range_description>\",\n",
    "      \"description\": <description>\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"yAxis\": {{{{\n",
    "    \"title\": {{{{\n",
    "      \"text\": \"<y_axis_title>\",\n",
    "      \"align\": null\n",
    "    }}}},\n",
    "    \"min\": null,\n",
    "    \"max\": null,\n",
    "    \"labels\": {{{{\n",
    "      \"overflow\": null\n",
    "    }}}},\n",
    "    \"gridLineWidth\": null\n",
    "  }}}},\n",
    "  \"series\": [\n",
    "    {{{{\n",
    "      \"name\": \"<series_name>\",\n",
    "      \"data\": [<data_points>],\n",
    "      \"color\": \"<hex_color>\"\n",
    "    }}}}\n",
    "  ],\n",
    "  \"tooltip\": {{{{\n",
    "    \"pointFormat\": \"<point_format>\",\n",
    "    \"valuePrefix\": <value_prefix>\n",
    "  }}}},\n",
    "  \"plotOptions\": {{{{\n",
    "    \"area\": {{{{\n",
    "      \"pointStart\": \"<start_year>\",\n",
    "      \"marker\": {{{{\n",
    "        \"enabled\": false,\n",
    "        \"symbol\": \"circle\",\n",
    "        \"radius\": 2,\n",
    "        \"states\": {{{{\n",
    "          \"hover\": {{{{\n",
    "            \"enabled\": true\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}}\n",
    "    }}}}\n",
    "  }}}},\n",
    "  \"legend\": {{{{\n",
    "    \"layout\": \"vertical\",\n",
    "    \"align\": \"right\",\n",
    "    \"verticalAlign\": \"middle\",\n",
    "    \"x\": null,\n",
    "    \"y\": null,\n",
    "    \"floating\": null,\n",
    "    \"borderWidth\": null,\n",
    "    \"backgroundColor\": null,\n",
    "    \"shadow\": null\n",
    "  }}}},\n",
    "  \"responsive\": {{{{\n",
    "    \"rules\": [\n",
    "      {{{{\n",
    "        \"condition\": {{{{ \"maxWidth\": 500 }}}},\n",
    "        \"chartOptions\": {{{{\n",
    "          \"legend\": {{{{\n",
    "            \"layout\": \"horizontal\",\n",
    "            \"align\": \"center\",\n",
    "            \"verticalAlign\": \"bottom\"\n",
    "          }}}}\n",
    "        }}}}\n",
    "      }}}}\n",
    "    ]\n",
    "  }}}}\n",
    "}}}}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the DataFrame structure and user query to determine:\n",
    "   - Appropriate columns for x-axis (time series) and y-axis (values)\n",
    "   - Meaningful title and labels\n",
    "   - Proper series configuration\n",
    "\n",
    "2. **Number Formatting Requirements:\n",
    "   - For currency values (Spend/Price/Cost/Charge):\n",
    "     * Use $ prefix (valuePrefix: '$')\n",
    "     * Format example: $36,614,583.26\n",
    "   - For non-currency values (like Encounter, Score):\n",
    "     * No currency symbol\n",
    "     * Format example: 36,614,583.26\n",
    "   - Always:\n",
    "     * Round to 2 decimal places\n",
    "     * Use commas for thousand separators\n",
    "     * Use Highcharts.numberFormat\n",
    "   - For percentages:\n",
    "     * Format to 1 decimal place\n",
    "     * Include % symbol\n",
    "\n",
    "3. Area Chart Specific Requirements:\n",
    "   - Best suited for time-series data showing cumulative totals\n",
    "   - Use pointStart for setting the starting time period\n",
    "   - Configure markers appropriately for data point visibility\n",
    "   - Consider using fillOpacity for overlapping areas\n",
    "\n",
    "4. Date and Text Requirements:\n",
    "   - Use full month names (e.g., \"January\" not \"Jan\")\n",
    "   - Use proper capitalizations in labels and titles\n",
    "\n",
    "5. The JSON must follow the exact template structure above\n",
    "6. Only use colors from the provided color list\n",
    "7. Return only the JSON, no explanations\n",
    "\n",
    "Generate the complete area chart JSON configuration:'''\n",
    "\n",
    "        return system_message.strip()\n",
    "\n",
    "    def _validate_and_clean_json(self, json_str: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validates and cleans the JSON string returned by the LLM.\n",
    "        \n",
    "        Args:\n",
    "            json_str (str): JSON string from LLM\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Cleaned and validated JSON dictionary\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Remove any markdown code block syntax if present\n",
    "            json_str = json_str.replace('```json', '').replace('```', '').strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            chart_config = json.loads(json_str)\n",
    "            \n",
    "            # Basic validation\n",
    "            required_keys = ['chart', 'title', 'xAxis', 'yAxis', 'series', 'plotOptions']\n",
    "            for key in required_keys:\n",
    "                if key not in chart_config:\n",
    "                    raise ValueError(f\"Missing required key: {key}\")\n",
    "            \n",
    "            # Ensure 'type' is 'area'\n",
    "            if chart_config['chart'].get('type') != 'area':\n",
    "                chart_config['chart']['type'] = 'area'\n",
    "                \n",
    "            # Validate area-specific configurations\n",
    "            plot_options = chart_config.get('plotOptions', {}).get('area', {})\n",
    "            if not isinstance(plot_options, dict):\n",
    "                chart_config['plotOptions'] = {'area': {\n",
    "                    'marker': {\n",
    "                        'enabled': False,\n",
    "                        'symbol': 'circle',\n",
    "                        'radius': 2,\n",
    "                        'states': {'hover': {'enabled': True}}\n",
    "                    }\n",
    "                }}\n",
    "            \n",
    "            return chart_config\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON format: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error processing chart configuration: {str(e)}\")\n",
    "\n",
    "    def generate_chart_json(\n",
    "        self,\n",
    "        query: str,\n",
    "        df: pd.DataFrame\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generates an area chart JSON configuration using LLM and data analysis.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query\n",
    "            df (pd.DataFrame): Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete area chart JSON configuration\n",
    "        \"\"\"\n",
    "        # Generate prompt for LLM\n",
    "        prompt = self._generate_area_chart_prompt(query, df)\n",
    "        \n",
    "        # Create chain\n",
    "        prompt_template = PromptTemplate.from_template(prompt)\n",
    "        chart_generation_chain = prompt_template | self.llm | StrOutputParser()\n",
    "        \n",
    "        # Get JSON from LLM\n",
    "        json_response = chart_generation_chain.invoke({})\n",
    "        \n",
    "        # Validate and clean the JSON\n",
    "        chart_config = self._validate_and_clean_json(json_response)\n",
    "        \n",
    "        return chart_config\n",
    "\n",
    "# Example initialization:\n",
    "area_chart_generator = AreaChartGenerator(colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json_Output Function (For my use only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save_chart_configs\n",
    "\n",
    "Generates and saves JSON files for a list of chart configurations.\n",
    "\n",
    "Args:\n",
    "    chart_configs (list): List of chart configurations.\n",
    "    output_directory (str): Directory to save the JSON files.\n",
    "\n",
    "Returns:\n",
    "    None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "#                 save_chart_configs\n",
    "#################################################\n",
    "\n",
    "def save_chart_configs(chart_configs, output_directory):\n",
    "    \"\"\"\n",
    "    Generates and saves JSON files for a list of chart configurations.\n",
    "\n",
    "    Args:\n",
    "        chart_configs (list): List of chart configurations.\n",
    "        output_directory (str): Directory to save the JSON files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    def create_highchart_file(json_input, output_filename='highchart_config.txt'):\n",
    "        \"\"\"\n",
    "        Takes a JSON input (as a dict or a valid JSON string) and writes a file\n",
    "        with the following format:\n",
    "            Highcharts.chart('container', <properly_indented_json>);\n",
    "        \n",
    "        :param json_input: A Python dictionary or a JSON string compatible with Highcharts.\n",
    "        :param output_filename: File name for the output text file (default: 'highchart_config.txt').\n",
    "        \"\"\"\n",
    "        # 1. If the input is a string, convert it to a Python dictionary\n",
    "        if isinstance(json_input, str):\n",
    "            try:\n",
    "                json_input = json.loads(json_input)\n",
    "            except json.JSONDecodeError as e:\n",
    "                raise ValueError(f\"Invalid JSON string provided: {e}\")\n",
    "        \n",
    "        # 2. Convert the dictionary to a pretty-printed JSON string\n",
    "        json_str = json.dumps(json_input, indent=2)\n",
    "        \n",
    "        # 3. Construct the final content\n",
    "        content = f\"Highcharts.chart('container', {json_str});\"\n",
    "        \n",
    "        # 4. Write the content to a text file\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "        print(f\"File '{output_filename}' created successfully.\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for idx, config in enumerate(chart_configs):\n",
    "        try:\n",
    "            # Extract chart type and generate output filename\n",
    "            chart_type = config.get('chart', {}).get('type', f\"chart_{idx}\")\n",
    "            output_filename = os.path.join(output_directory, f\"{chart_type}_chart_{idx}.json\")\n",
    "\n",
    "            # Save the configuration as a JSON file\n",
    "            with open(output_filename, 'w') as json_file:\n",
    "                json.dump(config, json_file, indent=4)\n",
    "\n",
    "            print(f\"Successfully saved {chart_type} chart configuration to {output_filename}\")\n",
    "\n",
    "            # Optionally create a Highcharts file\n",
    "            highchart_output_filename = os.path.join(output_directory, f\"{chart_type}_chart_{idx}.txt\")\n",
    "            create_highchart_file(config, highchart_output_filename)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving chart configuration {idx}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChartGeneratorManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class ChartGeneratorManager:\n",
    "   def __init__(self):\n",
    "       self.generators = {\n",
    "           'line': LineChartGenerator(colors=colors),\n",
    "           'area': AreaChartGenerator(colors=colors),\n",
    "           'bar': BarChartGenerator(colors=colors),\n",
    "           'column': ColumnChartGenerator(colors=colors),\n",
    "           'pie': PieChartGenerator(colors=colors),\n",
    "           'scatter': ScatterChartGenerator(colors=colors), \n",
    "           'bubble': BubbleChartGenerator(colors=colors)\n",
    "       }\n",
    "       self.executor = ThreadPoolExecutor()\n",
    "\n",
    "   def validate_dataframe(self, df):\n",
    "       \"\"\"Validates if DataFrame meets minimum requirements for charting\"\"\"\n",
    "       return not (\n",
    "           df.empty or  # Empty dataframe\n",
    "           len(df) < 2 or  # Minimum 2 rows needed\n",
    "           len(df.columns) < 2  # Minimum 2 columns needed\n",
    "       )\n",
    "\n",
    "   async def generate_single_chart(self, chart_type, query, df):\n",
    "       if chart_type not in self.generators or not self.validate_dataframe(df):\n",
    "           return None\n",
    "           \n",
    "       try:\n",
    "           generator = self.generators[chart_type]\n",
    "           chart_config = await asyncio.get_event_loop().run_in_executor(\n",
    "               self.executor,\n",
    "               generator.generate_chart_json,\n",
    "               query,\n",
    "               df\n",
    "           )\n",
    "           return chart_config\n",
    "       except Exception as e:\n",
    "           print(f\"Error generating {chart_type} chart: {e}\")\n",
    "           return None\n",
    "\n",
    "   async def generate_chart_configs(self, list_of_charts, query, df):\n",
    "       if not self.validate_dataframe(df):\n",
    "           return []\n",
    "           \n",
    "       tasks = [\n",
    "           self.generate_single_chart(chart_type, query, df)\n",
    "           for chart_type in list_of_charts\n",
    "       ]\n",
    "       chart_configs = await asyncio.gather(*tasks)\n",
    "       return [config for config in chart_configs if config is not None]\n",
    "\n",
    "   def close(self):\n",
    "       self.executor.shutdown()\n",
    "\n",
    "chart_manager = ChartGeneratorManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run - New chart_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Running - Chart Functions\n",
    "# print(f'user_prompt: {user_prompt}\\n')\n",
    "\n",
    "# # ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# # ║                      🌟 chart_recommender 🌟                         ║\n",
    "# # ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# list_of_charts, suitable_chart_types = chart_recommender.get_suitable_charts(user_prompt, generated_sql_query, df)\n",
    "# print(json.dumps(suitable_chart_types, indent = 2))\n",
    "# print(f'\\nlist_of_charts: {list_of_charts}\\n')\n",
    "\n",
    "# # ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# # ║                      🌟 chart_manager 🌟                             ║\n",
    "# # ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# chart_configs = await chart_manager.generate_chart_configs(list_of_charts, user_prompt, df)\n",
    "# print(f\"\\nGenerated Chart Configs: {[chart_type['chart']['type'] for chart_type in chart_configs]}\")\n",
    "\n",
    "# # ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# # ║                      🌟 save_chart_configs 🌟                        ║\n",
    "# # ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "# print(f'\\n================================= Saving the Files ==================================\\n')\n",
    "# output_directory = \"highchart_json_files\"\n",
    "# if chart_configs:\n",
    "#     save_chart_configs(chart_configs, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Execute SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_execute_sql(\n",
    "    user_prompt,\n",
    "    table_to_use,\n",
    "    table_columns,\n",
    "    column_info_from_knowledge_base,\n",
    "    prompt_to_use_for_complex_question,\n",
    "    columns_info_to_generate_sql\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate and execute SQL queries based on user prompts and available table information.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_prompt : str\n",
    "        The user's natural language query\n",
    "    table_to_use : str, optional\n",
    "        The table name to query against\n",
    "    table_columns : list, optional\n",
    "        List of columns in the table\n",
    "    column_info_from_knowledge_base : dict, optional\n",
    "        Additional context about columns from knowledge base\n",
    "    prompt_to_use_for_complex_question : str, optional\n",
    "        Additional instructions for complex query generation\n",
    "    sql_query_generator : object, optional\n",
    "        Object with get_result_from_sql_query method for direct SQL execution\n",
    "    multi_stage_sql_generator : object, optional\n",
    "        Object with generate_and_iteratively_refine_sql method for complex SQL generation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (generated_sql_query, df) where df is the pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    # Initialize defaults\n",
    "    generated_sql_query = ''\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Only proceed if we have a table to query\n",
    "    if table_to_use:\n",
    "        # Check if complex instruction prompt exists (multi-stage generation path)\n",
    "        if prompt_to_use_for_complex_question:\n",
    "            print(\"[INFO] Running in Multi-Stage SQL Generation Mode...\")\n",
    "            # Generate final SQL with refinement steps\n",
    "            final_sql = multi_stage_sql_generator.generate_and_iteratively_refine_sql(\n",
    "                user_prompt=user_prompt,\n",
    "                schema={\"table_name\": table_to_use, \"columns\": table_columns},\n",
    "                domain_context=column_info_from_knowledge_base,\n",
    "                complex_instructions=prompt_to_use_for_complex_question,\n",
    "                max_iterations=2\n",
    "            )\n",
    "            # Execute the generated SQL directly (Mode 2)\n",
    "            generated_sql_query, df = sql_query_generator.get_result_from_sql_query(\n",
    "                generated_sql_query=final_sql\n",
    "            )\n",
    "        else:\n",
    "            print(\"[INFO] Running in Normal SQL Generation Mode...\")\n",
    "            # Generate and execute SQL based on prompt + column + table info (Mode 1)\n",
    "            generated_sql_query, df = sql_query_generator.get_result_from_sql_query(\n",
    "                user_prompt=user_prompt,\n",
    "                column_list=columns_info_to_generate_sql,  # Assuming columns_info_to_generate_sql == table_columns\n",
    "                redshift_table=table_to_use\n",
    "            )\n",
    "    else:\n",
    "        print(\"\\n❌ No table found. Skipping query generation.\\n\")\n",
    "    \n",
    "    return generated_sql_query, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_sql_query, df = generate_and_execute_sql(\n",
    "#             user_prompt=user_prompt_to_use,\n",
    "#             table_to_use=table_to_use,\n",
    "#             table_columns=table_columns,\n",
    "#             column_info_from_knowledge_base=column_info_from_knowledge_base,\n",
    "#             prompt_to_use_for_complex_question=prompt_to_use_for_complex_question,\n",
    "#             columns_info_to_generate_sql = columns_info_to_generate_sql,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CostSavingQuestionRefiner [TEMP_USE_CASE] - refine_cost_savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostSavingQuestionRefiner:\n",
    "    def __init__(self, model_name: str = 'gpt-4.1-mini'):\n",
    "        \"\"\"\n",
    "        Initializes the cost saving question refiner using an OpenAI LLM.\n",
    "        \"\"\"\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "        # Example-guided prompt\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"You are a helpful assistant.\n",
    "\n",
    "When a user asks how to save money for a specific UNSPSC category, you must rephrase the question\n",
    "into a structured form that requests supplier-level spend and pricing details.\n",
    "\n",
    "### Rules:\n",
    "- Only rewrite if the user is asking how to save money on a specific UNSPSC category.\n",
    "- Do NOT change or omit the UNSPSC value.\n",
    "- Always return the question in the following format exactly:\n",
    "  \"What is the total_spend, total_quantity, average_unit_price, and emr_supply_supplier for UNSPSC <UNSPSC_VALUE>?\"\n",
    "\n",
    "- The refined question MUST ensure the returned dataframe includes the following columns:\n",
    "  - 'emr_supply_supplier'\n",
    "  - 'total_spend'\n",
    "  - 'total_quantity'\n",
    "  - 'average_unit_price'\n",
    "\n",
    "### Examples:\n",
    "\n",
    "User Question:\n",
    "How can I save money on @UNSPSC UNSPSC_Name?\n",
    "\n",
    "→ Rephrased:\n",
    "What is the total_spend, total_quantity, average_unit_price, and emr_supply_supplier for UNSPSC UNSPSC_Name?\n",
    "\n",
    "User Question:\n",
    "Can we reduce cost for UNSPSC UNSPSC_Name?\n",
    "\n",
    "→ Rephrased:\n",
    "What is the total_spend, total_quantity, average_unit_price, and emr_supply_supplier for UNSPSC UNSPSC_Name?\n",
    "\n",
    "----------------------------\n",
    "Now rewrite the following user question:\n",
    "\n",
    "User Question:\n",
    "{question}\n",
    "\n",
    "→ Rephrased:\"\"\"\n",
    "        )\n",
    "\n",
    "        self.chain = self.prompt | self.llm | StrOutputParser()\n",
    "\n",
    "    def calculate_unspsc_savings(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates total spend, total quantity, and estimated savings\n",
    "        by consolidating purchases to the supplier with the lowest average unit price.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): Must include columns:\n",
    "                - 'emr_supply_supplier'\n",
    "                - 'total_spend'\n",
    "                - 'total_quantity'\n",
    "                - 'average_unit_price'\n",
    "\n",
    "        Returns:\n",
    "            dict: Summary with total cost, min supplier, and estimated savings.\n",
    "        \"\"\"\n",
    "        df2 = df.copy()\n",
    "\n",
    "        # Calculate total current cost\n",
    "        total_cost = df2['total_spend'].sum()\n",
    "\n",
    "        # Identify minimum unit price and supplier\n",
    "        min_unit_price = df2['average_unit_price'].min()\n",
    "        min_supplier = df2[df2['average_unit_price'] == min_unit_price]['emr_supply_supplier'].values[0]\n",
    "\n",
    "        # Total quantity across all suppliers\n",
    "        total_quantity = df2['total_quantity'].sum()\n",
    "\n",
    "        # Optimal cost if purchased from lowest-cost supplier\n",
    "        optimal_cost = total_quantity * min_unit_price\n",
    "\n",
    "        # Estimated savings\n",
    "        savings = total_cost - optimal_cost\n",
    "        return {\n",
    "            \"total_cost\": round(total_cost, 2),\n",
    "            \"optimal_cost\": round(optimal_cost, 2),\n",
    "            \"estimated_savings\": round(savings, 2),\n",
    "            \"total_quantity\": int(total_quantity),\n",
    "            \"lowest_unit_price\": round(min_unit_price, 2),\n",
    "            \"lowest_cost_supplier\": min_supplier\n",
    "        }\n",
    "\n",
    "    def refine(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Refines a cost-saving question into a structured query for supplier pricing.\n",
    "        \"\"\"\n",
    "        return self.chain.invoke({\"question\": question}).strip()\n",
    "\n",
    "refine_cost_savings = CostSavingQuestionRefiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine_cost_savings.refine(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========\n",
      "**New User Found: temp_user.\n",
      "===========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "####                                  Creating a new User to store messages\n",
    "#### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "memory_manager = MemoryChainManager()\n",
    "user_id = 'temp_user'\n",
    "creating_new_session_memory = memory_manager.get_by_session_id(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "╭──────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│─────────────────────────────────────────── User Query ───────────────────────────────────────────│\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "\n",
      "whats the total spend \u001b[34mfor\u001b[39;49;00m the last \u001b[34m12\u001b[39;49;00m months\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\n",
      "\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
      "\n",
      "\n",
      "\t >>>>>>>> Classification: 2 | Calculative Query <<<<<<<<<\n",
      "Table Selected: production_load.l_mt_cqo_p_event_with_outliers_chatbot_qa\n",
      "[INFO] Running in Normal SQL Generation Mode...\n",
      "[MODE 1:] No SQL provided. Generating SQL from prompt, column list, and table...\n",
      "SQL Response: \n",
      "The total spend for the last 12 months is $547,315,674.60.\n",
      "\n",
      "Data range: October 1, 2024 to August 1, 2025.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_spend</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.473157e+08</td>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>2025-08-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    total_spend  start_date    end_date\n",
       "0  5.473157e+08  2024-10-01  2025-08-01"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = input()\n",
    "\n",
    "# print(f'\\n\\t >>>>>>>> User Query: {user_prompt} <<<<<<<<<')\n",
    "pretty_display(user_prompt, title=\"User Query\")\n",
    "\n",
    "sql_response_chain_result = True\n",
    "\n",
    "# # >>>>>>>>>>>>>>>>>>>>>>>>  Memory Chain | combined_memory_chain_function <<<<<<<<<<<<<<<<<<<<<<<\n",
    "user_prompt_to_use, result_from_memory = memory_manager.combined_chain(user_prompt, user_id)\n",
    "prompt_to_save = user_prompt\n",
    "if user_prompt != user_prompt_to_use:\n",
    "    prompt_to_save = f\"{user_prompt} [REFINED QUERY: ({user_prompt_to_use})]\"\n",
    "# # >>>>>>>>>>>>>>>>>>>>>>>>  Memory Chain | combined_memory_chain_function <<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "## ============ TEMPORARY USE CASE ============= ##\n",
    "temp_use_case_for_cost_savings = False\n",
    "if 'save' in user_prompt.lower() and 'money' in user_prompt.lower() and 'unspsc' in user_prompt.lower():\n",
    "\n",
    "    result_from_memory = None\n",
    "    prompt_to_save = user_prompt\n",
    "\n",
    "    print(f'\\n=================\\nUNSPSC Savings question Found.\\n=================\\n')\n",
    "    user_prompt_to_use = refine_cost_savings.refine(user_prompt)\n",
    "    print(f'Refined Question: {user_prompt_to_use}')\n",
    "    temp_use_case_for_cost_savings = True\n",
    "## ============ ==================== ============= ##\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "if not result_from_memory:\n",
    "    query_type = query_type_classifier.classify_query(user_prompt_to_use)\n",
    "\n",
    "    if query_type == '1': # API Endpoint | General Query - Running Chroma Chain\n",
    "        print(f'\\n\\t >>>>>>>> Classification: {query_type} | General Query - Running Chroma Chain <<<<<<<<<')\n",
    "        context_check = query_type_classifier.is_out_of_context_general_knowledge(user_prompt_to_use)\n",
    "\n",
    "        if context_check == '0':\n",
    "            print(f'\\n***** Its an out-of-scope Question *****')\n",
    "            \n",
    "            general_query_result = query_type_classifier.generate_out_of_scope_response_with_llm(user_prompt_to_use)\n",
    "        else:\n",
    "            general_query_result = general_query_api.get_result(user_prompt_to_use)\n",
    "        \n",
    "        print(f'\\nResponse: {general_query_result}')\n",
    "\n",
    "        ### Saving Memory -> memory_manager.store\n",
    "        memory_manager.store[user_id].add_user_message(prompt_to_save)\n",
    "        memory_manager.store[user_id].add_ai_message(general_query_result)\n",
    "    \n",
    "    else:\n",
    "        ## Running the Chain | TableSelection\n",
    "        print(f'\\n\\t >>>>>>>> Classification: {query_type} | Calculative Query <<<<<<<<<')\n",
    "        selected_table_text = table_selection_object.get_table_name(user_prompt_to_use)\n",
    "\n",
    "        #### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "        #### Retrieving best matched column info for SQL generation based on user prompt and selected table\n",
    "        #### ------------------------------------------------ ## ------------------------------------------------ ####\n",
    "\n",
    "        for run in range(2):\n",
    "            table_to_use, filtered_df, best_match_columns, table_columns, column_info_from_knowledge_base, prompt_to_use_for_complex_question, columns_info_to_generate_sql = embedding_task_helper.get_column_info_for_sql_generation(user_prompt_to_use, selected_table_text)\n",
    "            \n",
    "            print(f'Table Selected: {table_to_use}')\n",
    "            generated_sql_query, df = generate_and_execute_sql(\n",
    "                user_prompt=user_prompt_to_use,\n",
    "                table_to_use=table_to_use,\n",
    "                table_columns=table_columns,\n",
    "                column_info_from_knowledge_base=column_info_from_knowledge_base,\n",
    "                prompt_to_use_for_complex_question=prompt_to_use_for_complex_question,\n",
    "                columns_info_to_generate_sql = columns_info_to_generate_sql\n",
    "            )\n",
    "\n",
    "            if not df.empty:\n",
    "                break\n",
    "        # --------------------------------------------------------------------------------------------------------------------------------- # \n",
    "\n",
    "        #################### SQL RESPONSE #########################\n",
    "        if sql_response_chain_result:\n",
    "\n",
    "            if temp_use_case_for_cost_savings and not df.empty: ## ## ============ TEMPORARY USE CASE ============= ## ONLY FOR ONE WEEK\n",
    "                temp_result = refine_cost_savings.calculate_unspsc_savings(df)\n",
    "                user_prompt_to_use_temp = (\n",
    "                    user_prompt +\n",
    "                    f\"\\n\\nThe following table contains supplier-level data retrieved from the database for the specified UNSPSC category:\\n\\n\"\n",
    "                    f\"{str(temp_result).replace('{', '{{').replace('}', '}}')}\\n\\n\"\n",
    "                    f\"Please display this table clearly in your response and use its values to perform a detailed analysis of potential cost savings.\\n\"\n",
    "                    f\"Your explanation must reference the actual spend, quantity, and unit price for each supplier, and explain how consolidating purchases to the lowest-cost supplier can lead to savings.\\n\"\n",
    "                    f\"Ensure the table appears in your final response to help the user easily interpret the figures.\"\n",
    "                )\n",
    "                sql_response_chain_result = sql_response_generator.get_sql_response(user_prompt_to_use_temp, generated_sql_query, df)\n",
    "            else:\n",
    "                sql_response_chain_result = sql_response_generator.get_sql_response(user_prompt_to_use, generated_sql_query, df)\n",
    "            \n",
    "            print(f'SQL Response: \\n{sql_response_chain_result}')\n",
    "            # Saving Memory Context | DataFrame\n",
    "            memory_manager.store[user_id].add_user_message(prompt_to_save)\n",
    "            memory_manager.store[user_id].add_ai_message(sql_response_chain_result)\n",
    "\n",
    "else:\n",
    "    print(f'\\n\\t >>>>>>>> Result from Memory <<<<<<<<<\\n')\n",
    "    print(f'{result_from_memory}')\n",
    "\n",
    "################ ---------------------- ##################### # >>>>>>>> CHARTING <<<<<<<<< ANALYSIS >>>>>>>>>>>>>>>\n",
    "# >>>>>>>> CHARTING <<<<<<<<< ANALYSIS >>>>>>>>>>>>>>> # >>>>>>>> CHARTING <<<<<<<<< ANALYSIS >>>>>>>>>>>>>>>\n",
    "################ ---------------------- ##################### # >>>>>>>> CHARTING <<<<<<<<< ANALYSIS >>>>>>>>>>>>>>>\n",
    "\n",
    "if False:\n",
    "    \n",
    "    charting_flag = False\n",
    "    analysis_flag = False\n",
    "    recommendation_flag = False\n",
    "    follow_up_flag = False\n",
    "\n",
    "    if charting_flag:\n",
    "        saved_image_file_path, saved_image_name, html_content = hichart_module.get_highchart_info(user_prompt_to_use, df, user_id)\n",
    "\n",
    "    if analysis_flag:\n",
    "        analysis_chain_response = analysis_chain_manager.run_analysis_chain(user_prompt_to_use, df)\n",
    "\n",
    "    if recommendation_flag:\n",
    "        recommendations = recommendation_module.run_dynamic_recommendation_chain(user_prompt_to_use, df, '')\n",
    "        print(f'\\n\\t >>>>>>>>  Recommendations: <<<<<<<<<\\n{recommendations}')\n",
    "\n",
    "    if follow_up_flag:\n",
    "        previous_questions = [\n",
    "            msg.content\n",
    "            for msg in memory_manager.store[user_id].messages\n",
    "            if isinstance(msg, HumanMessage)\n",
    "        ][-8:] ## Feeding Latest 8 questions\n",
    "\n",
    "        suggestions = question_suggestions_generator.get_suggestions(\n",
    "            user_prompt= user_prompt_to_use,\n",
    "            generated_sql_query=generated_sql_query,\n",
    "            table_to_use=table_to_use,\n",
    "            table_columns=table_columns,\n",
    "            past_questions=previous_questions # 👈 Prevents repeats\n",
    "        )\n",
    "\n",
    "        print(suggestions.to_list())\n",
    "\n",
    "print(f'\\n================================================================================\\n')\n",
    "\n",
    "### Clear all Messages ###\n",
    "memory_manager.store[user_id].clear()\n",
    "# print(generated_sql_query)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \n",
      "    SUM(emr_total_acquisition_cost) AS total_spend,\n",
      "    TO_CHAR(MIN(TO_DATE(emr_discharge_date, 'YYYY-MM-DD')), 'YYYY-MM-DD') AS start_date,\n",
      "    TO_CHAR(MAX(TO_DATE(emr_discharge_date, 'YYYY-MM-DD')), 'YYYY-MM-DD') AS end_date\n",
      "FROM \n",
      "    production_load.l_mt_cqo_p_event_with_outliers_chatbot_qa\n",
      "WHERE \n",
      "    TO_DATE(emr_discharge_date, 'YYYY-MM-DD') >= DATEADD(month, -12, '2025-09-19');\n"
     ]
    }
   ],
   "source": [
    "print(generated_sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_response_chain_result = sql_response_generator.get_sql_response(user_prompt_to_use, generated_sql_query, df)\n",
    "# print(sql_response_chain_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  - Some Questions - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# --- Robotic Analysis ----\n",
    "# ------------------------------------\n",
    "\n",
    "# find out the total number of Inpatient Robotic Cases\n",
    "# whats the total number of Robotic cases in 2024\n",
    "# total number of robotic and non robotic cases for inpatient?\n",
    "# total number of robotic and non robotic cases for inpatient and Outpatient ?\n",
    "# whats the total number of unique procedures in robotic analysis?\n",
    "# compare the ssi rate between robotic vs non-robotic cases\n",
    "# whats the total number of cases available in robotic, non-robotic and overall\n",
    "\n",
    "\n",
    "# ======================================\n",
    "\n",
    "# How does avg CPC, and total encounter vary across top 20 procedures for @market LORAIN and @market @YOUNGSTOWN for @service line @WOMEN'S HEALTH?\n",
    "# Compare the top 10 primary procedures by avg cpc for @market LORAIN and @market @YOUNGSTOWN for @service line @WOMEN'S HEALTH?\n",
    "# What is the '42321610-SPINAL SCREWS OR SCREW EXTENSIONS' UNSPSC product utilization across physicians used in '0SG10AJ-FUSION 2-4 L JT W INTBD FUS DEV, POST APPR A COL, OPEN' procedure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_response_chain_result = sql_response_generator.get_sql_response(user_prompt_to_use, generated_sql_query, df)\n",
    "# Markdown(sql_response_chain_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the total number of robotic and non-robotic cases for both inpatient and outpatient settings?\n",
    "# How many unique procedures are included in the robotic procedure analysis?\n",
    "# How does the SSI (Surgical Site Infection) rate compare between robotic and non-robotic cases?\n",
    "# What is the total number of cases categorized as robotic, non-robotic, and overall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style=\"color: red; font-weight: bold;\">🔥 ROUGH TASK 🔥</h2>\n",
    "- Used for Correctional Purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save - > columns_info_to_generate_sql | Save -> prompt_to_use_for_complex_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save - > columns_info_to_generate_sql\n",
    "# with open(\"../Rough - Notes/columns_info_to_generate_sql.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(columns_info_to_generate_sql.strip())\n",
    "\n",
    "\n",
    "# if prompt_to_use_for_complex_question:\n",
    "#     with open(\"../Rough - Notes/prompt_to_use_for_complex_question.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(prompt_to_use_for_complex_question.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from file - > columns_info_to_generate_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read from file - > columns_info_to_generate_sql\n",
    "# with open(\"../Rough - Notes/columns_info_to_generate_sql.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     columns_info_to_generate_sql = f.read()\n",
    "\n",
    "# with open(\"../Rough - Notes/prompt_to_use_for_complex_question.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     prompt_to_use_for_complex_question_new = f.read()\n",
    "\n",
    "# print(user_prompt_to_use)\n",
    "\n",
    "# # ------------------------------------------- # \n",
    "# # generated_sql_query, df = sql_query_generator.get_result_from_sql_query(user_prompt_to_use, columns_info_to_generate_sql, table_to_use)\n",
    "\n",
    "# generated_sql_query, sf = generate_and_execute_sql(\n",
    "#     user_prompt=user_prompt_to_use,\n",
    "#     table_to_use=table_to_use,\n",
    "#     table_columns=table_columns,\n",
    "#     column_info_from_knowledge_base=column_info_from_knowledge_base,\n",
    "#     prompt_to_use_for_complex_question=prompt_to_use_for_complex_question_new,\n",
    "#     columns_info_to_generate_sql = columns_info_to_generate_sql\n",
    "# )\n",
    "# # ------------------------------------------- # \n",
    "\n",
    "# print(generated_sql_query)\n",
    "# sf.head()\n",
    "\n",
    "# # # ------------------------------------------- # \n",
    "# # sql_response_chain_result = sql_response_generator.get_sql_response(user_prompt_to_use, generated_sql_query, df)\n",
    "# # Markdown(sql_response_chain_result)\n",
    "# # # ------------------------------------------- # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generated_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Column Values from each table (Temporary Use Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_column_value_descriptions(table_name: str, column_list: list, connection_pool, sample_limit: int = 20) -> dict:\n",
    "#     \"\"\"\n",
    "#     Generates descriptions for each column in a table using SQL and a DB connection.\n",
    "\n",
    "#     Args:\n",
    "#         table_name (str): The table to query.\n",
    "#         column_list (list): List of column names to evaluate.\n",
    "#         connection_pool: Active DB connection pool (e.g., psycopg2.pool.SimpleConnectionPool).\n",
    "#         sample_limit (int): Number of values to fetch from each column.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Column descriptions based on unique values.\n",
    "#     \"\"\"\n",
    "#     column_descriptions = {}\n",
    "\n",
    "#     conn = connection_pool.getconn()\n",
    "#     try:\n",
    "#         cursor = conn.cursor()\n",
    "#         for col in column_list:\n",
    "#             query = f\"\"\"\n",
    "#                 SELECT DISTINCT {col}\n",
    "#                 FROM {table_name}\n",
    "#                 WHERE {col} IS NOT NULL\n",
    "#                 LIMIT {sample_limit};\n",
    "#             \"\"\"\n",
    "\n",
    "#             try:\n",
    "#                 cursor.execute(query)\n",
    "#                 results = cursor.fetchall()\n",
    "#                 values = [r[0] for r in results if r[0] is not None]\n",
    "\n",
    "#                 if not values:\n",
    "#                     column_descriptions[col] = \"No available data\"\n",
    "#                     continue\n",
    "\n",
    "#                 # Determine data type\n",
    "#                 sample_type = type(values[0])\n",
    "\n",
    "#                 if all(isinstance(v, (int, float)) for v in values):\n",
    "#                     if all(isinstance(v, int) for v in values):\n",
    "#                         desc = \"This will be a Positive Integer Value\" if min(values) > 0 else \"This will be a Non-Negative Integer Value\"\n",
    "#                     else:\n",
    "#                         desc = \"This will be a Positive Float Value\" if min(values) > 0 else \"This will be a Non-Negative Float Value\"\n",
    "#                 else:\n",
    "#                     str_values = [str(v).strip() for v in values if str(v).strip()]\n",
    "#                     unique_count = len(str_values)\n",
    "\n",
    "#                     if unique_count > 10:\n",
    "#                         desc = \"Example Values:\\n\" + \"\\n\".join([f\"{i+1}. {v}\" for i, v in enumerate(str_values[:2])])\n",
    "#                     else:\n",
    "#                         desc = \", \".join(str_values[:10])\n",
    "\n",
    "#                 column_descriptions[col] = desc\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 column_descriptions[col] = f\"Error retrieving data: {str(e)}\"\n",
    "\n",
    "#         cursor.close()\n",
    "#     finally:\n",
    "#         connection_pool.putconn(conn)\n",
    "\n",
    "#     return column_descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_mt_opportunity_case_procedure_p_event_with_outliers_v2_chatbot, l_mt_opportunity_case_procedure_with_outliers_v2_chatbot, production_load.l_mt_opportunity_case_procedure_p_event_w_outliers_rbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tb in table_info_from_db['columns'][2]:\n",
    "#     if tb not in table_info_from_db['columns'][0]:\n",
    "#         print(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Table 1\n",
    "# # table_name = list(column_dictionary.keys())[0]\n",
    "# # columns = column_dictionary[table_name]\n",
    "\n",
    "# ## Table 2\n",
    "# # table_name = list(column_dictionary.keys())[1]\n",
    "# # columns = column_dictionary[table_name]\n",
    "\n",
    "\n",
    "# # Table 3\n",
    "# table_name = list(column_dictionary.keys())[2]\n",
    "# columns = column_dictionary[table_name]\n",
    "\n",
    "\n",
    "\n",
    "# #############################################################\n",
    "# # print(f'Table Running: {table_name}')\n",
    "# column_info = get_column_value_descriptions(table_name, columns, connection_pool)\n",
    "\n",
    "# for col, desc in column_info.items():\n",
    "#     print(f\"\\n🧠 {col}:\\n{desc}\")\n",
    "\n",
    "\n",
    "# ## Saving the File\n",
    "# sf = pd.DataFrame([\n",
    "#     {\"column_name\": k, \"description\": v}\n",
    "#     for k, v in column_info.items()\n",
    "# ])\n",
    "# # sf.to_excel('temp_use_case/Pharma_Poc_Data_Column_Details.xlsx', index=False)\n",
    "# sf.to_excel('temp_use_case/ras_table.xlsx', index=False)\n",
    "# # sf.to_excel('temp_use_case/Drug_Shortages_Data_Column_Details.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
